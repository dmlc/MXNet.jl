
<!DOCTYPE html>
<html class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="shortcut icon" href="../../assets/images/favicon.ico">
      
      <meta name="generator" content="mkdocs+mkdocs-material#1.0.3">
    
    
      
        <title>NDArray API - MXNet.jl</title>
      
    
    
      <script src="../../assets/javascripts/modernizr-facb31f4a3.js"></script>
    
    
      
        
        
        
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/application-f3ab63f78a.css">
      
        <link rel="stylesheet" href="../../assets/stylesheets/application-02ce7adcc2.palette.css">
      
      
        <link rel="stylesheet" href="../../assets/Documenter.css">
      
    
    
  </head>
  
  
  
  
    <body data-md-color-primary="indigo" data-md-color-accent="blue">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="drawer">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="search">
    <label class="md-overlay" data-md-component="overlay" for="drawer"></label>
    
      <header class="md-header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../.." title="MXNet.jl" class=" md-icon md-icon--home  md-header-nav__button">
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <span class="md-flex__ellipsis md-header-nav__title">
          
            
              
                <span class="md-header-nav__parent">
                  API Documentation
                </span>
              
            
            NDArray API
          
        </span>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="search"></label>
          
<div class="md-search" data-md-component="search">
  <div class="md-search__overlay"></div>
  <div class="md-search__inner">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" accesskey="s" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false">
      <label class="md-icon md-search__icon" for="search"></label>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result"></div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            
              


  


  <a href="https://github.com/dmlc/MXNet.jl" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      GitHub
    </div>
  </a>

            
          </div>
      </div>
    </div>
  </nav>
</header>
    
    <div class="md-container">
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary">
  <label class="md-nav__title md-nav__title--site" for="drawer">
    <i class=" md-icon md-icon--home  md-nav__button">
      
    </i>
    MXNet.jl
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/dmlc/MXNet.jl" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      GitHub
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  <li class="md-nav__item">
    
      <a href="../.." title="Home" class="md-nav__link">
        Home
      </a>
    
  </li>

    
      
      
  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      Tutorial
    </label>
    <nav class="md-nav" data-md-component="collapsible">
      <label class="md-nav__title" for="nav-2">
        Tutorial
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
  <li class="md-nav__item">
    
      <a href="../../tutorial/mnist/" title="Digit Recognition on MNIST" class="md-nav__link">
        Digit Recognition on MNIST
      </a>
    
  </li>

        
          
          
  <li class="md-nav__item">
    
      <a href="../../tutorial/char-lstm/" title="Generating Random Sentence with LSTM RNN" class="md-nav__link">
        Generating Random Sentence with LSTM RNN
      </a>
    
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      User Guide
    </label>
    <nav class="md-nav" data-md-component="collapsible">
      <label class="md-nav__title" for="nav-3">
        User Guide
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
  <li class="md-nav__item">
    
      <a href="../../user-guide/install/" title="Installation Guide" class="md-nav__link">
        Installation Guide
      </a>
    
  </li>

        
          
          
  <li class="md-nav__item">
    
      <a href="../../user-guide/overview/" title="Overview" class="md-nav__link">
        Overview
      </a>
    
  </li>

        
          
          
  <li class="md-nav__item">
    
      <a href="../../user-guide/faq/" title="FAQ" class="md-nav__link">
        FAQ
      </a>
    
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
    
    <label class="md-nav__link" for="nav-4">
      API Documentation
    </label>
    <nav class="md-nav" data-md-component="collapsible">
      <label class="md-nav__title" for="nav-4">
        API Documentation
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
  <li class="md-nav__item">
    
      <a href="../context/" title="Context" class="md-nav__link">
        Context
      </a>
    
  </li>

        
          
          
  <li class="md-nav__item">
    
      <a href="../model/" title="Models" class="md-nav__link">
        Models
      </a>
    
  </li>

        
          
          
  <li class="md-nav__item">
    
      <a href="../initializer/" title="Initializers" class="md-nav__link">
        Initializers
      </a>
    
  </li>

        
          
          
  <li class="md-nav__item">
    
      <a href="../optimizer/" title="Optimizers" class="md-nav__link">
        Optimizers
      </a>
    
  </li>

        
          
          
  <li class="md-nav__item">
    
      <a href="../callback/" title="Callbacks in training" class="md-nav__link">
        Callbacks in training
      </a>
    
  </li>

        
          
          
  <li class="md-nav__item">
    
      <a href="../metric/" title="Evaluation Metrics" class="md-nav__link">
        Evaluation Metrics
      </a>
    
  </li>

        
          
          
  <li class="md-nav__item">
    
      <a href="../io/" title="Data Providers" class="md-nav__link">
        Data Providers
      </a>
    
  </li>

        
          
          
  <li class="md-nav__item">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="toc">
    
      
    
    
    <a href="./" title="NDArray API" class="md-nav__link md-nav__link--active">
      NDArray API
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
</nav>
    
  </li>

        
          
          
  <li class="md-nav__item">
    
      <a href="../symbolic-node/" title="Symbolic API" class="md-nav__link">
        Symbolic API
      </a>
    
  </li>

        
          
          
  <li class="md-nav__item">
    
      <a href="../nn-factory/" title="Neural Networks Factory" class="md-nav__link">
        Neural Networks Factory
      </a>
    
  </li>

        
          
          
  <li class="md-nav__item">
    
      <a href="../executor/" title="Executor" class="md-nav__link">
        Executor
      </a>
    
  </li>

        
          
          
  <li class="md-nav__item">
    
      <a href="../visualize/" title="Network Visualization" class="md-nav__link">
        Network Visualization
      </a>
    
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
               
                 <a href="https://github.com/dmlc/MXNet.jl/edit/master/docs/api/ndarray.md" title="Edit this page" class="md-icon md-content__edit">edit</a>
               
              
                
                <p><a id='NDArray-API-1'></a></p>
<h1 id="ndarray-api">NDArray API</h1>
<p><a id='Base.Flatten-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.Flatten-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.Flatten</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>Flatten(data)
</code></pre>

<p>Flatten input into 2D by collapsing all the higher dimensions. A (d1, d2, ..., dK) tensor is flatten to (d1, d2<em> ... </em>dK) matrix.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Input data to reshape.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1086' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.NDArray' href='#MXNet.mx.NDArray'>#</a>
<strong><code>MXNet.mx.NDArray</code></strong> &mdash; <em>Type</em>.</p>
<pre><code>NDArray
</code></pre>

<p>Wrapper of the <code>NDArray</code> type in <code>libmxnet</code>. This is the basic building block of tensor-based computation.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>since C/C++ use row-major ordering for arrays while Julia follows a   column-major ordering. To keep things consistent, we keep the underlying data   in their original layout, but use <em>language-native</em> convention when we talk   about shapes. For example, a mini-batch of 100 MNIST images is a tensor of   C/C++/Python shape (100,1,28,28), while in Julia, the same piece of memory   have shape (28,28,1,100).</p>
</div>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L70-L83' class='documenter-source'>source</a><br></p>
<p><a id='Base.:*-Tuple{MXNet.mx.NDArray,Real}' href='#Base.:*-Tuple{MXNet.mx.NDArray,Real}'>#</a>
<strong><code>Base.:*</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>*(arg0, arg1)
</code></pre>

<p>Currently only multiplication a scalar with an <code>NDArray</code> is implemented. Matrix multiplication is to be added soon.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L634-L639' class='documenter-source'>source</a><br></p>
<p><a id='Base.:+-Tuple{MXNet.mx.NDArray,Vararg{Union{MXNet.mx.NDArray,Real},N}}' href='#Base.:+-Tuple{MXNet.mx.NDArray,Vararg{Union{MXNet.mx.NDArray,Real},N}}'>#</a>
<strong><code>Base.:+</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>+(args...)
.+(args...)
</code></pre>

<p>Summation. Multiple arguments of either scalar or <code>NDArray</code> could be added together. Note at least the first or second argument needs to be an <code>NDArray</code> to avoid ambiguity of built-in summation.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L537-L544' class='documenter-source'>source</a><br></p>
<p><a id='Base.:--Tuple{MXNet.mx.NDArray,Union{MXNet.mx.NDArray,Real}}' href='#Base.:--Tuple{MXNet.mx.NDArray,Union{MXNet.mx.NDArray,Real}}'>#</a>
<strong><code>Base.:-</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>-(arg0, arg1)
-(arg0)
.-(arg0, arg1)
</code></pre>

<p>Subtraction <code>arg0 - arg1</code>, of scalar types or <code>NDArray</code>. Or create the negative of <code>arg0</code>.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L575-L582' class='documenter-source'>source</a><br></p>
<p><a id='Base.:.*-Tuple{MXNet.mx.NDArray,Union{MXNet.mx.NDArray,Real}}' href='#Base.:.*-Tuple{MXNet.mx.NDArray,Union{MXNet.mx.NDArray,Real}}'>#</a>
<strong><code>Base.:.*</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>.*(arg0, arg1)
</code></pre>

<p>Elementwise multiplication of <code>arg0</code> and <code>arg</code>, could be either scalar or <code>NDArray</code>.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L621-L625' class='documenter-source'>source</a><br></p>
<p><a id='Base.:./-Tuple{MXNet.mx.NDArray,Union{MXNet.mx.NDArray,Real}}' href='#Base.:./-Tuple{MXNet.mx.NDArray,Union{MXNet.mx.NDArray,Real}}'>#</a>
<strong><code>Base.:./</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>./(arg0 :: NDArray, arg :: Union{Real, NDArray})
</code></pre>

<p>Elementwise dividing an <code>NDArray</code> by a scalar or another <code>NDArray</code> of the same shape.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L663-L667' class='documenter-source'>source</a><br></p>
<p><a id='Base.:/-Tuple{MXNet.mx.NDArray,Real}' href='#Base.:/-Tuple{MXNet.mx.NDArray,Real}'>#</a>
<strong><code>Base.:/</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>/(arg0 :: NDArray, arg :: Real)
</code></pre>

<p>Divide an <code>NDArray</code> by a scalar. Matrix division (solving linear systems) is not implemented yet.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L673-L677' class='documenter-source'>source</a><br></p>
<p><a id='Base.LinAlg.dot-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.LinAlg.dot-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.LinAlg.dot</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>dot(lhs, rhs, transpose_a, transpose_b)
</code></pre>

<p>Calculate dot product of two matrices or two vectors.</p>
<p>From:src/operator/tensor/matrix_op.cc:231</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: Left input</li>
<li><code>rhs::NDArray</code>: Right input</li>
<li><code>transpose_a::boolean, optional, default=False</code>: True if the first matrix is transposed.</li>
<li><code>transpose_b::boolean, optional, default=False</code>: True if the second matrix is tranposed.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1093' class='documenter-source'>source</a><br></p>
<p><a id='Base.LinAlg.norm-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.LinAlg.norm-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.LinAlg.norm</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>norm(src)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>src::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1085' class='documenter-source'>source</a><br></p>
<p><a id='Base.Math.gamma-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.Math.gamma-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.Math.gamma</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>gamma(data)
</code></pre>

<p>Take the gamma function (extension of the factorial function) of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:306</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='Base._div-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base._div-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base._div</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_div(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='Base._sub-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base._sub-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base._sub</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_sub(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='Base.abs-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.abs-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.abs</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>abs(data)
</code></pre>

<p>Take absolute value of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:63</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='Base.ceil-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.ceil-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.ceil</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>ceil(data)
</code></pre>

<p>Take ceil of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:87</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='Base.convert-Tuple{Type{Array{T<:Real,N}},MXNet.mx.NDArray}' href='#Base.convert-Tuple{Type{Array{T<:Real,N}},MXNet.mx.NDArray}'>#</a>
<strong><code>Base.convert</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>convert(::Type{Array{T}}, arr :: NDArray)
</code></pre>

<p>Convert an <code>NDArray</code> into a Julia <code>Array</code> of specific type. Data will be copied.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L472-L476' class='documenter-source'>source</a><br></p>
<p><a id='Base.copy!-Tuple{MXNet.mx.NDArray,MXNet.mx.NDArray}' href='#Base.copy!-Tuple{MXNet.mx.NDArray,MXNet.mx.NDArray}'>#</a>
<strong><code>Base.copy!</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>copy!(dst :: Union{NDArray, Array}, src :: Union{NDArray, Array})
</code></pre>

<p>Copy contents of <code>src</code> into <code>dst</code>.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L400-L404' class='documenter-source'>source</a><br></p>
<p><a id='Base.copy-Tuple{MXNet.mx.NDArray}' href='#Base.copy-Tuple{MXNet.mx.NDArray}'>#</a>
<strong><code>Base.copy</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>copy(arr :: NDArray)
copy(arr :: NDArray, ctx :: Context)
copy(arr :: Array, ctx :: Context)
</code></pre>

<p>Create a copy of an array. When no <code>Context</code> is given, create a Julia <code>Array</code>. Otherwise, create an <code>NDArray</code> on the specified context.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L446-L453' class='documenter-source'>source</a><br></p>
<p><a id='Base.cos-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.cos-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.cos</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>cos(data)
</code></pre>

<p>Take cos of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:189</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='Base.cosh-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.cosh-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.cosh</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>cosh(data)
</code></pre>

<p>Take cosh of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:261</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='Base.eltype-Tuple{T<:Union{MXNet.mx.MX_NDArrayHandle,MXNet.mx.NDArray}}' href='#Base.eltype-Tuple{T<:Union{MXNet.mx.MX_NDArrayHandle,MXNet.mx.NDArray}}'>#</a>
<strong><code>Base.eltype</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>eltype(arr :: NDArray)
</code></pre>

<p>Get the element type of an <code>NDArray</code>.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L283-L287' class='documenter-source'>source</a><br></p>
<p><a id='Base.exp-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.exp-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.exp</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>exp(data)
</code></pre>

<p>Take exp of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:135</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='Base.expm1-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.expm1-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.expm1</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>expm1(data)
</code></pre>

<p>Take <code>exp(x) - 1</code> in a numerically stable way</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:180</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='Base.floor-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.floor-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.floor</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>floor(data)
</code></pre>

<p>Take floor of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:92</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='Base.getindex-Tuple{MXNet.mx.NDArray,Colon}' href='#Base.getindex-Tuple{MXNet.mx.NDArray,Colon}'>#</a>
<strong><code>Base.getindex</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>getindex(arr :: NDArray, idx)
</code></pre>

<p>Shortcut for <a href="./#Base.slice-Tuple{MXNet.mx.NDArray,Colon}"><code>slice</code></a>. A typical use is to write</p>
<pre><code class="julia">  arr[:] += 5
</code></pre>

<p>which translates into</p>
<pre><code class="julia">  arr[:] = arr[:] + 5
</code></pre>

<p>which furthur translates into</p>
<pre><code class="julia">  setindex!(getindex(arr, Colon()), 5, Colon())
</code></pre>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The behavior is quite different from indexing into Julia's <code>Array</code>. For example, <code>arr[2:5]</code> create a <strong>copy</strong> of the sub-array for Julia <code>Array</code>, while for <code>NDArray</code>, this is a <em>slice</em> that shares the memory.</p>
</div>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L361-L386' class='documenter-source'>source</a><br></p>
<p><a id='Base.getindex-Tuple{MXNet.mx.NDArray,UnitRange{Int64}}' href='#Base.getindex-Tuple{MXNet.mx.NDArray,UnitRange{Int64}}'>#</a>
<strong><code>Base.getindex</code></strong> &mdash; <em>Method</em>.</p>
<p>Shortcut for <a href="./#Base.slice-Tuple{MXNet.mx.NDArray,Colon}"><code>slice</code></a>. <strong>NOTE</strong> the behavior for Julia's built-in index slicing is to create a copy of the sub-array, while here we simply call <code>slice</code>, which shares the underlying memory.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L391-L394' class='documenter-source'>source</a><br></p>
<p><a id='Base.identity-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.identity-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.identity</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>identity(data)
</code></pre>

<p>identity is an alias of _copy.</p>
<p>Identity mapping, copy src to output</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:14</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='Base.length-Tuple{MXNet.mx.NDArray}' href='#Base.length-Tuple{MXNet.mx.NDArray}'>#</a>
<strong><code>Base.length</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>length(arr :: NDArray)
</code></pre>

<p>Get the number of elements in an <code>NDArray</code>.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L265-L269' class='documenter-source'>source</a><br></p>
<p><a id='Base.log-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.log-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.log</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>log(data)
</code></pre>

<p>Take log of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:141</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='Base.log10-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.log10-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.log10</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>log10(data)
</code></pre>

<p>Take base-10 log of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:147</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='Base.log1p-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.log1p-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.log1p</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>log1p(data)
</code></pre>

<p>Take <code>log(1 + x)</code> in a numerically stable way</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:171</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='Base.log2-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.log2-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.log2</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>log2(data)
</code></pre>

<p>Take base-2 log of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:153</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='Base.max-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.max-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.max</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>max(data, axis, keepdims)
</code></pre>

<p>Compute max along axis. If axis is empty, global reduction is performed</p>
<p>From:src/operator/tensor/broadcast_reduce_op_value.cc:66</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
<li><code>axis::Shape(tuple), optional, default=()</code>: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.</li>
<li><code>keepdims::boolean, optional, default=False</code>: If true, the axis which is reduced is left in the result as dimension with size one.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1091' class='documenter-source'>source</a><br></p>
<p><a id='Base.mean-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.mean-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.mean</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>mean(data, axis, keepdims)
</code></pre>

<p>Compute mean src along axis. If axis is empty, global reduction is performed</p>
<p>From:src/operator/tensor/broadcast_reduce_op_value.cc:26</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
<li><code>axis::Shape(tuple), optional, default=()</code>: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.</li>
<li><code>keepdims::boolean, optional, default=False</code>: If true, the axis which is reduced is left in the result as dimension with size one.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1091' class='documenter-source'>source</a><br></p>
<p><a id='Base.min-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.min-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.min</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>min(data, axis, keepdims)
</code></pre>

<p>Compute min along axis. If axis is empty, global reduction is performed</p>
<p>From:src/operator/tensor/broadcast_reduce_op_value.cc:76</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
<li><code>axis::Shape(tuple), optional, default=()</code>: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.</li>
<li><code>keepdims::boolean, optional, default=False</code>: If true, the axis which is reduced is left in the result as dimension with size one.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1091' class='documenter-source'>source</a><br></p>
<p><a id='Base.ndims-Tuple{MXNet.mx.NDArray}' href='#Base.ndims-Tuple{MXNet.mx.NDArray}'>#</a>
<strong><code>Base.ndims</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>ndims(arr :: NDArray)
</code></pre>

<p>Get the number of dimensions of an <code>NDArray</code>. Is equivalent to <code>length(size(arr))</code>.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L274-L278' class='documenter-source'>source</a><br></p>
<p><a id='Base.prod-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.prod-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.prod</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>prod(data, axis, keepdims)
</code></pre>

<p>Compute product of src along axis. If axis is empty, global reduction is performed</p>
<p>From:src/operator/tensor/broadcast_reduce_op_value.cc:36</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
<li><code>axis::Shape(tuple), optional, default=()</code>: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.</li>
<li><code>keepdims::boolean, optional, default=False</code>: If true, the axis which is reduced is left in the result as dimension with size one.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1091' class='documenter-source'>source</a><br></p>
<p><a id='Base.round-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.round-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.round</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>round(data)
</code></pre>

<p>Take round of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:81</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='Base.setindex!-Tuple{MXNet.mx.NDArray,Real,Colon}' href='#Base.setindex!-Tuple{MXNet.mx.NDArray,Real,Colon}'>#</a>
<strong><code>Base.setindex!</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>setindex!(arr :: NDArray, val, idx)
</code></pre>

<p>Assign values to an <code>NDArray</code>. Elementwise assignment is not implemented, only the following scenarios are supported</p>
<ul>
<li><code>arr[:] = val</code>: whole array assignment, <code>val</code> could be a scalar or an array (Julia <code>Array</code> or <code>NDArray</code>) of the same shape.</li>
<li><code>arr[start:stop] = val</code>: assignment to a <em>slice</em>, <code>val</code> could be a scalar or an array of the same shape to the slice. See also <a href="./#Base.slice-Tuple{MXNet.mx.NDArray,Colon}"><code>slice</code></a>.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L334-L344' class='documenter-source'>source</a><br></p>
<p><a id='Base.sign-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.sign-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.sign</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>sign(data)
</code></pre>

<p>Take sign of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:72</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='Base.similar-Tuple{MXNet.mx.NDArray}' href='#Base.similar-Tuple{MXNet.mx.NDArray}'>#</a>
<strong><code>Base.similar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>similar(arr :: NDArray)
</code></pre>

<p>Create an <code>NDArray</code> with similar shape, data type, and context with the given one.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L160-L164' class='documenter-source'>source</a><br></p>
<p><a id='Base.sin-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.sin-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.sin</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>sin(data)
</code></pre>

<p>Take sin of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:162</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='Base.sinh-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.sinh-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.sinh</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>sinh(data)
</code></pre>

<p>Take sinh of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:252</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='Base.size-Tuple{MXNet.mx.NDArray}' href='#Base.size-Tuple{MXNet.mx.NDArray}'>#</a>
<strong><code>Base.size</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>size(arr :: NDArray)
size(arr :: NDArray, dim :: Int)
</code></pre>

<p>Get the shape of an <code>NDArray</code>. The shape is in Julia's column-major convention. See also the notes on NDArray shapes <a href="./#MXNet.mx.NDArray"><code>NDArray</code></a>.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L247-L253' class='documenter-source'>source</a><br></p>
<p><a id='Base.slice-Tuple{MXNet.mx.NDArray,Colon}' href='#Base.slice-Tuple{MXNet.mx.NDArray,Colon}'>#</a>
<strong><code>Base.slice</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>slice(arr :: NDArray, start:stop)
</code></pre>

<p>Create a view into a sub-slice of an <code>NDArray</code>. Note only slicing at the slowest changing dimension is supported. In Julia's column-major perspective, this is the last dimension. For example, given an <code>NDArray</code> of shape (2,3,4), <code>slice(array, 2:3)</code> will create a <code>NDArray</code> of shape (2,3,2), sharing the data with the original array. This operation is used in data parallelization to split mini-batch into sub-batches for different devices.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L304-L312' class='documenter-source'>source</a><br></p>
<p><a id='Base.sort-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.sort-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.sort</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>sort(src, axis, is_ascend)
</code></pre>

<p>Return a sorted copy of an array.</p>
<p>From:src/operator/tensor/ordering_op.cc:59</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>src::NDArray</code>: Source input</li>
<li><code>axis::int or None, optional, default='-1'</code>: Axis along which to choose sort the input tensor. If not given, the flattened array is used. Default is -1.</li>
<li><code>is_ascend::boolean, optional, default=True</code>: Whether sort in ascending or descending order.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1091' class='documenter-source'>source</a><br></p>
<p><a id='Base.sqrt-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.sqrt-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.sqrt</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>sqrt(data)
</code></pre>

<p>Take square root of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:116</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='Base.sum-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.sum-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.sum</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>sum(data, axis, keepdims)
</code></pre>

<p>Sum src along axis. If axis is empty, global reduction is performed</p>
<p>From:src/operator/tensor/broadcast_reduce_op_value.cc:17</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
<li><code>axis::Shape(tuple), optional, default=()</code>: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.</li>
<li><code>keepdims::boolean, optional, default=False</code>: If true, the axis which is reduced is left in the result as dimension with size one.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1091' class='documenter-source'>source</a><br></p>
<p><a id='Base.take-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.take-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.take</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>take(a, indices, axis, mode)
</code></pre>

<p>Take row vectors from an NDArray according to the indices For an input of index with shape (d1, ..., dK), the output shape is (d1, ..., dK, row_vector_length).All the input values should be integers in the range [0, column_vector_length).</p>
<p>From:src/operator/tensor/indexing_op.cc:59</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>a::SymbolicNode</code>: The source array.</li>
<li><code>indices::SymbolicNode</code>: The indices of the values to extract.</li>
<li><code>axis::int, optional, default='0'</code>: the axis of data tensor to be taken.</li>
<li><code>mode::{'clip', 'raise', 'wrap'},optional, default='raise'</code>: specify how out-of-bound indices bahave.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1093' class='documenter-source'>source</a><br></p>
<p><a id='Base.tan-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.tan-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.tan</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>tan(data)
</code></pre>

<p>Take tan of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:198</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='Base.tanh-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.tanh-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.tanh</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>tanh(data)
</code></pre>

<p>Take tanh of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:270</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='Base.transpose-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#Base.transpose-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>Base.transpose</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>transpose(data, axes)
</code></pre>

<p>Transpose the input tensor and return a new one</p>
<p>From:src/operator/tensor/matrix_op.cc:94</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
<li><code>axes::Shape(tuple), optional, default=()</code>: Target axis order. By default the axes will be inverted.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.Activation-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.Activation-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.Activation</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>Activation(data, act_type)
</code></pre>

<p>Elementwise activation function.</p>
<p>The following activation types are supported (operations are applied elementwisely to each scalar of the input tensor):</p>
<ul>
<li><code>relu</code>: Rectified Linear Unit, <code>y = max(x, 0)</code></li>
<li><code>sigmoid</code>: <code>y = 1 / (1 + exp(-x))</code></li>
<li><code>tanh</code>: Hyperbolic tangent, <code>y = (exp(x) - exp(-x)) / (exp(x) + exp(-x))</code></li>
<li><code>softrelu</code>: Soft ReLU, or SoftPlus, <code>y = log(1 + exp(x))</code></li>
</ul>
<p>See <code>LeakyReLU</code> for other activations with parameters.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to activation function.</li>
<li><code>act_type::{'relu', 'sigmoid', 'softrelu', 'tanh'}, required</code>: Activation function to be applied.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1098' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.BatchNorm-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.BatchNorm-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.BatchNorm</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>BatchNorm(data, gamma, beta, eps, momentum, fix_gamma, use_global_stats, output_mean_var)
</code></pre>

<p>Apply batch normalization to input.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to batch normalization</li>
<li><code>gamma::SymbolicNode</code>: gamma matrix</li>
<li><code>beta::SymbolicNode</code>: beta matrix</li>
<li><code>eps::float, optional, default=0.001</code>: Epsilon to prevent div 0</li>
<li><code>momentum::float, optional, default=0.9</code>: Momentum for moving average</li>
<li><code>fix_gamma::boolean, optional, default=True</code>: Fix gamma while training</li>
<li><code>use_global_stats::boolean, optional, default=False</code>: Whether use global moving statistics instead of local batch-norm. This will force change batch-norm into a scale shift operator.</li>
<li><code>output_mean_var::boolean, optional, default=False</code>: Output All,normal mean and var</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1099' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.BlockGrad-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.BlockGrad-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.BlockGrad</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>BlockGrad(data)
</code></pre>

<p>Get output from a symbol and pass 0 gradient back</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:30</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.Cast-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.Cast-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.Cast</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>Cast(data, dtype)
</code></pre>

<p>Cast array to a different data type.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to cast function.</li>
<li><code>dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'}, required</code>: Target data type.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.Concat-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.Concat-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.Concat</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>Concat(data, num_args, dim)
</code></pre>

<p><strong>Note</strong>: Concat takes variable number of positional inputs. So instead of calling as Concat([x, y, z], num_args=3), one should call via Concat(x, y, z), and num_args will be determined automatically.</p>
<p>Perform a feature concat on channel dim (defaut is 1) over all</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode[]</code>: List of tensors to concatenate</li>
<li><code>num_args::int, required</code>: Number of inputs to be concated.</li>
<li><code>dim::int, optional, default='1'</code>: the dimension to be concated.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1091' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.Convolution-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.Convolution-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.Convolution</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>Convolution(data, weight, bias, kernel, stride, dilate, pad, num_filter, num_group, workspace, no_bias, cudnn_tune, cudnn_off, layout)
</code></pre>

<p>Apply convolution to input then add a bias.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to the ConvolutionOp.</li>
<li><code>weight::SymbolicNode</code>: Weight matrix.</li>
<li><code>bias::SymbolicNode</code>: Bias parameter.</li>
<li><code>kernel::Shape(tuple), required</code>: convolution kernel size: (h, w) or (d, h, w)</li>
<li><code>stride::Shape(tuple), optional, default=()</code>: convolution stride: (h, w) or (d, h, w)</li>
<li><code>dilate::Shape(tuple), optional, default=()</code>: convolution dilate: (h, w) or (d, h, w)</li>
<li><code>pad::Shape(tuple), optional, default=()</code>: pad for convolution: (h, w) or (d, h, w)</li>
<li><code>num_filter::int (non-negative), required</code>: convolution filter(channel) number</li>
<li><code>num_group::int (non-negative), optional, default=1</code>: Number of group partitions. Equivalent to slicing input into num_group   partitions, apply convolution on each, then concatenate the results</li>
<li><code>workspace::long (non-negative), optional, default=1024</code>: Maximum tmp workspace allowed for convolution (MB).</li>
<li><code>no_bias::boolean, optional, default=False</code>: Whether to disable bias parameter.</li>
<li><code>cudnn_tune::{None, 'fastest', 'limited_workspace', 'off'},optional, default='None'</code>: Whether to pick convolution algo by running performance test.   Leads to higher startup time but may give faster speed. Options are:   'off': no tuning   'limited_workspace': run test and pick the fastest algorithm that doesn't exceed workspace limit.   'fastest': pick the fastest algorithm and ignore workspace limit.   If set to None (default), behavior is determined by environment   variable MXNET_CUDNN_AUTOTUNE_DEFAULT: 0 for off,   1 for limited workspace (default), 2 for fastest.</li>
<li><code>cudnn_off::boolean, optional, default=False</code>: Turn off cudnn for this layer.</li>
<li><code>layout::{None, 'NCDHW', 'NCHW', 'NDHWC', 'NHWC'},optional, default='None'</code>: Set layout for input, output and weight. Empty for   default layout: NCHW for 2d and NCDHW for 3d.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1120' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.Correlation-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.Correlation-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.Correlation</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>Correlation(data1, data2, kernel_size, max_displacement, stride1, stride2, pad_size, is_multiply)
</code></pre>

<p>Apply correlation to inputs</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data1::SymbolicNode</code>: Input data1 to the correlation.</li>
<li><code>data2::SymbolicNode</code>: Input data2 to the correlation.</li>
<li><code>kernel_size::int (non-negative), optional, default=1</code>: kernel size for Correlation must be an odd number</li>
<li><code>max_displacement::int (non-negative), optional, default=1</code>: Max displacement of Correlation</li>
<li><code>stride1::int (non-negative), optional, default=1</code>: stride1 quantize data1 globally</li>
<li><code>stride2::int (non-negative), optional, default=1</code>: stride2 quantize data2 within the neighborhood centered around data1</li>
<li><code>pad_size::int (non-negative), optional, default=0</code>: pad for Correlation</li>
<li><code>is_multiply::boolean, optional, default=True</code>: operation type is either multiplication or subduction</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1099' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.Crop-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.Crop-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.Crop</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>Crop(data, num_args, offset, h_w, center_crop)
</code></pre>

<p><strong>Note</strong>: Crop takes variable number of positional inputs. So instead of calling as Crop([x, y, z], num_args=3), one should call via Crop(x, y, z), and num_args will be determined automatically.</p>
<p>Crop the 2nd and 3rd dim of input data, with the corresponding size of h_w or with width and height of the second input symbol, i.e., with one input, we need h_w to specify the crop height and width, otherwise the second input symbol's size will be used</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode or SymbolicNode[]</code>: Tensor or List of Tensors, the second input will be used as crop_like shape reference</li>
<li><code>num_args::int, required</code>: Number of inputs for crop, if equals one, then we will use the h_wfor crop height and width, else if equals two, then we will use the heightand width of the second input symbol, we name crop_like here</li>
<li><code>offset::Shape(tuple), optional, default=(0,0)</code>: crop offset coordinate: (y, x)</li>
<li><code>h_w::Shape(tuple), optional, default=(0,0)</code>: crop height and weight: (h, w)</li>
<li><code>center_crop::boolean, optional, default=False</code>: If set to true, then it will use be the center_crop,or it will crop using the shape of crop_like</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1095' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.Custom-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.Custom-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.Custom</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>Custom(op_type)
</code></pre>

<p>Custom operator implemented in frontend.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>op_type::string</code>: Type of custom operator. Must be registered first.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1085' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.Deconvolution-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.Deconvolution-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.Deconvolution</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>Deconvolution(data, weight, bias, kernel, stride, pad, adj, target_shape, num_filter, num_group, workspace, no_bias)
</code></pre>

<p>Apply deconvolution to input then add a bias.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to the DeconvolutionOp.</li>
<li><code>weight::SymbolicNode</code>: Weight matrix.</li>
<li><code>bias::SymbolicNode</code>: Bias parameter.</li>
<li><code>kernel::Shape(tuple), required</code>: deconvolution kernel size: (y, x)</li>
<li><code>stride::Shape(tuple), optional, default=(1,1)</code>: deconvolution stride: (y, x)</li>
<li><code>pad::Shape(tuple), optional, default=(0,0)</code>: pad for deconvolution: (y, x), a good number is : (kernel-1)/2, if target_shape set, pad will be ignored and will be computed automatically</li>
<li><code>adj::Shape(tuple), optional, default=(0,0)</code>: adjustment for output shape: (y, x), if target_shape set, adj will be ignored and will be computed automatically</li>
<li><code>target_shape::Shape(tuple), optional, default=(0,0)</code>: output shape with targe shape : (y, x)</li>
<li><code>num_filter::int (non-negative), required</code>: deconvolution filter(channel) number</li>
<li><code>num_group::int (non-negative), optional, default=1</code>: number of groups partition</li>
<li><code>workspace::long (non-negative), optional, default=512</code>: Tmp workspace for deconvolution (MB)</li>
<li><code>no_bias::boolean, optional, default=True</code>: Whether to disable bias parameter.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1107' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.Dropout-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.Dropout-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.Dropout</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>Dropout(data, p)
</code></pre>

<p>Apply dropout to input. During training, each element of the input is randomly set to zero with probability p. And then the whole tensor is rescaled by 1/(1-p) to keep the expectation the same as before applying dropout. During the test time, this behaves as an identity map.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to dropout.</li>
<li><code>p::float, optional, default=0.5</code>: Fraction of the input that gets dropped out at training time</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1091' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.ElementWiseSum-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.ElementWiseSum-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.ElementWiseSum</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>ElementWiseSum(args)
</code></pre>

<p><strong>Note</strong>: ElementWiseSum takes variable number of positional inputs. So instead of calling as ElementWiseSum([x, y, z], num_args=3), one should call via ElementWiseSum(x, y, z), and num_args will be determined automatically.</p>
<p>Perform element sum of inputs</p>
<p>From:src/operator/tensor/elemwise_sum.cc:56</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>args::NDArray[]</code>: List of input tensors</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.Embedding-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.Embedding-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.Embedding</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>Embedding(data, weight, input_dim, output_dim)
</code></pre>

<p>Map integer index to vector representations (embeddings). Those embeddings are learnable parameters. For a input of shape (d1, ..., dK), the output shape is (d1, ..., dK, output_dim). All the input values should be integers in the range [0, input_dim).</p>
<p>From:src/operator/tensor/indexing_op.cc:18</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to the EmbeddingOp.</li>
<li><code>weight::SymbolicNode</code>: Embedding weight matrix.</li>
<li><code>input_dim::int, required</code>: vocabulary size of the input indices.</li>
<li><code>output_dim::int, required</code>: dimension of the embedding vectors.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1093' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.FullyConnected-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.FullyConnected-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.FullyConnected</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>FullyConnected(data, weight, bias, num_hidden, no_bias)
</code></pre>

<p>Apply matrix multiplication to input then add a bias. It maps the input of shape <code>(batch_size, input_dim)</code> to the shape of <code>(batch_size, num_hidden)</code>. Learnable parameters include the weights of the linear transform and an optional bias vector.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to the FullyConnectedOp.</li>
<li><code>weight::SymbolicNode</code>: Weight matrix.</li>
<li><code>bias::SymbolicNode</code>: Bias parameter.</li>
<li><code>num_hidden::int, required</code>: Number of hidden nodes of the output.</li>
<li><code>no_bias::boolean, optional, default=False</code>: Whether to disable bias parameter.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1096' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.IdentityAttachKLSparseReg-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.IdentityAttachKLSparseReg-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.IdentityAttachKLSparseReg</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>IdentityAttachKLSparseReg(data, sparseness_target, penalty, momentum)
</code></pre>

<p>Apply a sparse regularization to the output a sigmoid activation function.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data.</li>
<li><code>sparseness_target::float, optional, default=0.1</code>: The sparseness target</li>
<li><code>penalty::float, optional, default=0.001</code>: The tradeoff parameter for the sparseness penalty</li>
<li><code>momentum::float, optional, default=0.9</code>: The momentum for running average</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1091' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.InstanceNorm-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.InstanceNorm-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.InstanceNorm</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>InstanceNorm(data, gamma, beta, eps)
</code></pre>

<p>An operator taking in a n-dimensional input tensor (n &gt; 2), and normalizing the input by subtracting the mean and variance calculated over the spatial dimensions. This is an implemention of the operator described in "Instance Normalization: The Missing Ingredient for Fast Stylization", D. Ulyanov, A. Vedaldi, V. Lempitsky, 2016 (arXiv:1607.08022v2). This layer is similar to batch normalization, with two differences: first, the normalization is carried out per example ('instance'), not over a batch. Second, the same normalization is applied both at test and train time. This operation is also known as 'contrast normalization'.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: A n-dimensional tensor (n &gt; 2) of the form [batch, channel, spatial_dim1, spatial_dim2, ...].</li>
<li><code>gamma::SymbolicNode</code>: A vector of length 'channel', which multiplies the normalized input.</li>
<li><code>beta::SymbolicNode</code>: A vector of length 'channel', which is added to the product of the normalized input and the weight.</li>
<li><code>eps::float, optional, default=0.001</code>: Epsilon to prevent division by 0.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1091' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.L2Normalization-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.L2Normalization-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.L2Normalization</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>L2Normalization(data, eps, mode)
</code></pre>

<p>Set the l2 norm of each instance to a constant.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to the L2NormalizationOp.</li>
<li><code>eps::float, optional, default=1e-10</code>: Epsilon to prevent div 0</li>
<li><code>mode::{'channel', 'instance', 'spatial'},optional, default='instance'</code>: Normalization Mode. If set to instance, this operator will compute a norm for each instance in the batch; this is the default mode. If set to channel, this operator will compute a cross channel norm at each position of each instance. If set to spatial, this operator will compute a norm for each channel.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.LRN-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.LRN-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.LRN</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>LRN(data, alpha, beta, knorm, nsize)
</code></pre>

<p>Apply convolution to input then add a bias.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to the ConvolutionOp.</li>
<li><code>alpha::float, optional, default=0.0001</code>: value of the alpha variance scaling parameter in the normalization formula</li>
<li><code>beta::float, optional, default=0.75</code>: value of the beta power parameter in the normalization formula</li>
<li><code>knorm::float, optional, default=2</code>: value of the k parameter in normalization formula</li>
<li><code>nsize::int (non-negative), required</code>: normalization window width in elements.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1093' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.LeakyReLU-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.LeakyReLU-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.LeakyReLU</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>LeakyReLU(data, act_type, slope, lower_bound, upper_bound)
</code></pre>

<p>Apply activation function to input.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to activation function.</li>
<li><code>act_type::{'elu', 'leaky', 'prelu', 'rrelu'},optional, default='leaky'</code>: Activation function to be applied.</li>
<li><code>slope::float, optional, default=0.25</code>: Init slope for the activation. (For leaky and elu only)</li>
<li><code>lower_bound::float, optional, default=0.125</code>: Lower bound of random slope. (For rrelu only)</li>
<li><code>upper_bound::float, optional, default=0.334</code>: Upper bound of random slope. (For rrelu only)</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1093' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.LinearRegressionOutput-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.LinearRegressionOutput-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.LinearRegressionOutput</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>LinearRegressionOutput(data, label, grad_scale)
</code></pre>

<p>Use linear regression for final output, this is used on final output of a net.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to function.</li>
<li><code>label::SymbolicNode</code>: Input label to function.</li>
<li><code>grad_scale::float, optional, default=1</code>: Scale the gradient by a float factor</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.LogisticRegressionOutput-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.LogisticRegressionOutput-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.LogisticRegressionOutput</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>LogisticRegressionOutput(data, label, grad_scale)
</code></pre>

<p>Use Logistic regression for final output, this is used on final output of a net. Logistic regression is suitable for binary classification or probability prediction tasks.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to function.</li>
<li><code>label::SymbolicNode</code>: Input label to function.</li>
<li><code>grad_scale::float, optional, default=1</code>: Scale the gradient by a float factor</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1090' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.MAERegressionOutput-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.MAERegressionOutput-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.MAERegressionOutput</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>MAERegressionOutput(data, label, grad_scale)
</code></pre>

<p>Use mean absolute error regression for final output, this is used on final output of a net.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to function.</li>
<li><code>label::SymbolicNode</code>: Input label to function.</li>
<li><code>grad_scale::float, optional, default=1</code>: Scale the gradient by a float factor</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.MakeLoss-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.MakeLoss-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.MakeLoss</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>MakeLoss(data, grad_scale, valid_thresh, normalization)
</code></pre>

<p>Get output from a symbol and pass 1 gradient back. This is used as a terminal loss if unary and binary operator are used to composite a loss with no declaration of backward dependency</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data.</li>
<li><code>grad_scale::float, optional, default=1</code>: gradient scale as a supplement to unary and binary operators</li>
<li><code>valid_thresh::float, optional, default=0</code>: regard element valid when x &gt; valid_thresh, this is used only in valid normalization mode.</li>
<li><code>normalization::{'batch', 'null', 'valid'},optional, default='null'</code>: If set to null, op will not normalize on output gradient.If set to batch, op will normalize gradient by divide batch size.If set to valid, op will normalize gradient by divide # sample marked as valid</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1091' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.Pad-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.Pad-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.Pad</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>Pad(data, mode, pad_width, constant_value)
</code></pre>

<p>Pads an n-dimensional input tensor. Allows for precise control of the padding type and how much padding to apply on both sides of a given dimension.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: An n-dimensional input tensor.</li>
<li><code>mode::{'constant', 'edge'}, required</code>: Padding type to use. "constant" pads all values with a constant value, the value of which can be specified with the constant_value option. "edge" uses the boundary values of the array as padding.</li>
<li><code>pad_width::Shape(tuple), required</code>: A tuple of padding widths of length 2*r, where r is the rank of the input tensor, specifying number of values padded to the edges of each axis. (before_1, after_1, ... , before_N, after_N) unique pad widths for each axis. Equivalent to pad_width in numpy.pad, but flattened.</li>
<li><code>constant_value::double, optional, default=0</code>: This option is only used when mode is "constant". This value will be used as the padding value. Defaults to 0 if not specified.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1091' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.Pooling-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.Pooling-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.Pooling</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>Pooling(data, global_pool, kernel, pool_type, pooling_convention, stride, pad)
</code></pre>

<p>Perform spatial pooling on inputs.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to the pooling operator.</li>
<li><code>global_pool::boolean, optional, default=False</code>: Ignore kernel size, do global pooling based on current input feature map. This is useful for input with different shape</li>
<li><code>kernel::Shape(tuple), required</code>: pooling kernel size: (y, x) or (d, y, x)</li>
<li><code>pool_type::{'avg', 'max', 'sum'}, required</code>: Pooling type to be applied.</li>
<li><code>pooling_convention::{'full', 'valid'},optional, default='valid'</code>: Pooling convention to be applied.kValid is default setting of Mxnet and rounds down the output pooling size.kFull is compatible with Caffe and rounds up the output pooling size.</li>
<li><code>stride::Shape(tuple), optional, default=(1,1)</code>: stride: for pooling (y, x) or (d, y, x)</li>
<li><code>pad::Shape(tuple), optional, default=(0,0)</code>: pad for pooling: (y, x) or (d, y, x)</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1097' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.RNN-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.RNN-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.RNN</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>RNN(data, parameters, state, state_cell, state_size, num_layers, bidirectional, mode, p, state_outputs)
</code></pre>

<p>Apply a recurrent layer to input.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to RNN</li>
<li><code>parameters::SymbolicNode</code>: Vector of all RNN trainable parameters concatenated</li>
<li><code>state::SymbolicNode</code>: initial hidden state of the RNN</li>
<li><code>state_cell::SymbolicNode</code>: initial cell state for LSTM networks (only for LSTM)</li>
<li><code>state_size::int (non-negative), required</code>: size of the state for each layer</li>
<li><code>num_layers::int (non-negative), required</code>: number of stacked layers</li>
<li><code>bidirectional::boolean, optional, default=False</code>: whether to use bidirectional recurrent layers</li>
<li><code>mode::{'gru', 'lstm', 'rnn_relu', 'rnn_tanh'}, required</code>: the type of RNN to compute</li>
<li><code>p::float, optional, default=0</code>: Dropout probability, fraction of the input that gets dropped out at training time</li>
<li><code>state_outputs::boolean, optional, default=False</code>: Whether to have the states as symbol outputs.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1103' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.ROIPooling-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.ROIPooling-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.ROIPooling</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>ROIPooling(data, rois, pooled_size, spatial_scale)
</code></pre>

<p>Performs region-of-interest pooling on inputs. Resize bounding box coordinates by spatial_scale and crop input feature maps accordingly. The cropped feature maps are pooled by max pooling to a fixed size output indicated by pooled_size. batch_size will change to the number of region bounding boxes after ROIPooling</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to the pooling operator, a 4D Feature maps</li>
<li><code>rois::SymbolicNode</code>: Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]]. (x1, y1) and (x2, y2) are top left and down right corners of designated region of interest. batch_index indicates the index of corresponding image in the input data</li>
<li><code>pooled_size::Shape(tuple), required</code>: fix pooled size: (h, w)</li>
<li><code>spatial_scale::float, required</code>: Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1091' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.Reshape-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.Reshape-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.Reshape</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>Reshape(data, target_shape, keep_highest, shape, reverse)
</code></pre>

<p>Reshape input according to a target shape spec. The target shape is a tuple and can be a simple list of dimensions such as (12,3) or it can incorporate special codes that correspond to contextual operations that refer to the input dimensions. The special codes are all expressed as integers less than 1. These codes effectively refer to a machine that pops input dims off the beginning of the input dims list and pushes resulting output dims onto the end of the output dims list, which starts empty. The codes are:   0  Copy     Pop one input dim and push it onto the output dims  -1  Infer    Push a dim that is inferred later from all other output dims  -2  CopyAll  Pop all remaining input dims and push them onto output dims  -3  Merge2   Pop two input dims, multiply them, and push result  -4  Split2   Pop one input dim, and read two next target shape specs,               push them both onto output dims (either can be -1 and will               be inferred from the other  The exact mathematical behavior of these codes is given in the description of the 'shape' parameter. All non-codes (positive integers) just pop a dim off the input dims (if any), throw it away, and then push the specified integer onto the output dims. Examples: Type     Input      Target            Output Copy     (2,3,4)    (4,0,2)           (4,3,2) Copy     (2,3,4)    (2,0,0)           (2,3,4) Infer    (2,3,4)    (6,1,-1)          (6,1,4) Infer    (2,3,4)    (3,-1,8)          (3,1,8) CopyAll  (9,8,7)    (-2)              (9,8,7) CopyAll  (9,8,7)    (9,-2)            (9,8,7) CopyAll  (9,8,7)    (-2,1,1)          (9,8,7,1,1) Merge2   (3,4)      (-3)              (12) Merge2   (3,4,5)    (-3,0)            (12,5) Merge2   (3,4,5)    (0,-3)            (3,20) Merge2   (3,4,5,6)  (-3,0,0)          (12,5,6) Merge2   (3,4,5,6)  (-3,-2)           (12,5,6) Split2   (12)       (-4,6,2)          (6,2) Split2   (12)       (-4,2,6)          (2,6) Split2   (12)       (-4,-1,6)         (2,6) Split2   (12,9)     (-4,2,6,0)        (2,6,9) Split2   (12,9,9,9) (-4,2,6,-2)       (2,6,9,9,9) Split2   (12,12)    (-4,2,-1,-4,-1,2) (2,6,6,2)</p>
<p>From:src/operator/tensor/matrix_op.cc:62</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Input data to reshape.</li>
<li><code>target_shape::Shape(tuple), optional, default=(0,0)</code>: (Deprecated! Use shape instead.) Target new shape. One and only one dim can be 0, in which case it will be inferred from the rest of dims</li>
<li><code>keep_highest::boolean, optional, default=False</code>: (Deprecated! Use shape instead.) Whether keep the highest dim unchanged.If set to true, then the first dim in target_shape is ignored,and always fixed as input</li>
<li><code>shape::Shape(tuple), optional, default=()</code>: Target shape, a tuple, t=(t_1,t_2,..,t_m).</li>
</ul>
<p>Let the input dims be s=(s_1,s_2,..,s_n). The output dims u=(u_1,u_2,..,u_p) are computed from s and t. The target shape tuple elements t_i are read in order, and used to  generate successive output dims u_p: t_i:       meaning:      behavior: +ve        explicit      u_p = t_i 0          copy          u_p = s_i -1         infer         u_p = (Prod s_i) / (Prod u_j | j != p) -2         copy all      u_p = s_i, u_p+1 = s_i+1, ... -3         merge two     u_p = s_i * s_i+1 -4,a,b     split two     u_p = a, u_p+1 = b | a * b = s_i The split directive (-4) in the target shape tuple is followed by two dimensions, one of which can be -1, which means it will be inferred from the other one and the original dimension. The can only be one globally inferred dimension (-1), aside from any -1 occuring in a split directive.</p>
<ul>
<li><code>reverse::boolean, optional, default=False</code>: Whether to match the shapes from the backward. If reverse is true, 0 values in the <code>shape</code> argument will be searched from the backward. E.g the original shape is (10, 5, 4) and the shape argument is (-1, 0). If reverse is true, the new shape should be (50, 4). Otherwise it will be (40, 5).</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1138' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.SVMOutput-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.SVMOutput-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.SVMOutput</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>SVMOutput(data, label, margin, regularization_coefficient, use_linear)
</code></pre>

<p>Support Vector Machine based transformation on input, backprop L2-SVM</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to svm.</li>
<li><code>label::SymbolicNode</code>: Label data.</li>
<li><code>margin::float, optional, default=1</code>: Scale the DType(param_.margin) for activation size</li>
<li><code>regularization_coefficient::float, optional, default=1</code>: Scale the coefficient responsible for balacing coefficient size and error tradeoff</li>
<li><code>use_linear::boolean, optional, default=False</code>: If set true, uses L1-SVM objective function. Default uses L2-SVM objective</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1093' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.SequenceLast-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.SequenceLast-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.SequenceLast</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>SequenceLast(data, sequence_length, use_sequence_length)
</code></pre>

<p>Takes the last element of a sequence. Takes an n-dimensional tensor of the form [max sequence length, batchsize, other dims] and returns a (n-1)-dimensional tensor of the form [batchsize, other dims]. This operator takes an optional input tensor sequence_length of positive ints of dimension [batchsize] when the sequence_length option is set to true. This allows the operator to handle variable-length sequences. If sequence_length is false, then each example in the batch is assumed to have the max sequence length.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: n-dimensional input tensor of the form [max sequence length, batchsize, other dims]</li>
<li><code>sequence_length::SymbolicNode</code>: vector of sequence lengths of size batchsize</li>
<li><code>use_sequence_length::boolean, optional, default=False</code>: If set to true, this layer takes in extra input sequence_length to specify variable length sequence</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.SequenceMask-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.SequenceMask-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.SequenceMask</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>SequenceMask(data, sequence_length, use_sequence_length, value)
</code></pre>

<p>Sets all elements outside the sequence to a constant value. Takes an n-dimensional tensor of the form [max sequence length, batchsize, other dims] and returns a tensor of the same shape. This operator takes an optional input tensor sequence_length of positive ints of dimension [batchsize] when the sequence_length option is set to true. This allows the operator to handle variable-length sequences. If sequence_length is false, then each example in the batch is assumed to have the max sequence length, and this operator becomes the identity operator.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: n-dimensional input tensor of the form [max sequence length, batchsize, other dims]</li>
<li><code>sequence_length::SymbolicNode</code>: vector of sequence lengths of size batchsize</li>
<li><code>use_sequence_length::boolean, optional, default=False</code>: If set to true, this layer takes in extra input sequence_length to specify variable length sequence</li>
<li><code>value::float, optional, default=0</code>: The value to be used as a mask.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1091' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.SequenceReverse-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.SequenceReverse-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.SequenceReverse</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>SequenceReverse(data, sequence_length, use_sequence_length)
</code></pre>

<p>Reverses the elements of each sequence. Takes an n-dimensional tensor of the form [max sequence length, batchsize, other dims] and returns a tensor of the same shape. This operator takes an optional input tensor sequence_length of positive ints of dimension [batchsize] when the sequence_length option is set to true. This allows the operator to handle variable-length sequences. If sequence_length is false, then each example in the batch is assumed to have the max sequence length.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: n-dimensional input tensor of the form [max sequence length, batchsize, other dims]</li>
<li><code>sequence_length::SymbolicNode</code>: vector of sequence lengths of size batchsize</li>
<li><code>use_sequence_length::boolean, optional, default=False</code>: If set to true, this layer takes in extra input sequence_length to specify variable length sequence</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.SliceChannel-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.SliceChannel-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.SliceChannel</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>SliceChannel(num_outputs, axis, squeeze_axis)
</code></pre>

<p>Slice input equally along specified axis</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>num_outputs::int, required</code>: Number of outputs to be sliced.</li>
<li><code>axis::int, optional, default='1'</code>: Dimension along which to slice.</li>
<li><code>squeeze_axis::boolean, optional, default=False</code>: If true AND the sliced dimension becomes 1, squeeze that dimension.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.Softmax-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.Softmax-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.Softmax</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>Softmax(data, grad_scale, ignore_label, multi_output, use_ignore, preserve_shape, normalization, out_grad)
</code></pre>

<p>DEPRECATED: Perform a softmax transformation on input. Please use SoftmaxOutput</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to softmax.</li>
<li><code>grad_scale::float, optional, default=1</code>: Scale the gradient by a float factor</li>
<li><code>ignore_label::float, optional, default=-1</code>: the label value will be ignored during backward (only works if use_ignore is set to be true).</li>
<li><code>multi_output::boolean, optional, default=False</code>: If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n<em>x_1</em>...*x_n output, each has k classes</li>
<li><code>use_ignore::boolean, optional, default=False</code>: If set to true, the ignore_label value will not contribute to the backward gradient</li>
<li><code>preserve_shape::boolean, optional, default=False</code>: If true, for a (n_1, n_2, ..., n_d, k) dimensional input tensor, softmax will generate (n1, n2, ..., n_d, k) output, normalizing the k classes as the last dimension.</li>
<li><code>normalization::{'batch', 'null', 'valid'},optional, default='null'</code>: If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored</li>
<li><code>out_grad::boolean, optional, default=False</code>: Apply weighting from output gradient</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1099' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.SoftmaxActivation-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.SoftmaxActivation-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.SoftmaxActivation</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>SoftmaxActivation(data, mode)
</code></pre>

<p>Apply softmax activation to input. This is intended for internal layers. For output (loss layer) please use SoftmaxOutput. If mode=instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If mode=channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to activation function.</li>
<li><code>mode::{'channel', 'instance'},optional, default='instance'</code>: Softmax Mode. If set to instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If set to channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.SoftmaxOutput-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.SoftmaxOutput-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.SoftmaxOutput</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>SoftmaxOutput(data, label, grad_scale, ignore_label, multi_output, use_ignore, preserve_shape, normalization, out_grad)
</code></pre>

<p>Perform a softmax transformation on input, backprop with logloss.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to softmax.</li>
<li><code>label::SymbolicNode</code>: Label data, can also be probability value with same shape as data</li>
<li><code>grad_scale::float, optional, default=1</code>: Scale the gradient by a float factor</li>
<li><code>ignore_label::float, optional, default=-1</code>: the label value will be ignored during backward (only works if use_ignore is set to be true).</li>
<li><code>multi_output::boolean, optional, default=False</code>: If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n<em>x_1</em>...*x_n output, each has k classes</li>
<li><code>use_ignore::boolean, optional, default=False</code>: If set to true, the ignore_label value will not contribute to the backward gradient</li>
<li><code>preserve_shape::boolean, optional, default=False</code>: If true, for a (n_1, n_2, ..., n_d, k) dimensional input tensor, softmax will generate (n1, n2, ..., n_d, k) output, normalizing the k classes as the last dimension.</li>
<li><code>normalization::{'batch', 'null', 'valid'},optional, default='null'</code>: If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored</li>
<li><code>out_grad::boolean, optional, default=False</code>: Apply weighting from output gradient</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1101' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.SpatialTransformer-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.SpatialTransformer-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.SpatialTransformer</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>SpatialTransformer(data, loc, target_shape, transform_type, sampler_type)
</code></pre>

<p>Apply spatial transformer to input feature map.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to the SpatialTransformerOp.</li>
<li><code>loc::SymbolicNode</code>: localisation net, the output dim should be 6 when transform_type is affine, and the name of loc symbol should better starts with 'stn_loc', so that initialization it with iddentify tranform, or you shold initialize the weight and bias by yourself.</li>
<li><code>target_shape::Shape(tuple), optional, default=(0,0)</code>: output shape(h, w) of spatial transformer: (y, x)</li>
<li><code>transform_type::{'affine'}, required</code>: transformation type</li>
<li><code>sampler_type::{'bilinear'}, required</code>: sampling type</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1093' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.SwapAxis-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.SwapAxis-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.SwapAxis</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>SwapAxis(data, dim1, dim2)
</code></pre>

<p>Apply swapaxis to input.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode</code>: Input data to the SwapAxisOp.</li>
<li><code>dim1::int (non-negative), optional, default=0</code>: the first axis to be swapped.</li>
<li><code>dim2::int (non-negative), optional, default=0</code>: the second axis to be swapped.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.UpSampling-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.UpSampling-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.UpSampling</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>UpSampling(data, scale, num_filter, sample_type, multi_input_mode, num_args, workspace)
</code></pre>

<p><strong>Note</strong>: UpSampling takes variable number of positional inputs. So instead of calling as UpSampling([x, y, z], num_args=3), one should call via UpSampling(x, y, z), and num_args will be determined automatically.</p>
<p>Perform nearest neighboor/bilinear up sampling to inputs</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::SymbolicNode[]</code>: Array of tensors to upsample</li>
<li><code>scale::int (non-negative), required</code>: Up sampling scale</li>
<li><code>num_filter::int (non-negative), optional, default=0</code>: Input filter. Only used by bilinear sample_type.</li>
<li><code>sample_type::{'bilinear', 'nearest'}, required</code>: upsampling method</li>
<li><code>multi_input_mode::{'concat', 'sum'},optional, default='concat'</code>: How to handle multiple input. concat means concatenate upsampled images along the channel dimension. sum means add all images together, only available for nearest neighbor upsampling.</li>
<li><code>num_args::int, required</code>: Number of inputs to be upsampled. For nearest neighbor upsampling, this can be 1-N; the size of output will be(scale<em>h_0,scale</em>w_0) and all other inputs will be upsampled to thesame size. For bilinear upsampling this must be 2; 1 input and 1 weight.</li>
<li><code>workspace::long (non-negative), optional, default=512</code>: Tmp workspace for deconvolution (MB)</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1099' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._CrossDeviceCopy-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._CrossDeviceCopy-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._CrossDeviceCopy</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_CrossDeviceCopy()
</code></pre>

<p>Special op to copy data cross device</p>
<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._Div-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._Div-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._Div</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_Div(lhs, rhs)
</code></pre>

<p>_Div is an alias of _div.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._DivScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._DivScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._DivScalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_DivScalar(data, scalar)
</code></pre>

<p>_DivScalar is an alias of _div_scalar.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._Equal-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._Equal-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._Equal</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_Equal(lhs, rhs)
</code></pre>

<p>_Equal is an alias of _equal.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._EqualScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._EqualScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._EqualScalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_EqualScalar(data, scalar)
</code></pre>

<p>_EqualScalar is an alias of _equal_scalar.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._Greater-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._Greater-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._Greater</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_Greater(lhs, rhs)
</code></pre>

<p>_Greater is an alias of _greater.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._GreaterEqualScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._GreaterEqualScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._GreaterEqualScalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_GreaterEqualScalar(data, scalar)
</code></pre>

<p>_GreaterEqualScalar is an alias of _greater_equal_scalar.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._GreaterScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._GreaterScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._GreaterScalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_GreaterScalar(data, scalar)
</code></pre>

<p>_GreaterScalar is an alias of _greater_scalar.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._Greater_Equal-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._Greater_Equal-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._Greater_Equal</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_Greater_Equal(lhs, rhs)
</code></pre>

<p>_Greater_Equal is an alias of _greater_equal.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._Hypot-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._Hypot-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._Hypot</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_Hypot(lhs, rhs)
</code></pre>

<p>_Hypot is an alias of _hypot.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._HypotScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._HypotScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._HypotScalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_HypotScalar(data, scalar)
</code></pre>

<p>_HypotScalar is an alias of _hypot_scalar.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._Lesser-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._Lesser-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._Lesser</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_Lesser(lhs, rhs)
</code></pre>

<p>_Lesser is an alias of _lesser.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._LesserEqualScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._LesserEqualScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._LesserEqualScalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_LesserEqualScalar(data, scalar)
</code></pre>

<p>_LesserEqualScalar is an alias of _lesser_equal_scalar.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._LesserScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._LesserScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._LesserScalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_LesserScalar(data, scalar)
</code></pre>

<p>_LesserScalar is an alias of _lesser_scalar.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._Lesser_Equal-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._Lesser_Equal-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._Lesser_Equal</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_Lesser_Equal(lhs, rhs)
</code></pre>

<p>_Lesser_Equal is an alias of _lesser_equal.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._Maximum-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._Maximum-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._Maximum</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_Maximum(lhs, rhs)
</code></pre>

<p>_Maximum is an alias of _maximum.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._MaximumScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._MaximumScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._MaximumScalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_MaximumScalar(data, scalar)
</code></pre>

<p>_MaximumScalar is an alias of _maximum_scalar.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._Minimum-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._Minimum-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._Minimum</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_Minimum(lhs, rhs)
</code></pre>

<p>_Minimum is an alias of _minimum.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._MinimumScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._MinimumScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._MinimumScalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_MinimumScalar(data, scalar)
</code></pre>

<p>_MinimumScalar is an alias of _minimum_scalar.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._Minus-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._Minus-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._Minus</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_Minus(lhs, rhs)
</code></pre>

<p>_Minus is an alias of _sub.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._MinusScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._MinusScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._MinusScalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_MinusScalar(data, scalar)
</code></pre>

<p>_MinusScalar is an alias of _minus_scalar.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._Mul-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._Mul-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._Mul</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_Mul(lhs, rhs)
</code></pre>

<p>_Mul is an alias of _mul.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._MulScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._MulScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._MulScalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_MulScalar(data, scalar)
</code></pre>

<p>_MulScalar is an alias of _mul_scalar.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._NDArray-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._NDArray-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._NDArray</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_NDArray(info)
</code></pre>

<p>Stub for implementing an operator implemented in native frontend language with ndarray.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>info::, required</code>:</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1085' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._Native-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._Native-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._Native</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_Native(info, need_top_grad)
</code></pre>

<p>Stub for implementing an operator implemented in native frontend language.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>info::, required</code>:</li>
<li><code>need_top_grad::boolean, optional, default=True</code>: Whether this layer needs out grad for backward. Should be false for loss layers.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._NoGradient-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._NoGradient-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._NoGradient</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_NoGradient()
</code></pre>

<p>Place holder for variable who cannot perform gradient</p>
<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._NotEqualScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._NotEqualScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._NotEqualScalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_NotEqualScalar(data, scalar)
</code></pre>

<p>_NotEqualScalar is an alias of _not_equal_scalar.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._Not_Equal-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._Not_Equal-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._Not_Equal</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_Not_Equal(lhs, rhs)
</code></pre>

<p>_Not_Equal is an alias of _not_equal.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._Plus-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._Plus-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._Plus</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_Plus(lhs, rhs)
</code></pre>

<p>_Plus is an alias of elemwise_add.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._PlusScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._PlusScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._PlusScalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_PlusScalar(data, scalar)
</code></pre>

<p>_PlusScalar is an alias of _plus_scalar.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._Power-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._Power-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._Power</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_Power(lhs, rhs)
</code></pre>

<p>_Power is an alias of _power.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._PowerScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._PowerScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._PowerScalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_PowerScalar(data, scalar)
</code></pre>

<p>_PowerScalar is an alias of _power_scalar.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._RDivScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._RDivScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._RDivScalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_RDivScalar(data, scalar)
</code></pre>

<p>_RDivScalar is an alias of _rdiv_scalar.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._RMinusScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._RMinusScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._RMinusScalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_RMinusScalar(data, scalar)
</code></pre>

<p>_RMinusScalar is an alias of _rminus_scalar.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._RPowerScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._RPowerScalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._RPowerScalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_RPowerScalar(data, scalar)
</code></pre>

<p>_RPowerScalar is an alias of _rpower_scalar.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._add-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._add-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._add</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_add(lhs, rhs)
</code></pre>

<p>_add is an alias of elemwise_add.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._arange-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._arange-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._arange</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_arange(start, stop, step, repeat, ctx, dtype)
</code></pre>

<p>Return evenly spaced values within a given interval. Similar to Numpy</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>start::float, required</code>: Start of interval. The interval includes this value. The default start value is 0.</li>
<li><code>stop::, optional, default=None</code>: End of interval. The interval does not include this value, except in some cases where step is not an integer and floating point round-off affects the length of out.</li>
<li><code>step::float, optional, default=1</code>: Spacing between values.</li>
<li><code>repeat::int, optional, default='1'</code>: The repeating time of all elements. E.g repeat=3, the element a will be repeated three times &gt; a, a, a.</li>
<li><code>ctx::string, optional, default=''</code>: Context of output, in format <a href="../n">cpu|gpu|cpu_pinned</a>.Only used for imperative calls.</li>
<li><code>dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32'</code>: Target data type.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1095' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_Activation-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_Activation-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_Activation</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_Activation()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_BatchNorm-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_BatchNorm-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_BatchNorm</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_BatchNorm()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_Cast-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_Cast-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_Cast</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_Cast()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_Concat-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_Concat-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_Concat</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_Concat()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_Convolution-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_Convolution-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_Convolution</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_Convolution()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_Correlation-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_Correlation-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_Correlation</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_Correlation()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_Crop-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_Crop-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_Crop</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_Crop()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_Custom-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_Custom-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_Custom</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_Custom()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_Deconvolution-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_Deconvolution-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_Deconvolution</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_Deconvolution()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_Dropout-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_Dropout-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_Dropout</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_Dropout()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_Embedding-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_Embedding-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_Embedding</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_Embedding()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_FullyConnected-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_FullyConnected-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_FullyConnected</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_FullyConnected()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_IdentityAttachKLSparseReg-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_IdentityAttachKLSparseReg-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_IdentityAttachKLSparseReg</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_IdentityAttachKLSparseReg()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_InstanceNorm-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_InstanceNorm-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_InstanceNorm</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_InstanceNorm()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_L2Normalization-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_L2Normalization-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_L2Normalization</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_L2Normalization()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_LRN-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_LRN-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_LRN</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_LRN()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_LeakyReLU-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_LeakyReLU-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_LeakyReLU</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_LeakyReLU()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_LinearRegressionOutput-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_LinearRegressionOutput-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_LinearRegressionOutput</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_LinearRegressionOutput()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_LogisticRegressionOutput-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_LogisticRegressionOutput-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_LogisticRegressionOutput</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_LogisticRegressionOutput()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_MAERegressionOutput-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_MAERegressionOutput-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_MAERegressionOutput</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_MAERegressionOutput()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_MakeLoss-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_MakeLoss-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_MakeLoss</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_MakeLoss()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_Pad-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_Pad-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_Pad</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_Pad()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_Pooling-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_Pooling-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_Pooling</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_Pooling()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_RNN-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_RNN-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_RNN</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_RNN()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_ROIPooling-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_ROIPooling-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_ROIPooling</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_ROIPooling()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_SVMOutput-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_SVMOutput-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_SVMOutput</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_SVMOutput()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_SequenceLast-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_SequenceLast-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_SequenceLast</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_SequenceLast()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_SequenceMask-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_SequenceMask-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_SequenceMask</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_SequenceMask()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_SequenceReverse-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_SequenceReverse-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_SequenceReverse</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_SequenceReverse()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_SliceChannel-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_SliceChannel-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_SliceChannel</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_SliceChannel()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_Softmax-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_Softmax-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_Softmax</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_Softmax()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_SoftmaxActivation-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_SoftmaxActivation-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_SoftmaxActivation</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_SoftmaxActivation()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_SoftmaxOutput-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_SoftmaxOutput-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_SoftmaxOutput</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_SoftmaxOutput()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_SpatialTransformer-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_SpatialTransformer-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_SpatialTransformer</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_SpatialTransformer()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_SwapAxis-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_SwapAxis-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_SwapAxis</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_SwapAxis()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_UpSampling-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_UpSampling-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_UpSampling</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_UpSampling()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward__CrossDeviceCopy-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward__CrossDeviceCopy-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward__CrossDeviceCopy</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward__CrossDeviceCopy()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward__NDArray-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward__NDArray-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward__NDArray</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward__NDArray()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward__Native-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward__Native-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward__Native</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward__Native()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_abs-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_abs-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_abs</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_abs(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_add-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_add-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_add</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_add()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_arccos-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_arccos-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_arccos</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_arccos(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_arccosh-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_arccosh-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_arccosh</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_arccosh(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_arcsin-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_arcsin-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_arcsin</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_arcsin(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_arcsinh-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_arcsinh-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_arcsinh</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_arcsinh(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_arctan-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_arctan-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_arctan</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_arctan(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_arctanh-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_arctanh-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_arctanh</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_arctanh(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_batch_dot-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_batch_dot-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_batch_dot</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_batch_dot()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_broadcast_add-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_broadcast_add-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_broadcast_add</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_broadcast_add()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_broadcast_div-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_broadcast_div-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_broadcast_div</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_broadcast_div()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_broadcast_hypot-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_broadcast_hypot-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_broadcast_hypot</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_broadcast_hypot()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_broadcast_maximum-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_broadcast_maximum-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_broadcast_maximum</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_broadcast_maximum()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_broadcast_minimum-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_broadcast_minimum-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_broadcast_minimum</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_broadcast_minimum()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_broadcast_mul-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_broadcast_mul-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_broadcast_mul</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_broadcast_mul()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_broadcast_power-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_broadcast_power-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_broadcast_power</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_broadcast_power()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_broadcast_sub-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_broadcast_sub-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_broadcast_sub</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_broadcast_sub()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_clip-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_clip-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_clip</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_clip()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_copy-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_copy-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_copy</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_copy()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_cos-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_cos-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_cos</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_cos(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_cosh-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_cosh-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_cosh</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_cosh(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_degrees-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_degrees-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_degrees</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_degrees(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_div-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_div-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_div</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_div()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_dot-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_dot-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_dot</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_dot(transpose_a, transpose_b)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>transpose_a::boolean, optional, default=False</code>: True if the first matrix is transposed.</li>
<li><code>transpose_b::boolean, optional, default=False</code>: True if the second matrix is tranposed.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_expm1-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_expm1-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_expm1</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_expm1(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_gamma-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_gamma-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_gamma</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_gamma(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_gammaln-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_gammaln-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_gammaln</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_gammaln(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_hypot-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_hypot-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_hypot</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_hypot()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_hypot_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_hypot_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_hypot_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_hypot_scalar(lhs, rhs, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
<li><code>scalar::float</code>: scalar value</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_log-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_log-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_log</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_log(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_log1p-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_log1p-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_log1p</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_log1p(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_max-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_max-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_max</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_max()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_maximum-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_maximum-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_maximum</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_maximum()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_maximum_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_maximum_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_maximum_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_maximum_scalar(lhs, rhs, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
<li><code>scalar::float</code>: scalar value</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_mean-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_mean-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_mean</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_mean()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_min-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_min-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_min</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_min()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_minimum-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_minimum-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_minimum</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_minimum()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_minimum_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_minimum_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_minimum_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_minimum_scalar(lhs, rhs, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
<li><code>scalar::float</code>: scalar value</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_mul-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_mul-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_mul</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_mul()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_nanprod-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_nanprod-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_nanprod</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_nanprod()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_nansum-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_nansum-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_nansum</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_nansum()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_power-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_power-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_power</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_power()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_power_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_power_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_power_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_power_scalar(lhs, rhs, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
<li><code>scalar::float</code>: scalar value</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_prod-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_prod-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_prod</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_prod()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_radians-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_radians-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_radians</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_radians(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_rdiv_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_rdiv_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_rdiv_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_rdiv_scalar(lhs, rhs, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
<li><code>scalar::float</code>: scalar value</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_rpower_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_rpower_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_rpower_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_rpower_scalar(lhs, rhs, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
<li><code>scalar::float</code>: scalar value</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_rsqrt-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_rsqrt-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_rsqrt</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_rsqrt(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_sign-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_sign-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_sign</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_sign(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_sin-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_sin-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_sin</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_sin(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_sinh-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_sinh-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_sinh</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_sinh(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_slice_axis-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_slice_axis-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_slice_axis</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_slice_axis()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_smooth_l1-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_smooth_l1-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_smooth_l1</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_smooth_l1(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_softmax_cross_entropy-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_softmax_cross_entropy-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_softmax_cross_entropy</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_softmax_cross_entropy()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_sqrt-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_sqrt-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_sqrt</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_sqrt(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_square-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_square-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_square</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_square(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_sub-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_sub-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_sub</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_sub()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_sum-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_sum-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_sum</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_sum()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_take-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_take-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_take</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_take()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_tan-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_tan-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_tan</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_tan(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_tanh-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_tanh-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_tanh</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_tanh(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._backward_topk-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._backward_topk-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._backward_topk</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_backward_topk()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._broadcast-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._broadcast-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._broadcast</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_broadcast(src, axis, size)
</code></pre>

<p>Broadcast array in the given axis to the given size</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>src::NDArray</code>: source ndarray</li>
<li><code>axis::int</code>: axis to broadcast</li>
<li><code>size::int</code>: size of broadcast</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._broadcast_backward-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._broadcast_backward-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._broadcast_backward</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_broadcast_backward()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._copy-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._copy-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._copy</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_copy(data)
</code></pre>

<p>Identity mapping, copy src to output</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:14</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._copyto-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._copyto-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._copyto</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_copyto(src)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>src::NDArray</code>: Source input to the function.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1085' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._crop_assign-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._crop_assign-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._crop_assign</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_crop_assign(lhs, rhs, begin, end)
</code></pre>

<p>(Assign the rhs to a cropped subset of lhs.</p>
<p><strong>Requirements</strong></p>
<ul>
<li>output should be explicitly given and be the same as lhs.</li>
<li>lhs and rhs are of the same data type, and on the same device.</li>
</ul>
<p>)</p>
<p>From:src/operator/tensor/matrix_op.cc:159</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: Source input</li>
<li><code>rhs::NDArray</code>: value to assign</li>
<li><code>begin::Shape(tuple), required</code>: starting coordinates</li>
<li><code>end::Shape(tuple), required</code>: ending coordinates</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1099' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._crop_assign_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._crop_assign_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._crop_assign_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_crop_assign_scalar(data, scalar, begin, end)
</code></pre>

<p>(Assign the scalar to a cropped subset of the input.</p>
<p><strong>Requirements</strong></p>
<ul>
<li>output should be explicitly given and be the same as input</li>
</ul>
<p>)</p>
<p>From:src/operator/tensor/matrix_op.cc:183</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
<li><code>scalar::float, optional, default=0</code>: The scalar value for assignment.</li>
<li><code>begin::Shape(tuple), required</code>: starting coordinates</li>
<li><code>end::Shape(tuple), required</code>: ending coordinates</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1098' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._cvcopyMakeBorder-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._cvcopyMakeBorder-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._cvcopyMakeBorder</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_cvcopyMakeBorder(src, top, bot, left, right, type, value)
</code></pre>

<p>Pad image border with OpenCV. </p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>src::NDArray</code>: source image</li>
<li><code>top::int, required</code>: Top margin.</li>
<li><code>bot::int, required</code>: Bottom margin.</li>
<li><code>left::int, required</code>: Left margin.</li>
<li><code>right::int, required</code>: Right margin.</li>
<li><code>type::int, optional, default='0'</code>: Filling type (default=cv2.BORDER_CONSTANT).</li>
<li><code>value::double, optional, default=0</code>: Fill with value.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1098' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._cvimdecode-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._cvimdecode-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._cvimdecode</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_cvimdecode(buf, flag, to_rgb)
</code></pre>

<p>Decode image with OpenCV.  Note: return image in RGB by default, instead of OpenCV's default BGR.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>buf::NDArray</code>: Buffer containing binary encoded image</li>
<li><code>flag::int, optional, default='1'</code>: Convert decoded image to grayscale (0) or color (1).</li>
<li><code>to_rgb::boolean, optional, default=True</code>: Whether to convert decoded image to mxnet's default RGB format (instead of opencv's default BGR).</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1090' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._cvimresize-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._cvimresize-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._cvimresize</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_cvimresize(src, w, h, interp)
</code></pre>

<p>Resize image with OpenCV. </p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>src::NDArray</code>: source image</li>
<li><code>w::int, required</code>: Width of resized image.</li>
<li><code>h::int, required</code>: Height of resized image.</li>
<li><code>interp::int, optional, default='1'</code>: Interpolation method (default=cv2.INTER_LINEAR).</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1092' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._div_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._div_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._div_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_div_scalar(data, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._equal-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._equal-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._equal</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_equal(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._equal_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._equal_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._equal_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_equal_scalar(data, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._get_ndarray_function_def-Tuple{String}' href='#MXNet.mx._get_ndarray_function_def-Tuple{String}'>#</a>
<strong><code>MXNet.mx._get_ndarray_function_def</code></strong> &mdash; <em>Method</em>.</p>
<p>The libxmnet APIs are automatically imported from <code>libmxnet.so</code>. The functions listed here operate on <code>NDArray</code> objects. The arguments to the functions are typically ordered as</p>
<pre><code class="julia">  func_name(arg_in1, arg_in2, ..., scalar1, scalar2, ..., arg_out1, arg_out2, ...)
</code></pre>

<p>unless <code>NDARRAY_ARG_BEFORE_SCALAR</code> is not set. In this case, the scalars are put before the input arguments:</p>
<pre><code class="julia">  func_name(scalar1, scalar2, ..., arg_in1, arg_in2, ..., arg_out1, arg_out2, ...)
</code></pre>

<p>If <code>ACCEPT_EMPTY_MUTATE_TARGET</code> is set. An overloaded function without the output arguments will also be defined:</p>
<pre><code class="julia">  func_name(arg_in1, arg_in2, ..., scalar1, scalar2, ...)
</code></pre>

<p>Upon calling, the output arguments will be automatically initialized with empty NDArrays.</p>
<p>Those functions always return the output arguments. If there is only one output (the typical situation), that object (<code>NDArray</code>) is returned. Otherwise, a tuple containing all the outputs will be returned.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L948-L973' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._grad_add-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._grad_add-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._grad_add</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_grad_add(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._greater-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._greater-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._greater</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_greater(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._greater_equal-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._greater_equal-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._greater_equal</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_greater_equal(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._greater_equal_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._greater_equal_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._greater_equal_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_greater_equal_scalar(data, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._greater_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._greater_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._greater_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_greater_scalar(data, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._hypot-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._hypot-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._hypot</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_hypot(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._hypot_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._hypot_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._hypot_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_hypot_scalar(data, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._identity_with_attr_like_rhs-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._identity_with_attr_like_rhs-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._identity_with_attr_like_rhs</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_identity_with_attr_like_rhs()
</code></pre>

<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._imdecode-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._imdecode-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._imdecode</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_imdecode(mean, index, x0, y0, x1, y1, c, size)
</code></pre>

<p>Decode an image, clip to (x0, y0, x1, y1), subtract mean, and write to buffer</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>mean::NDArray</code>: image mean</li>
<li><code>index::int</code>: buffer position for output</li>
<li><code>x0::int</code>: x0</li>
<li><code>y0::int</code>: y0</li>
<li><code>x1::int</code>: x1</li>
<li><code>y1::int</code>: y1</li>
<li><code>c::int</code>: channel</li>
<li><code>size::int</code>: length of str_img</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1099' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._lesser-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._lesser-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._lesser</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_lesser(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._lesser_equal-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._lesser_equal-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._lesser_equal</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_lesser_equal(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._lesser_equal_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._lesser_equal_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._lesser_equal_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_lesser_equal_scalar(data, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._lesser_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._lesser_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._lesser_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_lesser_scalar(data, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._maximum-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._maximum-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._maximum</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_maximum(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._maximum_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._maximum_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._maximum_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_maximum_scalar(data, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._minimum-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._minimum-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._minimum</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_minimum(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._minimum_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._minimum_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._minimum_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_minimum_scalar(data, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._minus-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._minus-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._minus</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_minus(lhs, rhs)
</code></pre>

<p>_minus is an alias of _sub.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._minus_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._minus_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._minus_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_minus_scalar(data, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._mul-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._mul-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._mul</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_mul(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._mul_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._mul_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._mul_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_mul_scalar(data, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._not_equal-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._not_equal-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._not_equal</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_not_equal(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._not_equal_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._not_equal_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._not_equal_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_not_equal_scalar(data, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._onehot_encode-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._onehot_encode-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._onehot_encode</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_onehot_encode(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: Left operand to the function.</li>
<li><code>rhs::NDArray</code>: Right operand to the function.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._ones-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._ones-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._ones</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_ones(shape, ctx, dtype)
</code></pre>

<p>fill target with ones</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>shape::Shape(tuple), optional, default=()</code>: The shape of the output</li>
<li><code>ctx::string, optional, default=''</code>: Context of output, in format <a href="../n">cpu|gpu|cpu_pinned</a>.Only used for imperative calls.</li>
<li><code>dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32'</code>: Target data type.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._plus-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._plus-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._plus</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_plus(lhs, rhs)
</code></pre>

<p>_plus is an alias of elemwise_add.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._plus_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._plus_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._plus_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_plus_scalar(data, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._power-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._power-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._power</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_power(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._power_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._power_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._power_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_power_scalar(data, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._rdiv_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._rdiv_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._rdiv_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_rdiv_scalar(data, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._rminus_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._rminus_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._rminus_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_rminus_scalar(data, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._rpower_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._rpower_scalar-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._rpower_scalar</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_rpower_scalar(data, scalar)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._sample_normal-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._sample_normal-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._sample_normal</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_sample_normal(loc, scale, shape, ctx, dtype)
</code></pre>

<p>_sample_normal is an alias of normal.</p>
<p>Sample a normal distribution</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>loc::float, optional, default=0</code>: Mean of the distribution.</li>
<li><code>scale::float, optional, default=1</code>: Standard deviation of the distribution.</li>
<li><code>shape::Shape(tuple), optional, default=()</code>: The shape of the output</li>
<li><code>ctx::string, optional, default=''</code>: Context of output, in format <a href="../n">cpu|gpu|cpu_pinned</a>.Only used for imperative calls.</li>
<li><code>dtype::{'float32'},optional, default='float32'</code>: DType of the output</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1095' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._sample_uniform-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._sample_uniform-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._sample_uniform</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_sample_uniform(low, high, shape, ctx, dtype)
</code></pre>

<p>_sample_uniform is an alias of uniform.</p>
<p>Sample a uniform distribution</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>low::float, optional, default=0</code>: The lower bound of distribution</li>
<li><code>high::float, optional, default=1</code>: The upper bound of distribution</li>
<li><code>shape::Shape(tuple), optional, default=()</code>: The shape of the output</li>
<li><code>ctx::string, optional, default=''</code>: Context of output, in format <a href="../n">cpu|gpu|cpu_pinned</a>.Only used for imperative calls.</li>
<li><code>dtype::{'float32'},optional, default='float32'</code>: DType of the output</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1095' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._set_value-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._set_value-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._set_value</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_set_value(src)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>src::real_t</code>: Source input to the function.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1085' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx._zeros-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx._zeros-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx._zeros</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>_zeros(shape, ctx, dtype)
</code></pre>

<p>fill target with zeros</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>shape::Shape(tuple), optional, default=()</code>: The shape of the output</li>
<li><code>ctx::string, optional, default=''</code>: Context of output, in format <a href="../n">cpu|gpu|cpu_pinned</a>.Only used for imperative calls.</li>
<li><code>dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32'</code>: Target data type.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.adam_update-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.adam_update-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.adam_update</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>adam_update()
</code></pre>

<p>Updater function for adam optimizer</p>
<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.add_to!-Tuple{MXNet.mx.NDArray,Vararg{Union{MXNet.mx.NDArray,Real},N}}' href='#MXNet.mx.add_to!-Tuple{MXNet.mx.NDArray,Vararg{Union{MXNet.mx.NDArray,Real},N}}'>#</a>
<strong><code>MXNet.mx.add_to!</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>add_to!(dst :: NDArray, args :: Union{Real, NDArray}...)
</code></pre>

<p>Add a bunch of arguments into <code>dst</code>. Inplace updating.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L518-L522' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.arccos-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.arccos-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.arccos</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>arccos(data)
</code></pre>

<p>Take arccos of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:216</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.arccosh-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.arccosh-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.arccosh</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>arccosh(data)
</code></pre>

<p>Take arccosh of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:288</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.arcsin-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.arcsin-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.arcsin</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>arcsin(data)
</code></pre>

<p>Take arcsin of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:207</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.arcsinh-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.arcsinh-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.arcsinh</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>arcsinh(data)
</code></pre>

<p>Take arcsinh of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:279</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.arctan-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.arctan-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.arctan</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>arctan(data)
</code></pre>

<p>Take arctan of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:225</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.arctanh-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.arctanh-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.arctanh</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>arctanh(data)
</code></pre>

<p>Take arctanh of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:297</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.argmax-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.argmax-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.argmax</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>argmax(data, axis, keepdims)
</code></pre>

<p>Compute argmax</p>
<p>From:src/operator/tensor/broadcast_reduce_op_index.cc:11</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
<li><code>axis::int, optional, default='-1'</code>: Empty or unsigned. The axis to perform the reduction.If left empty, a global reduction will be performed.</li>
<li><code>keepdims::boolean, optional, default=False</code>: If true, the axis which is reduced is left in the result as dimension with size one.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1091' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.argmax_channel-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.argmax_channel-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.argmax_channel</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>argmax_channel(src)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>src::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1085' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.argmin-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.argmin-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.argmin</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>argmin(data, axis, keepdims)
</code></pre>

<p>Compute argmin</p>
<p>From:src/operator/tensor/broadcast_reduce_op_index.cc:16</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
<li><code>axis::int, optional, default='-1'</code>: Empty or unsigned. The axis to perform the reduction.If left empty, a global reduction will be performed.</li>
<li><code>keepdims::boolean, optional, default=False</code>: If true, the axis which is reduced is left in the result as dimension with size one.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1091' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.argsort-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.argsort-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.argsort</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>argsort(src, axis, is_ascend)
</code></pre>

<p>Returns the indices that would sort an array.</p>
<p>From:src/operator/tensor/ordering_op.cc:89</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>src::NDArray</code>: Source input</li>
<li><code>axis::int or None, optional, default='-1'</code>: Axis along which to sort the input tensor. If not given, the flattened array is used. Default is -1.</li>
<li><code>is_ascend::boolean, optional, default=True</code>: Whether sort in ascending or descending order.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1091' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.batch_dot-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.batch_dot-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.batch_dot</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>batch_dot(lhs, rhs, transpose_a, transpose_b)
</code></pre>

<p>Calculate batched dot product of two matrices. (batch, M, K) X (batch, K, N) &gt; (batch, M, N).</p>
<p>From:src/operator/tensor/matrix_op.cc:257</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: Left input</li>
<li><code>rhs::NDArray</code>: Right input</li>
<li><code>transpose_a::boolean, optional, default=False</code>: True if the first matrix is transposed.</li>
<li><code>transpose_b::boolean, optional, default=False</code>: True if the second matrix is tranposed.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1093' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.batch_take-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.batch_take-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.batch_take</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>batch_take(a, indices)
</code></pre>

<p>Take scalar value from a batch of data vectos according to an index vector, i.e. out[i] = a[i, indices[i]]</p>
<p>From:src/operator/tensor/indexing_op.cc:97</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>a::NDArray</code>: Input data array</li>
<li><code>indices::NDArray</code>: index array</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.broadcast_add-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.broadcast_add-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.broadcast_add</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>broadcast_add(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.broadcast_axis-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.broadcast_axis-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.broadcast_axis</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>broadcast_axis(data, axis, size)
</code></pre>

<p>Broadcast src along axis</p>
<p>From:src/operator/tensor/broadcast_reduce_op_value.cc:85</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
<li><code>axis::Shape(tuple), optional, default=()</code>: The axes to perform the broadcasting.</li>
<li><code>size::Shape(tuple), optional, default=()</code>: Target sizes of the broadcasting axes.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1091' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.broadcast_div-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.broadcast_div-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.broadcast_div</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>broadcast_div(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.broadcast_equal-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.broadcast_equal-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.broadcast_equal</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>broadcast_equal(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.broadcast_greater-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.broadcast_greater-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.broadcast_greater</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>broadcast_greater(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.broadcast_greater_equal-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.broadcast_greater_equal-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.broadcast_greater_equal</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>broadcast_greater_equal(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.broadcast_hypot-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.broadcast_hypot-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.broadcast_hypot</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>broadcast_hypot(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.broadcast_lesser-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.broadcast_lesser-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.broadcast_lesser</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>broadcast_lesser(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.broadcast_lesser_equal-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.broadcast_lesser_equal-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.broadcast_lesser_equal</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>broadcast_lesser_equal(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.broadcast_maximum-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.broadcast_maximum-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.broadcast_maximum</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>broadcast_maximum(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.broadcast_minimum-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.broadcast_minimum-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.broadcast_minimum</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>broadcast_minimum(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.broadcast_minus-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.broadcast_minus-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.broadcast_minus</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>broadcast_minus(lhs, rhs)
</code></pre>

<p>broadcast_minus is an alias of broadcast_sub.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.broadcast_mul-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.broadcast_mul-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.broadcast_mul</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>broadcast_mul(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.broadcast_not_equal-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.broadcast_not_equal-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.broadcast_not_equal</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>broadcast_not_equal(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.broadcast_plus-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.broadcast_plus-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.broadcast_plus</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>broadcast_plus(lhs, rhs)
</code></pre>

<p>broadcast_plus is an alias of broadcast_add.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.broadcast_power-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.broadcast_power-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.broadcast_power</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>broadcast_power(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.broadcast_sub-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.broadcast_sub-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.broadcast_sub</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>broadcast_sub(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.broadcast_to-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.broadcast_to-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.broadcast_to</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>broadcast_to(data, shape)
</code></pre>

<p>Broadcast src to shape</p>
<p>From:src/operator/tensor/broadcast_reduce_op_value.cc:92</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
<li><code>shape::Shape(tuple), optional, default=()</code>: The shape of the desired array. We can set the dim to zero if it's same as the original. E.g <code>A = broadcast_to(B, shape=(10, 0, 0))</code> has the same meaning as <code>A = broadcast_axis(B, axis=0, size=10)</code>.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.choose_element_0index-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.choose_element_0index-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.choose_element_0index</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>choose_element_0index(lhs, rhs)
</code></pre>

<p>Choose one element from each line(row for python, column for R/Julia) in lhs according to index indicated by rhs. This function assume rhs uses 0-based index.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: Left operand to the function.</li>
<li><code>rhs::NDArray</code>: Right operand to the function.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.clip-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.clip-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.clip</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>clip(data, a_min, a_max)
</code></pre>

<p>Clip ndarray elements to range (a_min, a_max)</p>
<p>From:src/operator/tensor/matrix_op.cc:289</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
<li><code>a_min::float, required</code>: Minimum value</li>
<li><code>a_max::float, required</code>: Maximum value</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1091' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.context-Tuple{MXNet.mx.NDArray}' href='#MXNet.mx.context-Tuple{MXNet.mx.NDArray}'>#</a>
<strong><code>MXNet.mx.context</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>context(arr :: NDArray)
</code></pre>

<p>Get the context that this <code>NDArray</code> lives on.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L110-L114' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.crop-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.crop-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.crop</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>crop(data, begin, end)
</code></pre>

<p>(Crop the input tensor and return a new one.</p>
<p><strong>Requirements</strong></p>
<ul>
<li>the input and output (if explicitly given) are of the same data type, and on the same device.</li>
</ul>
<p>)</p>
<p>From:src/operator/tensor/matrix_op.cc:143</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
<li><code>begin::Shape(tuple), required</code>: starting coordinates</li>
<li><code>end::Shape(tuple), required</code>: ending coordinates</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1097' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.degrees-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.degrees-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.degrees</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>degrees(data)
</code></pre>

<p>Take degrees of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:234</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.div_from!-Tuple{MXNet.mx.NDArray,Union{MXNet.mx.NDArray,Real}}' href='#MXNet.mx.div_from!-Tuple{MXNet.mx.NDArray,Union{MXNet.mx.NDArray,Real}}'>#</a>
<strong><code>MXNet.mx.div_from!</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>div_from!(dst :: NDArray, arg :: Union{Real, NDArray})
</code></pre>

<p>Elementwise divide a scalar or an <code>NDArray</code> of the same shape from <code>dst</code>. Inplace updating.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L648-L652' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.elemwise_add-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.elemwise_add-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.elemwise_add</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>elemwise_add(lhs, rhs)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: first input</li>
<li><code>rhs::NDArray</code>: second input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.empty-Tuple{Tuple{Vararg{Int64,N}}}' href='#MXNet.mx.empty-Tuple{Tuple{Vararg{Int64,N}}}'>#</a>
<strong><code>MXNet.mx.empty</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>empty(shape :: Tuple, ctx :: Context)
empty(shape :: Tuple)
empty(dim1, dim2, ...)
</code></pre>

<p>Allocate memory for an uninitialized <code>NDArray</code> with specific shape of type Float32.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L141-L147' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.empty-Tuple{Type{T<:Union{Float16,Float32,Float64,Int32,UInt8}},Tuple{Vararg{Int64,N}}}' href='#MXNet.mx.empty-Tuple{Type{T<:Union{Float16,Float32,Float64,Int32,UInt8}},Tuple{Vararg{Int64,N}}}'>#</a>
<strong><code>MXNet.mx.empty</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>empty(DType, shape :: Tuple, ctx :: Context)
empty(DType, shape :: Tuple)
empty(DType, dim1, dim2, ...)
</code></pre>

<p>Allocate memory for an uninitialized <code>NDArray</code> with a specified type.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L124-L130' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.expand_dims-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.expand_dims-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.expand_dims</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>expand_dims(data, axis)
</code></pre>

<p>Expand the shape of array by inserting a new axis.</p>
<p>From:src/operator/tensor/matrix_op.cc:122</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
<li><code>axis::int (non-negative), required</code>: Position (amongst axes) where new axis is to be inserted.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.fill_element_0index-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.fill_element_0index-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.fill_element_0index</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>fill_element_0index(lhs, mhs, rhs)
</code></pre>

<p>Fill one element of each line(row for python, column for R/Julia) in lhs according to index indicated by rhs and values indicated by mhs. This function assume rhs uses 0-based index.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>lhs::NDArray</code>: Left operand to the function.</li>
<li><code>mhs::NDArray</code>: Middle operand to the function.</li>
<li><code>rhs::NDArray</code>: Right operand to the function.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.fix-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.fix-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.fix</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>fix(data)
</code></pre>

<p>Take round of the src to integer nearest 0</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:102</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.flip-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.flip-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.flip</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>flip(data, axis)
</code></pre>

<p>Flip the input tensor along axis and return a new one.</p>
<p>From:src/operator/tensor/matrix_op.cc:219</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
<li><code>axis::int, required</code>: The dimension to flip</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.gammaln-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.gammaln-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.gammaln</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>gammaln(data)
</code></pre>

<p>Take gammaln (log of the absolute value of gamma(x)) of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:315</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.is_shared-Tuple{Array,MXNet.mx.NDArray}' href='#MXNet.mx.is_shared-Tuple{Array,MXNet.mx.NDArray}'>#</a>
<strong><code>MXNet.mx.is_shared</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>is_shared(j_arr, arr)
</code></pre>

<p>Test whether <code>j_arr</code> is sharing data with <code>arr</code>.</p>
<p><strong>Arguments:</strong></p>
<ul>
<li>Array j_arr: the Julia Array.</li>
<li>NDArray arr: the <code>NDArray</code>.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L834-L842' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.load-Tuple{AbstractString,Type{MXNet.mx.NDArray}}' href='#MXNet.mx.load-Tuple{AbstractString,Type{MXNet.mx.NDArray}}'>#</a>
<strong><code>MXNet.mx.load</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>load(filename, ::Type{NDArray})
</code></pre>

<p>Load NDArrays from binary file.</p>
<p><strong>Arguments:</strong></p>
<ul>
<li><code>filename::String</code>: the path of the file to load. It could be S3 or HDFS address.</li>
</ul>
<p>Returns either <code>Dict{Symbol, NDArray}</code> or <code>Vector{NDArray}</code>.</p>
<p><code>filename</code> can point to <code>s3</code> or <code>hdfs</code> resources if the <code>libmxnet</code> is built with the corresponding components enabled. Examples:</p>
<ul>
<li><code>s3://my-bucket/path/my-s3-ndarray</code></li>
<li><code>hdfs://my-bucket/path/my-hdfs-ndarray</code></li>
<li><code>/path-to/my-local-ndarray</code></li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L856-L871' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.max_axis-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.max_axis-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.max_axis</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>max_axis(data, axis, keepdims)
</code></pre>

<p>max_axis is an alias of max.</p>
<p>Compute max along axis. If axis is empty, global reduction is performed</p>
<p>From:src/operator/tensor/broadcast_reduce_op_value.cc:66</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
<li><code>axis::Shape(tuple), optional, default=()</code>: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.</li>
<li><code>keepdims::boolean, optional, default=False</code>: If true, the axis which is reduced is left in the result as dimension with size one.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1093' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.min_axis-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.min_axis-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.min_axis</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>min_axis(data, axis, keepdims)
</code></pre>

<p>min_axis is an alias of min.</p>
<p>Compute min along axis. If axis is empty, global reduction is performed</p>
<p>From:src/operator/tensor/broadcast_reduce_op_value.cc:76</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
<li><code>axis::Shape(tuple), optional, default=()</code>: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.</li>
<li><code>keepdims::boolean, optional, default=False</code>: If true, the axis which is reduced is left in the result as dimension with size one.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1093' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.mul_to!-Tuple{MXNet.mx.NDArray,Union{MXNet.mx.NDArray,Real}}' href='#MXNet.mx.mul_to!-Tuple{MXNet.mx.NDArray,Union{MXNet.mx.NDArray,Real}}'>#</a>
<strong><code>MXNet.mx.mul_to!</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>mul_to!(dst :: NDArray, arg :: Union{Real, NDArray})
</code></pre>

<p>Elementwise multiplication into <code>dst</code> of either a scalar or an <code>NDArray</code> of the same shape. Inplace updating.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L603-L608' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.nanprod-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.nanprod-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.nanprod</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>nanprod(data, axis, keepdims)
</code></pre>

<p>Compute product of src along axis, ignoring NaN values. If axis is empty, global reduction is performed</p>
<p>From:src/operator/tensor/broadcast_reduce_op_value.cc:56</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
<li><code>axis::Shape(tuple), optional, default=()</code>: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.</li>
<li><code>keepdims::boolean, optional, default=False</code>: If true, the axis which is reduced is left in the result as dimension with size one.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1091' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.nansum-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.nansum-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.nansum</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>nansum(data, axis, keepdims)
</code></pre>

<p>Sum src along axis, ignoring NaN values. If axis is empty, global reduction is performed</p>
<p>From:src/operator/tensor/broadcast_reduce_op_value.cc:46</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
<li><code>axis::Shape(tuple), optional, default=()</code>: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.</li>
<li><code>keepdims::boolean, optional, default=False</code>: If true, the axis which is reduced is left in the result as dimension with size one.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1091' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.negative-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.negative-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.negative</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>negative(data)
</code></pre>

<p>Negate src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:57</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.normal-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.normal-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.normal</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>normal(loc, scale, shape, ctx, dtype)
</code></pre>

<p>Sample a normal distribution</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>loc::float, optional, default=0</code>: Mean of the distribution.</li>
<li><code>scale::float, optional, default=1</code>: Standard deviation of the distribution.</li>
<li><code>shape::Shape(tuple), optional, default=()</code>: The shape of the output</li>
<li><code>ctx::string, optional, default=''</code>: Context of output, in format <a href="../n">cpu|gpu|cpu_pinned</a>.Only used for imperative calls.</li>
<li><code>dtype::{'float32'},optional, default='float32'</code>: DType of the output</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1093' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.ones-Tuple{Tuple{Vararg{Int64,N}}}' href='#MXNet.mx.ones-Tuple{Tuple{Vararg{Int64,N}}}'>#</a>
<strong><code>MXNet.mx.ones</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>ones(shape :: Tuple, ctx :: Context)
ones(shape :: Tuple)
ones(dim1, dim2, ...)
</code></pre>

<p>Create an <code>NDArray</code> with specific shape and initialize with 1.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L226-L232' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.ones-Tuple{Type{T<:Union{Float16,Float32,Float64,Int32,UInt8}},Tuple{Vararg{Int64,N}}}' href='#MXNet.mx.ones-Tuple{Type{T<:Union{Float16,Float32,Float64,Int32,UInt8}},Tuple{Vararg{Int64,N}}}'>#</a>
<strong><code>MXNet.mx.ones</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>ones(DType, shape :: Tuple, ctx :: Context)
ones(DType, shape :: Tuple)
ones(DType, dim1, dim2, ...)
</code></pre>

<p>Create an <code>NDArray</code> with specific shape &amp; type, and initialize with 1.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L207-L213' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.radians-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.radians-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.radians</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>radians(data)
</code></pre>

<p>Take radians of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:243</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.rint-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.rint-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.rint</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>rint(data)
</code></pre>

<p>Take round of the src to nearest integer</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:97</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.rsqrt-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.rsqrt-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.rsqrt</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>rsqrt(data)
</code></pre>

<p>Take reciprocal square root of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:125</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.save-Tuple{String,MXNet.mx.NDArray}' href='#MXNet.mx.save-Tuple{String,MXNet.mx.NDArray}'>#</a>
<strong><code>MXNet.mx.save</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>save(filename :: AbstractString, data)
</code></pre>

<p>Save NDarrays to binary file. Filename could be S3 or HDFS address, if <code>libmxnet</code> is built with corresponding support (see <code>load</code>).</p>
<ul>
<li><code>filename::String</code>: path to the binary file to write to.</li>
<li><code>data</code>: data to save to file. Data can be a<code>NDArray</code>, a <code>Vector{NDArray}</code>, or a <code>Dict{Base.Symbol, NDArray}</code>.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L890-L898' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.sgd_mom_update-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.sgd_mom_update-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.sgd_mom_update</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>sgd_mom_update()
</code></pre>

<p>Updater function for sgd optimizer</p>
<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.sgd_update-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.sgd_update-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.sgd_update</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>sgd_update()
</code></pre>

<p>Updater function for sgd optimizer</p>
<p><strong>Arguments</strong></p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1084' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.slice_axis-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.slice_axis-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.slice_axis</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>slice_axis(data, axis, begin, end)
</code></pre>

<p>Slice the input along certain axis and return a sliced array. The slice will be taken from [begin, end). end can be None and axis can be negative.</p>
<p>From:src/operator/tensor/matrix_op.cc:200</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
<li><code>axis::int, required</code>: The axis to be sliced. Negative axis means to count from the last to the first axis.</li>
<li><code>begin::int, required</code>: The beginning index to be sliced. Negative values are interpreted as counting from the backward.</li>
<li><code>end::int or None, required</code>: The end index to be sliced. The end can be None, in which case all the rest elements are used. Also, negative values are interpreted as counting from the backward.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1093' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.smooth_l1-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.smooth_l1-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.smooth_l1</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>smooth_l1(data, scalar)
</code></pre>

<p>Calculate Smooth L1 Loss(lhs, scalar)</p>
<p>From:src/operator/tensor/elemwise_binary_scalar_op_extended.cc:63</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: source input</li>
<li><code>scalar::float</code>: scalar input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.softmax_cross_entropy-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.softmax_cross_entropy-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.softmax_cross_entropy</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>softmax_cross_entropy(data, label)
</code></pre>

<p>Calculate cross_entropy(data, one_hot(label))</p>
<p>From:src/operator/loss_binary_op.cc:12</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Input data</li>
<li><code>label::NDArray</code>: Input label</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1089' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.square-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.square-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.square</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>square(data)
</code></pre>

<p>Take square of the src</p>
<p>From:src/operator/tensor/elemwise_unary_op.cc:107</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1087' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.sub_from!-Tuple{MXNet.mx.NDArray,Union{MXNet.mx.NDArray,Real}}' href='#MXNet.mx.sub_from!-Tuple{MXNet.mx.NDArray,Union{MXNet.mx.NDArray,Real}}'>#</a>
<strong><code>MXNet.mx.sub_from!</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>sub_from!(dst :: NDArray, args :: Union{Real, NDArray}...)
</code></pre>

<p>Subtract a bunch of arguments from <code>dst</code>. Inplace updating.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L559-L563' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.sum_axis-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.sum_axis-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.sum_axis</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>sum_axis(data, axis, keepdims)
</code></pre>

<p>sum_axis is an alias of sum.</p>
<p>Sum src along axis. If axis is empty, global reduction is performed</p>
<p>From:src/operator/tensor/broadcast_reduce_op_value.cc:17</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>data::NDArray</code>: Source input</li>
<li><code>axis::Shape(tuple), optional, default=()</code>: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.</li>
<li><code>keepdims::boolean, optional, default=False</code>: If true, the axis which is reduced is left in the result as dimension with size one.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1093' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.topk-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.topk-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.topk</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>topk(src, axis, k, ret_typ, is_ascend)
</code></pre>

<p>Return the top k element of an input tensor along a given axis.</p>
<p>From:src/operator/tensor/ordering_op.cc:18</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>src::NDArray</code>: Source input</li>
<li><code>axis::int or None, optional, default='-1'</code>: Axis along which to choose the top k indices. If not given, the flattened array is used. Default is -1.</li>
<li><code>k::int, optional, default='1'</code>: Number of top elements to select, should be always smaller than or equal to the element number in the given axis. A global sort is performed if set k &lt; 1.</li>
<li><code>ret_typ::{'both', 'indices', 'mask', 'value'},optional, default='indices'</code>: The return type. "value" means returning the top k values, "indices" means returning the indices of the top k values, "mask" means to return a mask array containing 0 and 1. 1 means the top k values. "both" means to return both value and indices.</li>
<li><code>is_ascend::boolean, optional, default=False</code>: Whether to choose k largest or k smallest. Top K largest elements will be chosen if set to false.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1095' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.try_get_shared-Tuple{MXNet.mx.NDArray}' href='#MXNet.mx.try_get_shared-Tuple{MXNet.mx.NDArray}'>#</a>
<strong><code>MXNet.mx.try_get_shared</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>try_get_shared(arr)
</code></pre>

<p>Try to create a Julia array by sharing the data with the underlying <code>NDArray</code>.</p>
<p><strong>Arguments:</strong></p>
<ul>
<li><code>arr::NDArray</code>: the array to be shared.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The returned array does not guarantee to share data with the underlying <code>NDArray</code>. In particular, data sharing is possible only when the <code>NDArray</code> lives on CPU.</p>
</div>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L812-L823' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.uniform-Tuple{Vararg{MXNet.mx.NDArray,N}}' href='#MXNet.mx.uniform-Tuple{Vararg{MXNet.mx.NDArray,N}}'>#</a>
<strong><code>MXNet.mx.uniform</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>uniform(low, high, shape, ctx, dtype)
</code></pre>

<p>Sample a uniform distribution</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>low::float, optional, default=0</code>: The lower bound of distribution</li>
<li><code>high::float, optional, default=1</code>: The upper bound of distribution</li>
<li><code>shape::Shape(tuple), optional, default=()</code>: The shape of the output</li>
<li><code>ctx::string, optional, default=''</code>: Context of output, in format <a href="../n">cpu|gpu|cpu_pinned</a>.Only used for imperative calls.</li>
<li><code>dtype::{'float32'},optional, default='float32'</code>: DType of the output</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L1078-L1093' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.zeros-Tuple{Tuple{Vararg{Int64,N}}}' href='#MXNet.mx.zeros-Tuple{Tuple{Vararg{Int64,N}}}'>#</a>
<strong><code>MXNet.mx.zeros</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>zeros(shape :: Tuple, ctx :: Context)
zeros(shape :: Tuple)
zeros(dim1, dim2, ...)
</code></pre>

<p>Create zero-ed <code>NDArray</code> with specific shape.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L188-L194' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.zeros-Tuple{Type{T<:Union{Float16,Float32,Float64,Int32,UInt8}},Tuple{Vararg{Int64,N}}}' href='#MXNet.mx.zeros-Tuple{Type{T<:Union{Float16,Float32,Float64,Int32,UInt8}},Tuple{Vararg{Int64,N}}}'>#</a>
<strong><code>MXNet.mx.zeros</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>zeros(DType, shape :: Tuple, ctx :: Context)
zeros(DType, shape :: Tuple)
zeros(DType, dim1, dim2, ...)
</code></pre>

<p>Create zero-ed <code>NDArray</code> with specific shape and type</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L169-L175' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.@inplace-Tuple{Any}' href='#MXNet.mx.@inplace-Tuple{Any}'>#</a>
<strong><code>MXNet.mx.@inplace</code></strong> &mdash; <em>Macro</em>.</p>
<pre><code>@inplace
</code></pre>

<p>Julia does not support re-definiton of <code>+=</code> operator (like <code>__iadd__</code> in python), When one write <code>a += b</code>, it gets translated to <code>a = a+b</code>. <code>a+b</code> will allocate new memory for the results, and the newly allocated <code>NDArray</code> object is then assigned back to a, while the original contents in a is discarded. This is very inefficient when we want to do inplace update.</p>
<p>This macro is a simple utility to implement this behavior. Write</p>
<pre><code class="julia">  @mx.inplace a += b
</code></pre>

<p>will translate into</p>
<pre><code class="julia">  mx.add_to!(a, b)
</code></pre>

<p>which will do inplace adding of the contents of <code>b</code> into <code>a</code>.</p>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L481-L503' class='documenter-source'>source</a><br></p>
<p><a id='MXNet.mx.@nd_as_jl-Tuple{Vararg{Any,N}}' href='#MXNet.mx.@nd_as_jl-Tuple{Vararg{Any,N}}'>#</a>
<strong><code>MXNet.mx.@nd_as_jl</code></strong> &mdash; <em>Macro</em>.</p>
<p><strong>Manipulating as Julia Arrays</strong></p>
<pre><code>@nd_as_jl(captures..., statement)
</code></pre>

<p>A convenient macro that allows to operate <code>NDArray</code> as Julia Arrays. For example,</p>
<pre><code class="julia">  x = mx.zeros(3,4)
  y = mx.ones(3,4)
  z = mx.zeros((3,4), mx.gpu())

  @mx.nd_as_jl ro=(x,y) rw=z begin
    # now x, y, z are just ordinary Julia Arrays
    z[:,1] = y[:,2]
    z[:,2] = 5
  end
</code></pre>

<p>Under the hood, the macro convert all the declared captures from <code>NDArray</code> into Julia Arrays, by using <code>try_get_shared</code>. And automatically commit the modifications back into the <code>NDArray</code> that is declared as <code>rw</code>. This is useful for fast prototyping and when implement non-critical computations, such as <code>AbstractEvalMetric</code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
</div>
<ul>
<li>Multiple <code>rw</code> and / or <code>ro</code> capture declaration could be made.</li>
<li>The macro does <strong>not</strong> check to make sure that <code>ro</code> captures are not modified. If the original <code>NDArray</code> lives in CPU memory, then it is very likely the corresponding Julia Array shares data with the <code>NDArray</code>, so modifying the Julia Array will also modify the underlying <code>NDArray</code>.</li>
<li>More importantly, since the <code>NDArray</code> is asynchronized, we will wait for <em>writing</em> for <code>rw</code> variables but wait only for <em>reading</em> in <code>ro</code> variables. If we write into those <code>ro</code> variables, <strong>and</strong> if the memory is shared, racing condition might happen, and the behavior is undefined.</li>
<li>When an <code>NDArray</code> is declared to be captured as <code>rw</code>, its contents is always sync back in the end.</li>
<li>The execution results of the expanded macro is always <code>nothing</code>.</li>
<li>The statements are wrapped in a <code>let</code>, thus locally introduced new variables will not be available after the statements. So you will need to declare the variables before calling the macro if needed.</li>
</ul>
<p><a target='_blank' href='https://github.com/dmlc/MXNet.jl/tree/ba3c9fb52ee13680a40aacdd13a34ed210595d2e/src/ndarray.jl#L683-L724' class='documenter-source'>source</a><br></p>
              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../io/" title="Data Providers" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Data Providers
              </span>
            </div>
          </a>
        
        
          <a href="../symbolic-node/" title="Symbolic API" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Symbolic API
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="http://www.mkdocs.org" title="MkDocs">MkDocs</a>
        and
        <a href="http://squidfunk.github.io/mkdocs-material/" title="Material for MkDocs">
          Material for MkDocs</a>
      </div>
      
        
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/application-2afe21e0b2.js"></script>
      <script>var config={url:{base:"../.."}},app=new Application(config);app.initialize()</script>
      
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.jl?config=TeX-AMS-MML_HTMLorMML"></script>
      
        <script src="../../assets/mathjaxhelper.js"></script>
      
    
    
      
    
  </body>
</html>