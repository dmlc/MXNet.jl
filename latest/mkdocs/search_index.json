{
    "docs": [
        {
            "location": "/", 
            "text": "MXNet Documentation\n\n\nMXNet.jl\n is the \nJulia\n package of \ndmlc/mxnet\n. MXNet.jl brings flexible and efficient GPU computing and state-of-art deep learning to Julia. Some highlight of features include:\n\n\n\n\nEfficient tensor/matrix computation across multiple devices, including multiple CPUs, GPUs and distributed server nodes.\n\n\nFlexible symbolic manipulation to composite and construct state-of-the-art deep learning models.\n\n\n\n\nFor more details, see documentation below. Please also checkout the \nexamples\n directory.\n\n\n\n\nTutorials\n\n\n\n\nDigit Recognition on MNIST\n\n\nSimple 3-layer MLP\n\n\nConvolutional Neural Networks\n\n\nPredicting with a trained model\n\n\n\n\n\n\nGenerating Random Sentence with LSTM RNN\n\n\nLSTM Cells\n\n\nUnfolding LSTM\n\n\nData Provider for Text Sequences\n\n\nTraining the LSTM\n\n\nSampling Random Sentences\n\n\nVisualizing the LSTM\n\n\n\n\n\n\n\n\n\n\nUser's Guide\n\n\n\n\nInstallation Guide\n\n\nAutomatic Installation\n\n\nManual Compilation\n\n\n\n\n\n\nOverview\n\n\nMXNet.jl Namespace\n\n\nLow Level Interface\n\n\nIntermediate Level Interface\n\n\nHigh Level Interface\n\n\n\n\n\n\nFAQ\n\n\nRunning MXNet on AWS GPU instances\n\n\n\n\n\n\n\n\n\n\nAPI Documentation\n\n\n\n\nContext\n\n\nModel\n\n\nEvaluation Metrics\n\n\nData Providers\n\n\nAbstractDataProvider interface\n\n\nAbstractDataBatch interface\n\n\nImplemented providers and other methods\n\n\n\n\n\n\nNDArray API\n\n\nSymbolic API\n\n\nNeural Network Factory\n\n\nExecutor\n\n\nNetwork Visualization", 
            "title": "Home"
        }, 
        {
            "location": "/#mxnet-documentation", 
            "text": "MXNet.jl  is the  Julia  package of  dmlc/mxnet . MXNet.jl brings flexible and efficient GPU computing and state-of-art deep learning to Julia. Some highlight of features include:   Efficient tensor/matrix computation across multiple devices, including multiple CPUs, GPUs and distributed server nodes.  Flexible symbolic manipulation to composite and construct state-of-the-art deep learning models.   For more details, see documentation below. Please also checkout the  examples  directory.", 
            "title": "MXNet Documentation"
        }, 
        {
            "location": "/#tutorials", 
            "text": "Digit Recognition on MNIST  Simple 3-layer MLP  Convolutional Neural Networks  Predicting with a trained model    Generating Random Sentence with LSTM RNN  LSTM Cells  Unfolding LSTM  Data Provider for Text Sequences  Training the LSTM  Sampling Random Sentences  Visualizing the LSTM", 
            "title": "Tutorials"
        }, 
        {
            "location": "/#users-guide", 
            "text": "Installation Guide  Automatic Installation  Manual Compilation    Overview  MXNet.jl Namespace  Low Level Interface  Intermediate Level Interface  High Level Interface    FAQ  Running MXNet on AWS GPU instances", 
            "title": "User's Guide"
        }, 
        {
            "location": "/#api-documentation", 
            "text": "Context  Model  Evaluation Metrics  Data Providers  AbstractDataProvider interface  AbstractDataBatch interface  Implemented providers and other methods    NDArray API  Symbolic API  Neural Network Factory  Executor  Network Visualization", 
            "title": "API Documentation"
        }, 
        {
            "location": "/tutorial/mnist/", 
            "text": "Digit Recognition on MNIST\n\n\nIn this tutorial, we will work through examples of training a simple multi-layer perceptron and then a convolutional neural network (the LeNet architecture) on the \nMNIST handwritten digit dataset\n. The code for this tutorial could be found in \nexamples/mnist\n.\n\n\n\n\nSimple 3-layer MLP\n\n\nThis is a tiny 3-layer MLP that could be easily trained on CPU. The script starts with\n\n\nusing MXNet\n\n\n\n\nto load the \nMXNet\n module. Then we are ready to define the network architecture via the \nsymbolic API\n. We start with a placeholder \ndata\n symbol,\n\n\ndata = mx.Variable(:data)\n\n\n\n\nand then cascading fully-connected layers and activation functions:\n\n\nfc1  = mx.FullyConnected(data = data, name=:fc1, num_hidden=128)\nact1 = mx.Activation(data = fc1, name=:relu1, act_type=:relu)\nfc2  = mx.FullyConnected(data = act1, name=:fc2, num_hidden=64)\nact2 = mx.Activation(data = fc2, name=:relu2, act_type=:relu)\nfc3  = mx.FullyConnected(data = act2, name=:fc3, num_hidden=10)\n\n\n\n\nNote each composition we take the previous symbol as the data argument, forming a feedforward chain. The architecture looks like\n\n\nInput --\n 128 units (ReLU) --\n 64 units (ReLU) --\n 10 units\n\n\n\n\nwhere the last 10 units correspond to the 10 output classes (digits 0,...,9). We then add a final \nSoftmaxOutput\n operation to turn the 10-dimensional prediction to proper probability values for the 10 classes:\n\n\nmlp  = mx.SoftmaxOutput(data = fc3, name=:softmax)\n\n\n\n\nAs we can see, the MLP is just a chain of layers. For this case, we can also use the \nmx.chain\n macro. The same architecture above can be defined as\n\n\nmlp = @mx.chain mx.Variable(:data)             =\n\n  mx.FullyConnected(name=:fc1, num_hidden=128) =\n\n  mx.Activation(name=:relu1, act_type=:relu)   =\n\n  mx.FullyConnected(name=:fc2, num_hidden=64)  =\n\n  mx.Activation(name=:relu2, act_type=:relu)   =\n\n  mx.FullyConnected(name=:fc3, num_hidden=10)  =\n\n  mx.SoftmaxOutput(name=:softmax)\n\n\n\n\nAfter defining the architecture, we are ready to load the MNIST data. MXNet.jl provide built-in data providers for the MNIST dataset, which could automatically download the dataset into \nPkg.dir(\"MXNet\")/data/mnist\n if necessary. We wrap the code to construct the data provider into \nmnist-data.jl\n so that it could be shared by both the MLP example and the LeNet ConvNets example.\n\n\nbatch_size = 100\ninclude(\nmnist-data.jl\n)\ntrain_provider, eval_provider = get_mnist_providers(batch_size)\n\n\n\n\nIf you need to write your own data providers for customized data format, please refer to \nmx.AbstractDataProvider\n.\n\n\nGiven the architecture and data, we can instantiate an \nmodel\n to do the actual training. \nmx.FeedForward\n is the built-in model that is suitable for most feed-forward architectures. When constructing the model, we also specify the \ncontext\n on which the computation should be carried out. Because this is a really tiny MLP, we will just run on a single CPU device.\n\n\nmodel = mx.FeedForward(mlp, context=mx.cpu())\n\n\n\n\nYou can use a \nmx.gpu()\n or if a list of devices (e.g. \n[mx.gpu(0), mx.gpu(1)]\n) is provided, data-parallelization will be used automatically. But for this tiny example, using a GPU device might not help.\n\n\nThe last thing we need to specify is the optimization algorithm (a.k.a. \noptimizer\n) to use. We use the basic SGD with a fixed learning rate 0.1 and momentum 0.9:\n\n\noptimizer = mx.SGD(lr=0.1, momentum=0.9, weight_decay=0.00001)\n\n\n\n\nNow we can do the training. Here the \nn_epoch\n parameter specifies that we want to train for 20 epochs. We also supply a \neval_data\n to monitor validation accuracy on the validation set.\n\n\nmx.fit(model, optimizer, train_provider, n_epoch=20, eval_data=eval_provider)\n\n\n\n\nHere is a sample output\n\n\nINFO: Start training on [CPU0]\nINFO: Initializing parameters...\nINFO: Creating KVStore...\nINFO: == Epoch 001 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.7554\nINFO:            time = 1.3165 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9502\n...\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.9949\nINFO:            time = 0.9287 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9775\n\n\n\n\n\n\nConvolutional Neural Networks\n\n\nIn the second example, we show a slightly more complicated architecture that involves convolution and pooling. This architecture for the MNIST is usually called the [LeNet]_. The first part of the architecture is listed below:\n\n\n# input\ndata = mx.Variable(:data)\n\n# first conv\nconv1 = @mx.chain mx.Convolution(data=data, kernel=(5,5), num_filter=20)  =\n\n                  mx.Activation(act_type=:tanh) =\n\n                  mx.Pooling(pool_type=:max, kernel=(2,2), stride=(2,2))\n\n# second conv\nconv2 = @mx.chain mx.Convolution(data=conv1, kernel=(5,5), num_filter=50) =\n\n                  mx.Activation(act_type=:tanh) =\n\n                  mx.Pooling(pool_type=:max, kernel=(2,2), stride=(2,2))\n\n\n\n\nWe basically defined two convolution modules. Each convolution module is actually a chain of \nConvolution\n, \ntanh\n activation and then max \nPooling\n operations.\n\n\nEach sample in the MNIST dataset is a 28x28 single-channel grayscale image. In the tensor format used by \nNDArray\n, a batch of 100 samples is a tensor of shape \n(28,28,1,100)\n. The convolution and pooling operates in the spatial axis, so \nkernel=(5,5)\n indicate a square region of 5-width and 5-height. The rest of the architecture follows as:\n\n\n# first fully-connected\nfc1   = @mx.chain mx.Flatten(data=conv2) =\n\n                  mx.FullyConnected(num_hidden=500) =\n\n                  mx.Activation(act_type=:tanh)\n\n# second fully-connected\nfc2   = mx.FullyConnected(data=fc1, num_hidden=10)\n\n# softmax loss\nlenet = mx.Softmax(data=fc2, name=:softmax)\n\n\n\n\nNote a fully-connected operator expects the input to be a matrix. However, the results from spatial convolution and pooling are 4D tensors. So we explicitly used a \nFlatten\n operator to flat the tensor, before connecting it to the \nFullyConnected\n operator.\n\n\nThe rest of the network is the same as the previous MLP example. As before, we can now load the MNIST dataset:\n\n\nbatch_size = 100\ninclude(\nmnist-data.jl\n)\ntrain_provider, eval_provider = get_mnist_providers(batch_size; flat=false)\n\n\n\n\nNote we specified \nflat=false\n to tell the data provider to provide 4D tensors instead of 2D matrices because the convolution operators needs correct spatial shape information. We then construct a feedforward model on GPU, and train it.\n\n\n# fit model\nmodel = mx.FeedForward(lenet, context=mx.gpu())\n\n# optimizer\noptimizer = mx.SGD(lr=0.05, momentum=0.9, weight_decay=0.00001)\n\n# fit parameters\nmx.fit(model, optimizer, train_provider, n_epoch=20, eval_data=eval_provider)\n\n\n\n\nAnd here is a sample of running outputs:\n\n\nINFO: == Epoch 001 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.6750\nINFO:            time = 4.9814 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9712\n...\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 1.0000\nINFO:            time = 4.0086 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9915\n\n\n\n\n\n\nPredicting with a trained model\n\n\nPredicting with a trained model is very simple. By calling \nmx.predict\n with the model and a data provider, we get the model output as a Julia Array:\n\n\nprobs = mx.predict(model, eval_provider)\n\n\n\n\nThe following code shows a stupid way of getting all the labels from the data provider, and compute the prediction accuracy manually:\n\n\n# collect all labels from eval data\nlabels = Array[]\nfor batch in eval_provider\n  push!(labels, copy(mx.get_label(batch)))\nend\nlabels = cat(1, labels...)\n\n# Now we use compute the accuracy\ncorrect = 0\nfor i = 1:length(labels)\n  # labels are 0...9\n  if indmax(probs[:,i]) == labels[i]+1\n    correct += 1\n  end\nend\nprintln(mx.format(\nAccuracy on eval set: {1:.2f}%\n, 100correct/length(labels)))\n\n\n\n\nAlternatively, when the dataset is huge, one can provide a callback to \nmx.predict\n, then the callback function will be invoked with the outputs of each mini-batch. The callback could, for example, write the data to disk for future inspection. In this case, no value is returned from \nmx.predict\n. See also predict.", 
            "title": "Digit Recognition on MNIST"
        }, 
        {
            "location": "/tutorial/mnist/#digit-recognition-on-mnist", 
            "text": "In this tutorial, we will work through examples of training a simple multi-layer perceptron and then a convolutional neural network (the LeNet architecture) on the  MNIST handwritten digit dataset . The code for this tutorial could be found in  examples/mnist .", 
            "title": "Digit Recognition on MNIST"
        }, 
        {
            "location": "/tutorial/mnist/#simple-3-layer-mlp", 
            "text": "This is a tiny 3-layer MLP that could be easily trained on CPU. The script starts with  using MXNet  to load the  MXNet  module. Then we are ready to define the network architecture via the  symbolic API . We start with a placeholder  data  symbol,  data = mx.Variable(:data)  and then cascading fully-connected layers and activation functions:  fc1  = mx.FullyConnected(data = data, name=:fc1, num_hidden=128)\nact1 = mx.Activation(data = fc1, name=:relu1, act_type=:relu)\nfc2  = mx.FullyConnected(data = act1, name=:fc2, num_hidden=64)\nact2 = mx.Activation(data = fc2, name=:relu2, act_type=:relu)\nfc3  = mx.FullyConnected(data = act2, name=:fc3, num_hidden=10)  Note each composition we take the previous symbol as the data argument, forming a feedforward chain. The architecture looks like  Input --  128 units (ReLU) --  64 units (ReLU) --  10 units  where the last 10 units correspond to the 10 output classes (digits 0,...,9). We then add a final  SoftmaxOutput  operation to turn the 10-dimensional prediction to proper probability values for the 10 classes:  mlp  = mx.SoftmaxOutput(data = fc3, name=:softmax)  As we can see, the MLP is just a chain of layers. For this case, we can also use the  mx.chain  macro. The same architecture above can be defined as  mlp = @mx.chain mx.Variable(:data)             = \n  mx.FullyConnected(name=:fc1, num_hidden=128) = \n  mx.Activation(name=:relu1, act_type=:relu)   = \n  mx.FullyConnected(name=:fc2, num_hidden=64)  = \n  mx.Activation(name=:relu2, act_type=:relu)   = \n  mx.FullyConnected(name=:fc3, num_hidden=10)  = \n  mx.SoftmaxOutput(name=:softmax)  After defining the architecture, we are ready to load the MNIST data. MXNet.jl provide built-in data providers for the MNIST dataset, which could automatically download the dataset into  Pkg.dir(\"MXNet\")/data/mnist  if necessary. We wrap the code to construct the data provider into  mnist-data.jl  so that it could be shared by both the MLP example and the LeNet ConvNets example.  batch_size = 100\ninclude( mnist-data.jl )\ntrain_provider, eval_provider = get_mnist_providers(batch_size)  If you need to write your own data providers for customized data format, please refer to  mx.AbstractDataProvider .  Given the architecture and data, we can instantiate an  model  to do the actual training.  mx.FeedForward  is the built-in model that is suitable for most feed-forward architectures. When constructing the model, we also specify the  context  on which the computation should be carried out. Because this is a really tiny MLP, we will just run on a single CPU device.  model = mx.FeedForward(mlp, context=mx.cpu())  You can use a  mx.gpu()  or if a list of devices (e.g.  [mx.gpu(0), mx.gpu(1)] ) is provided, data-parallelization will be used automatically. But for this tiny example, using a GPU device might not help.  The last thing we need to specify is the optimization algorithm (a.k.a.  optimizer ) to use. We use the basic SGD with a fixed learning rate 0.1 and momentum 0.9:  optimizer = mx.SGD(lr=0.1, momentum=0.9, weight_decay=0.00001)  Now we can do the training. Here the  n_epoch  parameter specifies that we want to train for 20 epochs. We also supply a  eval_data  to monitor validation accuracy on the validation set.  mx.fit(model, optimizer, train_provider, n_epoch=20, eval_data=eval_provider)  Here is a sample output  INFO: Start training on [CPU0]\nINFO: Initializing parameters...\nINFO: Creating KVStore...\nINFO: == Epoch 001 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.7554\nINFO:            time = 1.3165 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9502\n...\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.9949\nINFO:            time = 0.9287 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9775", 
            "title": "Simple 3-layer MLP"
        }, 
        {
            "location": "/tutorial/mnist/#convolutional-neural-networks", 
            "text": "In the second example, we show a slightly more complicated architecture that involves convolution and pooling. This architecture for the MNIST is usually called the [LeNet]_. The first part of the architecture is listed below:  # input\ndata = mx.Variable(:data)\n\n# first conv\nconv1 = @mx.chain mx.Convolution(data=data, kernel=(5,5), num_filter=20)  = \n                  mx.Activation(act_type=:tanh) = \n                  mx.Pooling(pool_type=:max, kernel=(2,2), stride=(2,2))\n\n# second conv\nconv2 = @mx.chain mx.Convolution(data=conv1, kernel=(5,5), num_filter=50) = \n                  mx.Activation(act_type=:tanh) = \n                  mx.Pooling(pool_type=:max, kernel=(2,2), stride=(2,2))  We basically defined two convolution modules. Each convolution module is actually a chain of  Convolution ,  tanh  activation and then max  Pooling  operations.  Each sample in the MNIST dataset is a 28x28 single-channel grayscale image. In the tensor format used by  NDArray , a batch of 100 samples is a tensor of shape  (28,28,1,100) . The convolution and pooling operates in the spatial axis, so  kernel=(5,5)  indicate a square region of 5-width and 5-height. The rest of the architecture follows as:  # first fully-connected\nfc1   = @mx.chain mx.Flatten(data=conv2) = \n                  mx.FullyConnected(num_hidden=500) = \n                  mx.Activation(act_type=:tanh)\n\n# second fully-connected\nfc2   = mx.FullyConnected(data=fc1, num_hidden=10)\n\n# softmax loss\nlenet = mx.Softmax(data=fc2, name=:softmax)  Note a fully-connected operator expects the input to be a matrix. However, the results from spatial convolution and pooling are 4D tensors. So we explicitly used a  Flatten  operator to flat the tensor, before connecting it to the  FullyConnected  operator.  The rest of the network is the same as the previous MLP example. As before, we can now load the MNIST dataset:  batch_size = 100\ninclude( mnist-data.jl )\ntrain_provider, eval_provider = get_mnist_providers(batch_size; flat=false)  Note we specified  flat=false  to tell the data provider to provide 4D tensors instead of 2D matrices because the convolution operators needs correct spatial shape information. We then construct a feedforward model on GPU, and train it.  # fit model\nmodel = mx.FeedForward(lenet, context=mx.gpu())\n\n# optimizer\noptimizer = mx.SGD(lr=0.05, momentum=0.9, weight_decay=0.00001)\n\n# fit parameters\nmx.fit(model, optimizer, train_provider, n_epoch=20, eval_data=eval_provider)  And here is a sample of running outputs:  INFO: == Epoch 001 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.6750\nINFO:            time = 4.9814 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9712\n...\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 1.0000\nINFO:            time = 4.0086 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9915", 
            "title": "Convolutional Neural Networks"
        }, 
        {
            "location": "/tutorial/mnist/#predicting-with-a-trained-model", 
            "text": "Predicting with a trained model is very simple. By calling  mx.predict  with the model and a data provider, we get the model output as a Julia Array:  probs = mx.predict(model, eval_provider)  The following code shows a stupid way of getting all the labels from the data provider, and compute the prediction accuracy manually:  # collect all labels from eval data\nlabels = Array[]\nfor batch in eval_provider\n  push!(labels, copy(mx.get_label(batch)))\nend\nlabels = cat(1, labels...)\n\n# Now we use compute the accuracy\ncorrect = 0\nfor i = 1:length(labels)\n  # labels are 0...9\n  if indmax(probs[:,i]) == labels[i]+1\n    correct += 1\n  end\nend\nprintln(mx.format( Accuracy on eval set: {1:.2f}% , 100correct/length(labels)))  Alternatively, when the dataset is huge, one can provide a callback to  mx.predict , then the callback function will be invoked with the outputs of each mini-batch. The callback could, for example, write the data to disk for future inspection. In this case, no value is returned from  mx.predict . See also predict.", 
            "title": "Predicting with a trained model"
        }, 
        {
            "location": "/tutorial/char-lstm/", 
            "text": "Generating Random Sentence with LSTM RNN\n\n\nThis tutorial shows how to train a LSTM (Long short-term memory) RNN (recurrent neural network) to perform character-level sequence training and prediction. The original model, usually called \nchar-rnn\n is described in \nAndrej Karpathy's blog\n, with a reference implementation in Torch available \nhere\n.\n\n\nBecause MXNet.jl does not have a specialized model for recurrent neural networks yet, the example shown here is an implementation of LSTM by using the default FeedForward model via explicitly unfolding over time. We will be using fixed-length input sequence for training. The code is adapted from the \nchar-rnn example for MXNet's Python binding\n, which demonstrates how to use low-level \nSymbolic API\n to build customized neural network models directly.\n\n\nThe most important code snippets of this example is shown and explained here. To see and run the complete code, please refer to the \nexamples/char-lstm\n directory. You will need to install \nIterators.jl\n and \nStatsBase.jl\n to run this example.\n\n\n\n\nLSTM Cells\n\n\nChristopher Olah has a \ngreat blog post about LSTM\n with beautiful and clear illustrations. So we will not repeat the definition and explanation of what an LSTM cell is here. Basically, an LSTM cell takes input \nx\n, as well as previous states (including \nc\n and \nh\n), and produce the next states. We define a helper type to bundle the two state variables together:\n\n\nBecause LSTM weights are shared at every time when we do explicit unfolding, so we also define a helper type to hold all the weights (and bias) for an LSTM cell for convenience.\n\n\nNote all the variables are of type SymbolicNode. We will construct the LSTM network as a symbolic computation graph, which is then instantiated with NDArray for actual computation.\n\n\nThe following figure is stolen (permission requested) from \nChristopher Olah's blog\n, which illustrate exactly what the code snippet above is doing.\n\n\n\n\nIn particular, instead of defining the four gates independently, we do the computation together and then use SliceChannel to split them into four outputs. The computation of gates are all done with the symbolic API. The return value is a LSTM state containing the output of a LSTM cell.\n\n\n\n\nUnfolding LSTM\n\n\nUsing the LSTM cell defined above, we are now ready to define a function to unfold a LSTM network with L layers and T time steps. The first part of the function is just defining all the symbolic variables for the shared weights and states.\n\n\nThe \nembed_W\n is the weights used for character embedding \u2013- i.e. mapping the one-hot encoded characters into real vectors. The \npred_W\n and \npred_b\n are weights and bias for the final prediction at each time step.\n\n\nThen we define the weights for each LSTM cell. Note there is one cell for each layer, and it will be replicated (unrolled) over time. The states are, however, \nnot\n shared over time. Instead, here we define the initial states here at the beginning of a sequence, and we will update them with the output states at each time step as we explicitly unroll the LSTM.\n\n\nUnrolling over time is a straightforward procedure of stacking the embedding layer, and then LSTM cells, on top of which the prediction layer. During unrolling, we update the states and collect all the outputs. Note each time step takes data and label as inputs. If the LSTM is named as \n:ptb\n, the data and label at step \nt\n will be named \n:ptb_data_$t\n and \n:ptb_label_$t\n. Late on when we prepare the data, we will define the data provider to match those names.\n\n\nNote at each time step, the prediction is connected to a SoftmaxOutput operator, which could back propagate when corresponding labels are provided. The states are then connected to the next time step, which allows back propagate through time. However, at the end of the sequence, the final states are not connected to anything. This dangling outputs is problematic, so we explicitly connect each of them to a BlockGrad operator, which simply back propagates 0-gradient and closes the computation graph.\n\n\nIn the end, we just group all the prediction outputs at each time step as a single SymbolicNode and return. Optionally we will also group the final states, this is used when we use the trained LSTM to sample sentences.\n\n\n\n\nData Provider for Text Sequences\n\n\nNow we need to construct a data provider that takes a text file, divide the text into mini-batches of fixed-length character-sequences, and provide them as one-hot encoded vectors.\n\n\nNote the is no fancy feature extraction at all. Each character is simply encoded as a one-hot vector: a 0-1 vector of the size given by the vocabulary. Here we just construct the vocabulary by collecting all the unique characters in the training text \u2013 there are not too many of them (including punctuations and whitespace) for English text. Each input character is then encoded as a vector of 0s on all coordinates, and 1 on the coordinate corresponding to that character. The character-to-coordinate mapping is giving by the vocabulary.\n\n\nThe text sequence data provider implements the \nData Providers\n api. We define the \nCharSeqProvider\n as below:\n\n\nThe provided data and labels follow the naming convention of inputs used when unrolling the LSTM. Note in the code below, apart from \n$name_data_$t\n and \n$name_label_$t\n, we also provides the initial \nc\n and \nh\n states for each layer. This is because we are using the high-level FeedForward API, which has no idea about time and states. So we will feed the initial states for each sequence from the data provider. Since the initial states is always zero, we just need to always provide constant zero blobs.\n\n\nNext we implement the \neachbatch\n method from the \nmx.AbstractDataProvider\n interface for the provider. We start by defining the data and label arrays, and the \nDataBatch\n object we will provide in each iteration.\n\n\nThe actual data providing iteration is implemented as a Julia \ncoroutine\n. In this way, we can write the data loading logic as a simple coherent \nfor\n loop, and do not need to implement the interface functions like Base.start, Base.next, etc.\n\n\nBasically, we partition the text into batches, each batch containing several contiguous text sequences. Note at each time step, the LSTM is trained to predict the next character, so the label is the same as the data, but shifted ahead by one index.\n\n\n\n\nTraining the LSTM\n\n\nNow we have implemented all the supporting infrastructures for our char-lstm. To train the model, we just follow the standard high-level API. Firstly, we construct a LSTM symbolic architecture:\n\n\nNote all the parameters are defined in \nexamples/char-lstm/config.jl\n. Now we load the text file and define the data provider. The data \ninput.txt\n we used in this example is \na tiny Shakespeare dataset\n. But you can try with other text files.\n\n\nThe last step is to construct a model, an optimizer and fit the mode to the data. We are using the ADAM optimizer [Adam]_ in this example.\n\n\nNote we are also using a customized \nNLL\n evaluation metric, which calculate the negative log-likelihood during training. Here is an output sample at the end of the training process.\n\n\n...\nINFO: Speed: 357.72 samples/sec\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:                NLL = 1.4672\nINFO:         perplexity = 4.3373\nINFO:               time = 87.2631 seconds\nINFO: ## Validation summary\nINFO:                NLL = 1.6374\nINFO:         perplexity = 5.1418\nINFO: Saved checkpoint to 'char-lstm/checkpoints/ptb-0020.params'\nINFO: Speed: 368.74 samples/sec\nINFO: Speed: 361.04 samples/sec\nINFO: Speed: 360.02 samples/sec\nINFO: Speed: 362.34 samples/sec\nINFO: Speed: 360.80 samples/sec\nINFO: Speed: 362.77 samples/sec\nINFO: Speed: 357.18 samples/sec\nINFO: Speed: 355.30 samples/sec\nINFO: Speed: 362.33 samples/sec\nINFO: Speed: 359.23 samples/sec\nINFO: Speed: 358.09 samples/sec\nINFO: Speed: 356.89 samples/sec\nINFO: Speed: 371.91 samples/sec\nINFO: Speed: 372.24 samples/sec\nINFO: Speed: 356.59 samples/sec\nINFO: Speed: 356.64 samples/sec\nINFO: Speed: 360.24 samples/sec\nINFO: Speed: 360.32 samples/sec\nINFO: Speed: 362.38 samples/sec\nINFO: == Epoch 021 ==========\nINFO: ## Training summary\nINFO:                NLL = 1.4655\nINFO:         perplexity = 4.3297\nINFO:               time = 86.9243 seconds\nINFO: ## Validation summary\nINFO:                NLL = 1.6366\nINFO:         perplexity = 5.1378\nINFO: Saved checkpoint to 'examples/char-lstm/checkpoints/ptb-0021.params'\n\n\n\n\n\n\nSampling Random Sentences\n\n\nAfter training the LSTM, we can now sample random sentences from the trained model. The sampler works in the following way:\n\n\n\n\nStarting from some fixed character, take \na\n for example, and feed   it as input to the LSTM.\n\n\nThe LSTM will produce an output distribution over the vocabulary and   a state in the first time step. We sample a character from the   output distribution, fix it as the second character.\n\n\nIn the next time step, we feed the previously sampled character as   input and continue running the LSTM by also taking the previous   states (instead of the 0 initial states).\n\n\nContinue running until we sampled enough characters.\n\n\n\n\nNote we are running with mini-batches, so several sentences could be sampled simultaneously. Here are some sampled outputs from a network I trained for around half an hour on the Shakespeare dataset. Note all the line-breaks, punctuations and upper-lower case letters are produced by the sampler itself. I did not do any post-processing.\n\n\n## Sample 1\nall have sir,\nAway will fill'd in His time, I'll keep her, do not madam, if they here? Some more ha?\n\n## Sample 2\nam.\n\nCLAUDIO:\nHone here, let her, the remedge, and I know not slept a likely, thou some soully free?\n\n## Sample 3\narrel which noble thing\nThe exchnachsureding worns: I ne'er drunken Biancas, fairer, than the lawfu?\n\n## Sample 4\naugh assalu, you'ld tell me corn;\nFarew. First, for me of a loved. Has thereat I knock you presents?\n\n## Sample 5\name the first answer.\n\nMARIZARINIO:\nDoor of Angelo as her lord, shrield liken Here fellow the fool ?\n\n## Sample 6\nad well.\n\nCLAUDIO:\nSoon him a fellows here; for her fine edge in a bogms' lord's wife.\n\nLUCENTIO:\nI?\n\n## Sample 7\nadrezilian measure.\n\nLUCENTIO:\nSo, help'd you hath nes have a than dream's corn, beautio, I perchas?\n\n## Sample 8\nas eatter me;\nThe girlly: and no other conciolation!\n\nBISTRUMIO:\nI have be rest girl. O, that I a h?\n\n## Sample 9\nand is intend you sort:\nWhat held her all 'clama's for maffice. Some servant.' what I say me the cu?\n\n## Sample 10\nan thoughts will said in our pleasue,\nNot scanin on him that you live; believaries she.\n\nISABELLLLL?\n\n\n\n\nSee \nAndrej Karpathy's blog post\n on more examples and links including Linux source codes, Algebraic Geometry Theorems, and even cooking recipes. The code for sampling can be found in \nexamples/char-lstm/sampler.jl\n.\n\n\n\n\nVisualizing the LSTM\n\n\nFinally, you could visualize the LSTM by calling to_graphviz on the constructed LSTM symbolic architecture. We only show an example of 1-layer and 2-time-step LSTM below. The automatic layout produced by GraphViz is definitely much less clear than \nChristopher Olah's illustrations\n, but could otherwise be very useful for debugging. As we can see, the LSTM unfolded over time is just a (very) deep neural network. The complete code for producing this visualization can be found in \nexamples/char-lstm/visualize.jl\n.", 
            "title": "Generating Random Sentence with LSTM RNN"
        }, 
        {
            "location": "/tutorial/char-lstm/#generating-random-sentence-with-lstm-rnn", 
            "text": "This tutorial shows how to train a LSTM (Long short-term memory) RNN (recurrent neural network) to perform character-level sequence training and prediction. The original model, usually called  char-rnn  is described in  Andrej Karpathy's blog , with a reference implementation in Torch available  here .  Because MXNet.jl does not have a specialized model for recurrent neural networks yet, the example shown here is an implementation of LSTM by using the default FeedForward model via explicitly unfolding over time. We will be using fixed-length input sequence for training. The code is adapted from the  char-rnn example for MXNet's Python binding , which demonstrates how to use low-level  Symbolic API  to build customized neural network models directly.  The most important code snippets of this example is shown and explained here. To see and run the complete code, please refer to the  examples/char-lstm  directory. You will need to install  Iterators.jl  and  StatsBase.jl  to run this example.", 
            "title": "Generating Random Sentence with LSTM RNN"
        }, 
        {
            "location": "/tutorial/char-lstm/#lstm-cells", 
            "text": "Christopher Olah has a  great blog post about LSTM  with beautiful and clear illustrations. So we will not repeat the definition and explanation of what an LSTM cell is here. Basically, an LSTM cell takes input  x , as well as previous states (including  c  and  h ), and produce the next states. We define a helper type to bundle the two state variables together:  Because LSTM weights are shared at every time when we do explicit unfolding, so we also define a helper type to hold all the weights (and bias) for an LSTM cell for convenience.  Note all the variables are of type SymbolicNode. We will construct the LSTM network as a symbolic computation graph, which is then instantiated with NDArray for actual computation.  The following figure is stolen (permission requested) from  Christopher Olah's blog , which illustrate exactly what the code snippet above is doing.   In particular, instead of defining the four gates independently, we do the computation together and then use SliceChannel to split them into four outputs. The computation of gates are all done with the symbolic API. The return value is a LSTM state containing the output of a LSTM cell.", 
            "title": "LSTM Cells"
        }, 
        {
            "location": "/tutorial/char-lstm/#unfolding-lstm", 
            "text": "Using the LSTM cell defined above, we are now ready to define a function to unfold a LSTM network with L layers and T time steps. The first part of the function is just defining all the symbolic variables for the shared weights and states.  The  embed_W  is the weights used for character embedding \u2013- i.e. mapping the one-hot encoded characters into real vectors. The  pred_W  and  pred_b  are weights and bias for the final prediction at each time step.  Then we define the weights for each LSTM cell. Note there is one cell for each layer, and it will be replicated (unrolled) over time. The states are, however,  not  shared over time. Instead, here we define the initial states here at the beginning of a sequence, and we will update them with the output states at each time step as we explicitly unroll the LSTM.  Unrolling over time is a straightforward procedure of stacking the embedding layer, and then LSTM cells, on top of which the prediction layer. During unrolling, we update the states and collect all the outputs. Note each time step takes data and label as inputs. If the LSTM is named as  :ptb , the data and label at step  t  will be named  :ptb_data_$t  and  :ptb_label_$t . Late on when we prepare the data, we will define the data provider to match those names.  Note at each time step, the prediction is connected to a SoftmaxOutput operator, which could back propagate when corresponding labels are provided. The states are then connected to the next time step, which allows back propagate through time. However, at the end of the sequence, the final states are not connected to anything. This dangling outputs is problematic, so we explicitly connect each of them to a BlockGrad operator, which simply back propagates 0-gradient and closes the computation graph.  In the end, we just group all the prediction outputs at each time step as a single SymbolicNode and return. Optionally we will also group the final states, this is used when we use the trained LSTM to sample sentences.", 
            "title": "Unfolding LSTM"
        }, 
        {
            "location": "/tutorial/char-lstm/#data-provider-for-text-sequences", 
            "text": "Now we need to construct a data provider that takes a text file, divide the text into mini-batches of fixed-length character-sequences, and provide them as one-hot encoded vectors.  Note the is no fancy feature extraction at all. Each character is simply encoded as a one-hot vector: a 0-1 vector of the size given by the vocabulary. Here we just construct the vocabulary by collecting all the unique characters in the training text \u2013 there are not too many of them (including punctuations and whitespace) for English text. Each input character is then encoded as a vector of 0s on all coordinates, and 1 on the coordinate corresponding to that character. The character-to-coordinate mapping is giving by the vocabulary.  The text sequence data provider implements the  Data Providers  api. We define the  CharSeqProvider  as below:  The provided data and labels follow the naming convention of inputs used when unrolling the LSTM. Note in the code below, apart from  $name_data_$t  and  $name_label_$t , we also provides the initial  c  and  h  states for each layer. This is because we are using the high-level FeedForward API, which has no idea about time and states. So we will feed the initial states for each sequence from the data provider. Since the initial states is always zero, we just need to always provide constant zero blobs.  Next we implement the  eachbatch  method from the  mx.AbstractDataProvider  interface for the provider. We start by defining the data and label arrays, and the  DataBatch  object we will provide in each iteration.  The actual data providing iteration is implemented as a Julia  coroutine . In this way, we can write the data loading logic as a simple coherent  for  loop, and do not need to implement the interface functions like Base.start, Base.next, etc.  Basically, we partition the text into batches, each batch containing several contiguous text sequences. Note at each time step, the LSTM is trained to predict the next character, so the label is the same as the data, but shifted ahead by one index.", 
            "title": "Data Provider for Text Sequences"
        }, 
        {
            "location": "/tutorial/char-lstm/#training-the-lstm", 
            "text": "Now we have implemented all the supporting infrastructures for our char-lstm. To train the model, we just follow the standard high-level API. Firstly, we construct a LSTM symbolic architecture:  Note all the parameters are defined in  examples/char-lstm/config.jl . Now we load the text file and define the data provider. The data  input.txt  we used in this example is  a tiny Shakespeare dataset . But you can try with other text files.  The last step is to construct a model, an optimizer and fit the mode to the data. We are using the ADAM optimizer [Adam]_ in this example.  Note we are also using a customized  NLL  evaluation metric, which calculate the negative log-likelihood during training. Here is an output sample at the end of the training process.  ...\nINFO: Speed: 357.72 samples/sec\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:                NLL = 1.4672\nINFO:         perplexity = 4.3373\nINFO:               time = 87.2631 seconds\nINFO: ## Validation summary\nINFO:                NLL = 1.6374\nINFO:         perplexity = 5.1418\nINFO: Saved checkpoint to 'char-lstm/checkpoints/ptb-0020.params'\nINFO: Speed: 368.74 samples/sec\nINFO: Speed: 361.04 samples/sec\nINFO: Speed: 360.02 samples/sec\nINFO: Speed: 362.34 samples/sec\nINFO: Speed: 360.80 samples/sec\nINFO: Speed: 362.77 samples/sec\nINFO: Speed: 357.18 samples/sec\nINFO: Speed: 355.30 samples/sec\nINFO: Speed: 362.33 samples/sec\nINFO: Speed: 359.23 samples/sec\nINFO: Speed: 358.09 samples/sec\nINFO: Speed: 356.89 samples/sec\nINFO: Speed: 371.91 samples/sec\nINFO: Speed: 372.24 samples/sec\nINFO: Speed: 356.59 samples/sec\nINFO: Speed: 356.64 samples/sec\nINFO: Speed: 360.24 samples/sec\nINFO: Speed: 360.32 samples/sec\nINFO: Speed: 362.38 samples/sec\nINFO: == Epoch 021 ==========\nINFO: ## Training summary\nINFO:                NLL = 1.4655\nINFO:         perplexity = 4.3297\nINFO:               time = 86.9243 seconds\nINFO: ## Validation summary\nINFO:                NLL = 1.6366\nINFO:         perplexity = 5.1378\nINFO: Saved checkpoint to 'examples/char-lstm/checkpoints/ptb-0021.params'", 
            "title": "Training the LSTM"
        }, 
        {
            "location": "/tutorial/char-lstm/#sampling-random-sentences", 
            "text": "After training the LSTM, we can now sample random sentences from the trained model. The sampler works in the following way:   Starting from some fixed character, take  a  for example, and feed   it as input to the LSTM.  The LSTM will produce an output distribution over the vocabulary and   a state in the first time step. We sample a character from the   output distribution, fix it as the second character.  In the next time step, we feed the previously sampled character as   input and continue running the LSTM by also taking the previous   states (instead of the 0 initial states).  Continue running until we sampled enough characters.   Note we are running with mini-batches, so several sentences could be sampled simultaneously. Here are some sampled outputs from a network I trained for around half an hour on the Shakespeare dataset. Note all the line-breaks, punctuations and upper-lower case letters are produced by the sampler itself. I did not do any post-processing.  ## Sample 1\nall have sir,\nAway will fill'd in His time, I'll keep her, do not madam, if they here? Some more ha?\n\n## Sample 2\nam.\n\nCLAUDIO:\nHone here, let her, the remedge, and I know not slept a likely, thou some soully free?\n\n## Sample 3\narrel which noble thing\nThe exchnachsureding worns: I ne'er drunken Biancas, fairer, than the lawfu?\n\n## Sample 4\naugh assalu, you'ld tell me corn;\nFarew. First, for me of a loved. Has thereat I knock you presents?\n\n## Sample 5\name the first answer.\n\nMARIZARINIO:\nDoor of Angelo as her lord, shrield liken Here fellow the fool ?\n\n## Sample 6\nad well.\n\nCLAUDIO:\nSoon him a fellows here; for her fine edge in a bogms' lord's wife.\n\nLUCENTIO:\nI?\n\n## Sample 7\nadrezilian measure.\n\nLUCENTIO:\nSo, help'd you hath nes have a than dream's corn, beautio, I perchas?\n\n## Sample 8\nas eatter me;\nThe girlly: and no other conciolation!\n\nBISTRUMIO:\nI have be rest girl. O, that I a h?\n\n## Sample 9\nand is intend you sort:\nWhat held her all 'clama's for maffice. Some servant.' what I say me the cu?\n\n## Sample 10\nan thoughts will said in our pleasue,\nNot scanin on him that you live; believaries she.\n\nISABELLLLL?  See  Andrej Karpathy's blog post  on more examples and links including Linux source codes, Algebraic Geometry Theorems, and even cooking recipes. The code for sampling can be found in  examples/char-lstm/sampler.jl .", 
            "title": "Sampling Random Sentences"
        }, 
        {
            "location": "/tutorial/char-lstm/#visualizing-the-lstm", 
            "text": "Finally, you could visualize the LSTM by calling to_graphviz on the constructed LSTM symbolic architecture. We only show an example of 1-layer and 2-time-step LSTM below. The automatic layout produced by GraphViz is definitely much less clear than  Christopher Olah's illustrations , but could otherwise be very useful for debugging. As we can see, the LSTM unfolded over time is just a (very) deep neural network. The complete code for producing this visualization can be found in  examples/char-lstm/visualize.jl .", 
            "title": "Visualizing the LSTM"
        }, 
        {
            "location": "/user-guide/install/", 
            "text": "Installation Guide\n\n\n\n\nAutomatic Installation\n\n\nTo install MXNet.jl, simply type\n\n\nPkg.add(\nMXNet\n)\n\n\n\n\nin the Julia REPL. Or to use the latest git version of MXNet.jl, use the following command instead\n\n\nPkg.checkout(\nMXNet\n)\n\n\n\n\nMXNet.jl is built on top of \nlibmxnet\n. Upon installation, Julia will try to automatically download and build libmxnet.\n\n\nThe libmxnet source is downloaded to \nPkg.dir(\"MXNet\")/deps/src/mxnet\n. The automatic build is using default configurations, with OpenCV, CUDA disabled. If the compilation failed due to unresolved dependency, or if you want to customize the build, you can compile and install libmxnet manually. Please see below for more details.\n\n\n\n\nManual Compilation\n\n\nIt is possible to compile libmxnet separately and point MXNet.jl to a the existing library in case automatic compilation fails due to unresolved dependencies in an un-standard environment; Or when one want to work with a seperate, maybe customized libmxnet.\n\n\nTo build libmxnet, please refer to \nthe installation guide of libmxnet\n. After successfully installing libmxnet, set the \nMXNET_HOME\n \nenvironment variable\n to the location of libmxnet. In other words, the compiled \nlibmxnet.so\n should be found in \n$MXNET_HOME/lib\n.\n\n\n\n\nnote\n\n\nThe constant \nMXNET_HOME\n is pre-compiled in MXNet.jl package cache. If you updated the environment variable after installing MXNet.jl, make sure to update the pre-compilation cache by \nBase.compilecache(\"MXNet\")\n.\n\n\n\n\nWhen the \nMXNET_HOME\n environment variable is detected and the corresponding \nlibmxnet.so\n could be loaded successfully, MXNet.jl will skip automatic building during installation and use the specified libmxnet instead.\n\n\nBasically, MXNet.jl will search \nlibmxnet.so\n or \nlibmxnet.dll\n in the following paths (and in that order):\n\n\n\n\n$MXNET_HOME/lib\n: customized libmxnet builds\n\n\nPkg.dir(\"MXNet\")/deps/usr/lib\n: automatic builds\n\n\nAny system wide library search path\n\n\n\n\nNote that MXNet.jl can not load \nlibmxnet.so\n even if it is on one of the paths above in case a library it depends upon is missing from the \nLD_LIBRARY_PATH\n. Thus, if you are going to compile to add CUDA, the path to the CUDA libraries will have to be added to \nLD_LIBRARY_PATH\n.", 
            "title": "Installation Guide"
        }, 
        {
            "location": "/user-guide/install/#installation-guide", 
            "text": "", 
            "title": "Installation Guide"
        }, 
        {
            "location": "/user-guide/install/#automatic-installation", 
            "text": "To install MXNet.jl, simply type  Pkg.add( MXNet )  in the Julia REPL. Or to use the latest git version of MXNet.jl, use the following command instead  Pkg.checkout( MXNet )  MXNet.jl is built on top of  libmxnet . Upon installation, Julia will try to automatically download and build libmxnet.  The libmxnet source is downloaded to  Pkg.dir(\"MXNet\")/deps/src/mxnet . The automatic build is using default configurations, with OpenCV, CUDA disabled. If the compilation failed due to unresolved dependency, or if you want to customize the build, you can compile and install libmxnet manually. Please see below for more details.", 
            "title": "Automatic Installation"
        }, 
        {
            "location": "/user-guide/install/#manual-compilation", 
            "text": "It is possible to compile libmxnet separately and point MXNet.jl to a the existing library in case automatic compilation fails due to unresolved dependencies in an un-standard environment; Or when one want to work with a seperate, maybe customized libmxnet.  To build libmxnet, please refer to  the installation guide of libmxnet . After successfully installing libmxnet, set the  MXNET_HOME   environment variable  to the location of libmxnet. In other words, the compiled  libmxnet.so  should be found in  $MXNET_HOME/lib .   note  The constant  MXNET_HOME  is pre-compiled in MXNet.jl package cache. If you updated the environment variable after installing MXNet.jl, make sure to update the pre-compilation cache by  Base.compilecache(\"MXNet\") .   When the  MXNET_HOME  environment variable is detected and the corresponding  libmxnet.so  could be loaded successfully, MXNet.jl will skip automatic building during installation and use the specified libmxnet instead.  Basically, MXNet.jl will search  libmxnet.so  or  libmxnet.dll  in the following paths (and in that order):   $MXNET_HOME/lib : customized libmxnet builds  Pkg.dir(\"MXNet\")/deps/usr/lib : automatic builds  Any system wide library search path   Note that MXNet.jl can not load  libmxnet.so  even if it is on one of the paths above in case a library it depends upon is missing from the  LD_LIBRARY_PATH . Thus, if you are going to compile to add CUDA, the path to the CUDA libraries will have to be added to  LD_LIBRARY_PATH .", 
            "title": "Manual Compilation"
        }, 
        {
            "location": "/user-guide/overview/", 
            "text": "Overview\n\n\n\n\nMXNet.jl Namespace\n\n\nMost the functions and types in MXNet.jl are organized in a flat namespace. Because many some functions are conflicting with existing names in the Julia Base module, we wrap them all in a \nmx\n module. The convention of accessing the MXNet.jl interface is the to use the \nmx.\n prefix explicitly:\n\n\nusing MXNet\n\nx = mx.zeros(2,3)              # MXNet NDArray\ny = zeros(eltype(x), size(x))  # Julia Array\ncopy!(y, x)                    # Overloaded function in Julia Base\nz = mx.ones(size(x), mx.gpu()) # MXNet NDArray on GPU\nmx.copy!(z, y)                 # Same as copy!(z, y)\n\n\n\n\nNote functions like \nsize\n, \ncopy!\n that is extensively overloaded for various types works out of the box. But functions like \nzeros\n and \nones\n will be ambiguous, so we always use the \nmx.\n prefix. If you prefer, the \nmx.\n prefix can be used explicitly for all MXNet.jl functions, including \nsize\n and \ncopy!\n as shown in the last line.\n\n\n\n\nLow Level Interface\n\n\n\n\nNDArrays\n\n\nNDArray is the basic building blocks of the actual computations in MXNet. It is like a Julia \nArray\n object, with some important differences listed here:\n\n\n\n\nThe actual data could live on different \nContext\n (e.g. GPUs). For   some contexts, iterating into the elements one by one is very slow,   thus indexing into NDArray is not supported in general. The easiest   way to inspect the contents of an NDArray is to use the \ncopy\n   function to copy the contents as a Julia \nArray\n.\n\n\nOperations on NDArray (including basic arithmetics and neural   network related operators) are executed in parallel with automatic   dependency tracking to ensure correctness.\n\n\nThere is no generics in NDArray, the \neltype\n is always   \nmx.MX_float\n. Because for applications in machine learning, single   precision floating point numbers are typical a best choice balancing   between precision, speed and portability. Also since libmxnet is   designed to support multiple languages as front-ends, it is much   simpler to implement with a fixed data type.\n\n\n\n\nWhile most of the computation is hidden in libmxnet by operators corresponding to various neural network layers. Getting familiar with the NDArray API is useful for implementing \nOptimizer\n or customized operators in Julia directly.\n\n\nThe followings are common ways to create NDArray objects:\n\n\n\n\nmx.empty(shape[, context])\n: create on uninitialized array of a   given shape on a specific device. For example,   \nmx.empty(2,3)\n, \nmx.((2,3), mx.gpu(2))\n.\n\n\nmx.zeros(shape[, context])\n and \nmx.ones(shape[, context])\n:   similar to the Julia's built-in \nzeros\n and \nones\n.\n\n\nmx.copy(jl_arr, context)\n: copy the contents of a Julia \nArray\n to   a specific device.\n\n\n\n\nMost of the convenient functions like \nsize\n, \nlength\n, \nndims\n, \neltype\n on array objects should work out-of-the-box. Although indexing is not supported, it is possible to take \nslices\n:\n\n\na = mx.ones(2,3)\nb = mx.slice(a, 1:2)\nb[:] = 2\nprintln(copy(a))\n# =\n\n# Float32[2.0 2.0 1.0\n#         2.0 2.0 1.0]\n\n\n\n\nA slice is a sub-region sharing the same memory with the original NDArray object. A slice is always a contiguous piece of memory, so only slicing on the \nlast\n dimension is supported. The example above also shows a way to set the contents of an NDArray.\n\n\na = mx.empty(2,3)\na[:] = 0.5              # set all elements to a scalar\na[:] = rand(size(a))    # set contents with a Julia Array\ncopy!(a, rand(size(a))) # set value by copying a Julia Array\nb = mx.empty(size(a))\nb[:] = a                # copying and assignment between NDArrays\n\n\n\n\nNote due to the intrinsic design of the Julia language, a normal assignment\n\n\na = b\n\n\n\n\ndoes \nnot\n mean copying the contents of \nb\n to \na\n. Instead, it just make the variable \na\n pointing to a new object, which is \nb\n. Similarly, inplace arithmetics does not work as expected:\n\n\na = mx.ones(2)\nr = a           # keep a reference to a\nb = mx.ones(2)\na += b          # translates to a = a + b\nprintln(copy(a))\n# =\n Float32[2.0f0,2.0f0]\nprintln(copy(r))\n# =\n Float32[1.0f0,1.0f0]\n\n\n\n\nAs we can see, \na\n has expected value, but instead of inplace updating, a new NDArray is created and \na\n is set to point to this new object. If we look at \nr\n, which still reference to the old \na\n, its content has not changed. There is currently no way in Julia to overload the operators like \n+=\n to get customized behavior.\n\n\nInstead, you will need to write \na[:] = a+b\n, or if you want \nreal\n inplace \n+=\n operation, MXNet.jl provides a simple macro \n@mx.inplace\n:\n\n\n@mx.inplace a += b\nmacroexpand(:(@mx.inplace a += b))\n# =\n :(MXNet.mx.add_to!(a,b))\n\n\n\n\nAs we can see, it translate the \n+=\n operator to an explicit \nadd_to!\n function call, which invokes into libmxnet to add the contents of \nb\n into \na\n directly. For example, the following is the update rule in the SGD \nOptimizer\n (both \ngrad\n and \nweight\n are NDArray objects):\n\n\n@inplace weight += -lr * (grad_scale * grad + self.weight_decay * weight)\n\n\n\n\nNote there is no much magic in \nmx.inplace\n: it only does a shallow translation. In the SGD update rule example above, the computation like scaling the gradient by \ngrad_scale\n and adding the weight decay all create temporary NDArray objects. To mitigate this issue, libmxnet has a customized memory allocator designed specifically to handle this kind of situations. The following snippet does a simple benchmark on allocating temp NDArray vs. pre-allocating:\n\n\nusing Benchmark\nusing MXNet\n\nN_REP = 1000\nSHAPE = (128, 64)\nCTX   = mx.cpu()\nLR    = 0.1\n\nfunction inplace_op()\n  weight = mx.zeros(SHAPE, CTX)\n  grad   = mx.ones(SHAPE, CTX)\n\n  # pre-allocate temp objects\n  grad_lr = mx.empty(SHAPE, CTX)\n\n  for i = 1:N_REP\n    copy!(grad_lr, grad)\n    @mx.inplace grad_lr .*= LR\n    @mx.inplace weight -= grad_lr\n  end\n  return weight\nend\n\nfunction normal_op()\n  weight = mx.zeros(SHAPE, CTX)\n  grad   = mx.ones(SHAPE, CTX)\n\n  for i = 1:N_REP\n    weight[:] -= LR * grad\n  end\n  return weight\nend\n\n# make sure the results are the same\n@assert(maximum(abs(copy(normal_op() - inplace_op()))) \n 1e-6)\n\nprintln(compare([inplace_op, normal_op], 100))\n\n\n\n\nThe comparison on my laptop shows that \nnormal_op\n while allocating a lot of temp NDArray in the loop (the performance gets worse when increasing \nN_REP\n), is only about twice slower than the pre-allocated one.\n\n\n\n\n\n\n\n\nRow\n\n\nFunction\n\n\nAverage\n\n\nRelative\n\n\nReplications\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\"inplace_op\"\n\n\n0.0074854\n\n\n1.0\n\n\n100\n\n\n\n\n\n\n2\n\n\n\"normal_op\"\n\n\n0.0174202\n\n\n2.32723\n\n\n100\n\n\n\n\n\n\n\n\nSo it will usually not be a big problem unless you are at the bottleneck of the computation.\n\n\n\n\nDistributed Key-value Store\n\n\nThe type \nKVStore\n and related methods are used for data sharing across different devices or machines. It provides a simple and efficient integer - NDArray key-value storage system that each device can pull or push.\n\n\nThe following example shows how to create a local \nKVStore\n, initialize a value and then pull it back.\n\n\nkv    = mx.KVStore(:local)\nshape = (2,3)\nkey   = 3\n\nmx.init!(kv, key, mx.ones(shape)*2)\na = mx.empty(shape)\nmx.pull!(kv, key, a) # pull value into a\nprintln(copy(a))\n# =\n\n# Float32[2.0 2.0 2.0\n#        2.0 2.0 2.0]\n\n\n\n\n\n\nIntermediate Level Interface\n\n\n\n\nSymbols and Composition\n\n\nThe way we build deep learning models in MXNet.jl is to use the powerful symbolic composition system. It is like \nTheano\n, except that we avoided long expression compiliation time by providing \nlarger\n neural network related building blocks to guarantee computation performance. See also \nthis note\n for the design and trade-off of the MXNet symbolic composition system.\n\n\nThe basic type is \nmx.Symbol\n. The following is a trivial example of composing two symbols with the \n+\n operation.\n\n\nA = mx.Variable(:A)\nB = mx.Variable(:B)\nC = A + B\n\n\n\n\nWe get a new \nsymbol\n by composing existing \nsymbols\n by some \noperations\n. A hierarchical architecture of a deep neural network could be realized by recursive composition. For example, the following code snippet shows a simple 2-layer MLP construction, using a hidden layer of 128 units and a ReLU activation function.\n\n\nnet = mx.Variable(:data)\nnet = mx.FullyConnected(data=net, name=:fc1, num_hidden=128)\nnet = mx.Activation(data=net, name=:relu1, act_type=:relu)\nnet = mx.FullyConnected(data=net, name=:fc2, num_hidden=64)\nnet = mx.Softmax(data=net, name=:out)\n\n\n\n\nEach time we take the previous symbol, and compose with an operation. Unlike the simple \n+\n example above, the \noperations\n here are \"bigger\" ones, that correspond to common computation layers in deep neural networks.\n\n\nEach of those operation takes one or more input symbols for composition, with optional hyper-parameters (e.g. \nnum_hidden\n, \nact_type\n) to further customize the composition results.\n\n\nWhen applying those operations, we can also specify a \nname\n for the result symbol. This is convenient if we want to refer to this symbol later on. If not supplied, a name will be automatically generated.\n\n\nEach symbol takes some arguments. For example, in the \n+\n case above, to compute the value of \nC\n, we will need to know the values of the two inputs \nA\n and \nB\n. For neural networks, the arguments are primarily two categories: \ninputs\n and \nparameters\n. \ninputs\n are data and labels for the networks, while \nparameters\n are typically trainable \nweights\n, \nbias\n, \nfilters\n.\n\n\nWhen composing symbols, their arguments accumulates. We can list all the arguments by\n\n\njulia\n mx.list_arguments(net)\n6-element Array{Symbol,1}:\n :data         # Input data, name from the first data variable\n :fc1_weight   # Weights of the fully connected layer named :fc1\n :fc1_bias     # Bias of the layer :fc1\n :fc2_weight   # Weights of the layer :fc2\n :fc2_bias     # Bias of the layer :fc2\n :out_label    # Input label, required by the softmax layer named :out\n\n\n\n\nNote the names of the arguments are generated according to the provided name for each layer. We can also specify those names explicitly:\n\n\nnet = mx.Variable(:data)\nw   = mx.Variable(:myweight)\nnet = mx.FullyConnected(data=data, weight=w, name=:fc1, num_hidden=128)\nmx.list_arguments(net)\n# =\n\n# 3-element Array{Symbol,1}:\n#  :data\n#  :myweight\n#  :fc1_bias\n\n\n\n\nThe simple fact is that a \nVariable\n is just a placeholder \nmx.Symbol\n. In composition, we can use arbitrary symbols for arguments. For example:\n\n\nnet  = mx.Variable(:data)\nnet  = mx.FullyConnected(data=net, name=:fc1, num_hidden=128)\nnet2 = mx.Variable(:data2)\nnet2 = mx.FullyConnected(data=net2, name=:net2, num_hidden=128)\nmx.list_arguments(net2)\n# =\n\n# 3-element Array{Symbol,1}:\n#  :data2\n#  :net2_weight\n#  :net2_bias\ncomposed_net = net2(data2=net, name=:composed)\nmx.list_arguments(composed_net)\n# =\n\n# 5-element Array{Symbol,1}:\n#  :data\n#  :fc1_weight\n#  :fc1_bias\n#  :net2_weight\n#  :net2_bias\n\n\n\n\nNote we use a composed symbol, \nnet\n as the argument \ndata2\n for \nnet2\n to get a new symbol, which we named \n:composed\n. It also shows that a symbol itself is a call-able object, which can be invoked to fill in missing arguments and get more complicated symbol compositions.\n\n\n\n\nShape Inference\n\n\nGiven enough information, the shapes of all arguments in a composed symbol could be inferred automatically. For example, given the input shape, and some hyper-parameters like \nnum_hidden\n, the shapes for the weights and bias in a neural network could be inferred.\n\n\nnet = mx.Variable(:data)\nnet = mx.FullyConnected(data=net, name=:fc1, num_hidden=10)\narg_shapes, out_shapes, aux_shapes = mx.infer_shape(net, data=(10, 64))\n\n\n\n\nThe returned shapes corresponds to arguments with the same order as returned by \nmx.list_arguments\n. The \nout_shapes\n are shapes for outputs, and \naux_shapes\n can be safely ignored for now.\n\n\nfor (n,s) in zip(mx.list_arguments(net), arg_shapes)\n  println(\n$n =\n $s\n)\nend\n# =\n\n# data =\n (10,64)\n# fc1_weight =\n (10,10)\n# fc1_bias =\n (10,)\nfor (n,s) in zip(mx.list_outputs(net), out_shapes)\n  println(\n$n =\n $s\n)\nend\n# =\n\n# fc1_output =\n (10,64)\n\n\n\n\n\n\nBinding and Executing\n\n\nIn order to execute the computation graph specified a composed symbol, we will \nbind\n the free variables to concrete values, specified as \nmx.NDArray\n. This will create an \nmx.Executor\n on a given \nmx.Context\n. A context describes the computation devices (CPUs, GPUs, etc.) and an executor will carry out the computation (forward/backward) specified in the corresponding symbolic composition.\n\n\nA = mx.Variable(:A)\nB = mx.Variable(:B)\nC = A .* B\na = mx.ones(3) * 4\nb = mx.ones(3) * 2\nc_exec = mx.bind(C, context=mx.cpu(), args=Dict(:A =\n a, :B =\n b))\n\nmx.forward(c_exec)\ncopy(c_exec.outputs[1])  # copy turns NDArray into Julia Array\n# =\n\n# 3-element Array{Float32,1}:\n#  8.0\n#  8.0\n#  8.0\n\n\n\n\nFor neural networks, it is easier to use \nsimple_bind\n. By providing the shape for input arguments, it will perform a shape inference for the rest of the arguments and create the NDArray automatically. In practice, the binding and executing steps are hidden under the \nModel\n interface.\n\n\nTODO\n Provide pointers to model tutorial and further details about binding and symbolic API.\n\n\n\n\nHigh Level Interface\n\n\nThe high level interface include model training and prediction API, etc.", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/overview/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/overview/#mxnetjl-namespace", 
            "text": "Most the functions and types in MXNet.jl are organized in a flat namespace. Because many some functions are conflicting with existing names in the Julia Base module, we wrap them all in a  mx  module. The convention of accessing the MXNet.jl interface is the to use the  mx.  prefix explicitly:  using MXNet\n\nx = mx.zeros(2,3)              # MXNet NDArray\ny = zeros(eltype(x), size(x))  # Julia Array\ncopy!(y, x)                    # Overloaded function in Julia Base\nz = mx.ones(size(x), mx.gpu()) # MXNet NDArray on GPU\nmx.copy!(z, y)                 # Same as copy!(z, y)  Note functions like  size ,  copy!  that is extensively overloaded for various types works out of the box. But functions like  zeros  and  ones  will be ambiguous, so we always use the  mx.  prefix. If you prefer, the  mx.  prefix can be used explicitly for all MXNet.jl functions, including  size  and  copy!  as shown in the last line.", 
            "title": "MXNet.jl Namespace"
        }, 
        {
            "location": "/user-guide/overview/#low-level-interface", 
            "text": "", 
            "title": "Low Level Interface"
        }, 
        {
            "location": "/user-guide/overview/#ndarrays", 
            "text": "NDArray is the basic building blocks of the actual computations in MXNet. It is like a Julia  Array  object, with some important differences listed here:   The actual data could live on different  Context  (e.g. GPUs). For   some contexts, iterating into the elements one by one is very slow,   thus indexing into NDArray is not supported in general. The easiest   way to inspect the contents of an NDArray is to use the  copy    function to copy the contents as a Julia  Array .  Operations on NDArray (including basic arithmetics and neural   network related operators) are executed in parallel with automatic   dependency tracking to ensure correctness.  There is no generics in NDArray, the  eltype  is always    mx.MX_float . Because for applications in machine learning, single   precision floating point numbers are typical a best choice balancing   between precision, speed and portability. Also since libmxnet is   designed to support multiple languages as front-ends, it is much   simpler to implement with a fixed data type.   While most of the computation is hidden in libmxnet by operators corresponding to various neural network layers. Getting familiar with the NDArray API is useful for implementing  Optimizer  or customized operators in Julia directly.  The followings are common ways to create NDArray objects:   mx.empty(shape[, context]) : create on uninitialized array of a   given shape on a specific device. For example,    mx.empty(2,3) ,  mx.((2,3), mx.gpu(2)) .  mx.zeros(shape[, context])  and  mx.ones(shape[, context]) :   similar to the Julia's built-in  zeros  and  ones .  mx.copy(jl_arr, context) : copy the contents of a Julia  Array  to   a specific device.   Most of the convenient functions like  size ,  length ,  ndims ,  eltype  on array objects should work out-of-the-box. Although indexing is not supported, it is possible to take  slices :  a = mx.ones(2,3)\nb = mx.slice(a, 1:2)\nb[:] = 2\nprintln(copy(a))\n# = \n# Float32[2.0 2.0 1.0\n#         2.0 2.0 1.0]  A slice is a sub-region sharing the same memory with the original NDArray object. A slice is always a contiguous piece of memory, so only slicing on the  last  dimension is supported. The example above also shows a way to set the contents of an NDArray.  a = mx.empty(2,3)\na[:] = 0.5              # set all elements to a scalar\na[:] = rand(size(a))    # set contents with a Julia Array\ncopy!(a, rand(size(a))) # set value by copying a Julia Array\nb = mx.empty(size(a))\nb[:] = a                # copying and assignment between NDArrays  Note due to the intrinsic design of the Julia language, a normal assignment  a = b  does  not  mean copying the contents of  b  to  a . Instead, it just make the variable  a  pointing to a new object, which is  b . Similarly, inplace arithmetics does not work as expected:  a = mx.ones(2)\nr = a           # keep a reference to a\nb = mx.ones(2)\na += b          # translates to a = a + b\nprintln(copy(a))\n# =  Float32[2.0f0,2.0f0]\nprintln(copy(r))\n# =  Float32[1.0f0,1.0f0]  As we can see,  a  has expected value, but instead of inplace updating, a new NDArray is created and  a  is set to point to this new object. If we look at  r , which still reference to the old  a , its content has not changed. There is currently no way in Julia to overload the operators like  +=  to get customized behavior.  Instead, you will need to write  a[:] = a+b , or if you want  real  inplace  +=  operation, MXNet.jl provides a simple macro  @mx.inplace :  @mx.inplace a += b\nmacroexpand(:(@mx.inplace a += b))\n# =  :(MXNet.mx.add_to!(a,b))  As we can see, it translate the  +=  operator to an explicit  add_to!  function call, which invokes into libmxnet to add the contents of  b  into  a  directly. For example, the following is the update rule in the SGD  Optimizer  (both  grad  and  weight  are NDArray objects):  @inplace weight += -lr * (grad_scale * grad + self.weight_decay * weight)  Note there is no much magic in  mx.inplace : it only does a shallow translation. In the SGD update rule example above, the computation like scaling the gradient by  grad_scale  and adding the weight decay all create temporary NDArray objects. To mitigate this issue, libmxnet has a customized memory allocator designed specifically to handle this kind of situations. The following snippet does a simple benchmark on allocating temp NDArray vs. pre-allocating:  using Benchmark\nusing MXNet\n\nN_REP = 1000\nSHAPE = (128, 64)\nCTX   = mx.cpu()\nLR    = 0.1\n\nfunction inplace_op()\n  weight = mx.zeros(SHAPE, CTX)\n  grad   = mx.ones(SHAPE, CTX)\n\n  # pre-allocate temp objects\n  grad_lr = mx.empty(SHAPE, CTX)\n\n  for i = 1:N_REP\n    copy!(grad_lr, grad)\n    @mx.inplace grad_lr .*= LR\n    @mx.inplace weight -= grad_lr\n  end\n  return weight\nend\n\nfunction normal_op()\n  weight = mx.zeros(SHAPE, CTX)\n  grad   = mx.ones(SHAPE, CTX)\n\n  for i = 1:N_REP\n    weight[:] -= LR * grad\n  end\n  return weight\nend\n\n# make sure the results are the same\n@assert(maximum(abs(copy(normal_op() - inplace_op())))   1e-6)\n\nprintln(compare([inplace_op, normal_op], 100))  The comparison on my laptop shows that  normal_op  while allocating a lot of temp NDArray in the loop (the performance gets worse when increasing  N_REP ), is only about twice slower than the pre-allocated one.     Row  Function  Average  Relative  Replications      1  \"inplace_op\"  0.0074854  1.0  100    2  \"normal_op\"  0.0174202  2.32723  100     So it will usually not be a big problem unless you are at the bottleneck of the computation.", 
            "title": "NDArrays"
        }, 
        {
            "location": "/user-guide/overview/#distributed-key-value-store", 
            "text": "The type  KVStore  and related methods are used for data sharing across different devices or machines. It provides a simple and efficient integer - NDArray key-value storage system that each device can pull or push.  The following example shows how to create a local  KVStore , initialize a value and then pull it back.  kv    = mx.KVStore(:local)\nshape = (2,3)\nkey   = 3\n\nmx.init!(kv, key, mx.ones(shape)*2)\na = mx.empty(shape)\nmx.pull!(kv, key, a) # pull value into a\nprintln(copy(a))\n# = \n# Float32[2.0 2.0 2.0\n#        2.0 2.0 2.0]", 
            "title": "Distributed Key-value Store"
        }, 
        {
            "location": "/user-guide/overview/#intermediate-level-interface", 
            "text": "", 
            "title": "Intermediate Level Interface"
        }, 
        {
            "location": "/user-guide/overview/#symbols-and-composition", 
            "text": "The way we build deep learning models in MXNet.jl is to use the powerful symbolic composition system. It is like  Theano , except that we avoided long expression compiliation time by providing  larger  neural network related building blocks to guarantee computation performance. See also  this note  for the design and trade-off of the MXNet symbolic composition system.  The basic type is  mx.Symbol . The following is a trivial example of composing two symbols with the  +  operation.  A = mx.Variable(:A)\nB = mx.Variable(:B)\nC = A + B  We get a new  symbol  by composing existing  symbols  by some  operations . A hierarchical architecture of a deep neural network could be realized by recursive composition. For example, the following code snippet shows a simple 2-layer MLP construction, using a hidden layer of 128 units and a ReLU activation function.  net = mx.Variable(:data)\nnet = mx.FullyConnected(data=net, name=:fc1, num_hidden=128)\nnet = mx.Activation(data=net, name=:relu1, act_type=:relu)\nnet = mx.FullyConnected(data=net, name=:fc2, num_hidden=64)\nnet = mx.Softmax(data=net, name=:out)  Each time we take the previous symbol, and compose with an operation. Unlike the simple  +  example above, the  operations  here are \"bigger\" ones, that correspond to common computation layers in deep neural networks.  Each of those operation takes one or more input symbols for composition, with optional hyper-parameters (e.g.  num_hidden ,  act_type ) to further customize the composition results.  When applying those operations, we can also specify a  name  for the result symbol. This is convenient if we want to refer to this symbol later on. If not supplied, a name will be automatically generated.  Each symbol takes some arguments. For example, in the  +  case above, to compute the value of  C , we will need to know the values of the two inputs  A  and  B . For neural networks, the arguments are primarily two categories:  inputs  and  parameters .  inputs  are data and labels for the networks, while  parameters  are typically trainable  weights ,  bias ,  filters .  When composing symbols, their arguments accumulates. We can list all the arguments by  julia  mx.list_arguments(net)\n6-element Array{Symbol,1}:\n :data         # Input data, name from the first data variable\n :fc1_weight   # Weights of the fully connected layer named :fc1\n :fc1_bias     # Bias of the layer :fc1\n :fc2_weight   # Weights of the layer :fc2\n :fc2_bias     # Bias of the layer :fc2\n :out_label    # Input label, required by the softmax layer named :out  Note the names of the arguments are generated according to the provided name for each layer. We can also specify those names explicitly:  net = mx.Variable(:data)\nw   = mx.Variable(:myweight)\nnet = mx.FullyConnected(data=data, weight=w, name=:fc1, num_hidden=128)\nmx.list_arguments(net)\n# = \n# 3-element Array{Symbol,1}:\n#  :data\n#  :myweight\n#  :fc1_bias  The simple fact is that a  Variable  is just a placeholder  mx.Symbol . In composition, we can use arbitrary symbols for arguments. For example:  net  = mx.Variable(:data)\nnet  = mx.FullyConnected(data=net, name=:fc1, num_hidden=128)\nnet2 = mx.Variable(:data2)\nnet2 = mx.FullyConnected(data=net2, name=:net2, num_hidden=128)\nmx.list_arguments(net2)\n# = \n# 3-element Array{Symbol,1}:\n#  :data2\n#  :net2_weight\n#  :net2_bias\ncomposed_net = net2(data2=net, name=:composed)\nmx.list_arguments(composed_net)\n# = \n# 5-element Array{Symbol,1}:\n#  :data\n#  :fc1_weight\n#  :fc1_bias\n#  :net2_weight\n#  :net2_bias  Note we use a composed symbol,  net  as the argument  data2  for  net2  to get a new symbol, which we named  :composed . It also shows that a symbol itself is a call-able object, which can be invoked to fill in missing arguments and get more complicated symbol compositions.", 
            "title": "Symbols and Composition"
        }, 
        {
            "location": "/user-guide/overview/#shape-inference", 
            "text": "Given enough information, the shapes of all arguments in a composed symbol could be inferred automatically. For example, given the input shape, and some hyper-parameters like  num_hidden , the shapes for the weights and bias in a neural network could be inferred.  net = mx.Variable(:data)\nnet = mx.FullyConnected(data=net, name=:fc1, num_hidden=10)\narg_shapes, out_shapes, aux_shapes = mx.infer_shape(net, data=(10, 64))  The returned shapes corresponds to arguments with the same order as returned by  mx.list_arguments . The  out_shapes  are shapes for outputs, and  aux_shapes  can be safely ignored for now.  for (n,s) in zip(mx.list_arguments(net), arg_shapes)\n  println( $n =  $s )\nend\n# = \n# data =  (10,64)\n# fc1_weight =  (10,10)\n# fc1_bias =  (10,)\nfor (n,s) in zip(mx.list_outputs(net), out_shapes)\n  println( $n =  $s )\nend\n# = \n# fc1_output =  (10,64)", 
            "title": "Shape Inference"
        }, 
        {
            "location": "/user-guide/overview/#binding-and-executing", 
            "text": "In order to execute the computation graph specified a composed symbol, we will  bind  the free variables to concrete values, specified as  mx.NDArray . This will create an  mx.Executor  on a given  mx.Context . A context describes the computation devices (CPUs, GPUs, etc.) and an executor will carry out the computation (forward/backward) specified in the corresponding symbolic composition.  A = mx.Variable(:A)\nB = mx.Variable(:B)\nC = A .* B\na = mx.ones(3) * 4\nb = mx.ones(3) * 2\nc_exec = mx.bind(C, context=mx.cpu(), args=Dict(:A =  a, :B =  b))\n\nmx.forward(c_exec)\ncopy(c_exec.outputs[1])  # copy turns NDArray into Julia Array\n# = \n# 3-element Array{Float32,1}:\n#  8.0\n#  8.0\n#  8.0  For neural networks, it is easier to use  simple_bind . By providing the shape for input arguments, it will perform a shape inference for the rest of the arguments and create the NDArray automatically. In practice, the binding and executing steps are hidden under the  Model  interface.  TODO  Provide pointers to model tutorial and further details about binding and symbolic API.", 
            "title": "Binding and Executing"
        }, 
        {
            "location": "/user-guide/overview/#high-level-interface", 
            "text": "The high level interface include model training and prediction API, etc.", 
            "title": "High Level Interface"
        }, 
        {
            "location": "/user-guide/faq/", 
            "text": "FAQ\n\n\n\n\nRunning MXNet on AWS GPU instances\n\n\nSee the discussions and notes \nhere\n.", 
            "title": "FAQ"
        }, 
        {
            "location": "/user-guide/faq/#faq", 
            "text": "", 
            "title": "FAQ"
        }, 
        {
            "location": "/user-guide/faq/#running-mxnet-on-aws-gpu-instances", 
            "text": "See the discussions and notes  here .", 
            "title": "Running MXNet on AWS GPU instances"
        }, 
        {
            "location": "/api/context/", 
            "text": "Context\n\n\n#\n\n\nMXNet.mx.Context\n \n \nType\n.\n\n\nContext(dev_type, dev_id)\n\n\n\n\nA context describes the device type and id on which computation should be carried on.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.cpu\n \n \nFunction\n.\n\n\ncpu(dev_id)\n\n\n\n\nGet a CPU context with a specific id. \ncpu()\n is usually the default context for many operations when no context is specified.\n\n\nArguments\n\n\n\n\ndev_id::Int = 0\n: the CPU id.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.gpu\n \n \nFunction\n.\n\n\ngpu(dev_id)\n\n\n\n\nGet a GPU context with a specific id. The K GPUs on a node is typically numbered as 0,...,K-1.\n\n\nArguments\n\n\n\n\ndev_id :: Int = 0\n the GPU device id.\n\n\n\n\nsource", 
            "title": "Context"
        }, 
        {
            "location": "/api/context/#context", 
            "text": "#  MXNet.mx.Context     Type .  Context(dev_type, dev_id)  A context describes the device type and id on which computation should be carried on.  source  #  MXNet.mx.cpu     Function .  cpu(dev_id)  Get a CPU context with a specific id.  cpu()  is usually the default context for many operations when no context is specified.  Arguments   dev_id::Int = 0 : the CPU id.   source  #  MXNet.mx.gpu     Function .  gpu(dev_id)  Get a GPU context with a specific id. The K GPUs on a node is typically numbered as 0,...,K-1.  Arguments   dev_id :: Int = 0  the GPU device id.   source", 
            "title": "Context"
        }, 
        {
            "location": "/api/model/", 
            "text": "Model\n\n\nThe model API provides convenient high-level interface to do training and predicting on a network described using the symbolic API.\n\n\n#\n\n\nMXNet.mx.AbstractModel\n \n \nType\n.\n\n\nAbstractModel\n\n\n\n\nThe abstract super type of all models in MXNet.jl.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.FeedForward\n \n \nType\n.\n\n\nFeedForward\n\n\n\n\nThe feedforward model provides convenient interface to train and predict on feedforward architectures like multi-layer MLP, ConvNets, etc. There is no explicitly handling of \ntime index\n, but it is relatively easy to implement unrolled RNN / LSTM under this framework (\nTODO\n: add example). For models that handles sequential data explicitly, please use \nTODO\n...\n\n\nsource\n\n\n#\n\n\nMXNet.mx.FeedForward\n \n \nMethod\n.\n\n\nFeedForward(arch :: SymbolicNode, ctx)\n\n\n\n\nArguments:\n\n\n\n\narch\n: the architecture of the network constructed using the symbolic API.\n\n\nctx\n: the devices on which this model should do computation. It could be a single \nContext\n        or a list of \nContext\n objects. In the latter case, data parallelization will be used        for training. If no context is provided, the default context \ncpu()\n will be used.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._split_inputs\n \n \nMethod\n.\n\n\nGet a split of \nbatch_size\n into \nn_split\n pieces for data parallelization. Returns a vector of length \nn_split\n, with each entry a \nUnitRange{Int}\n indicating the slice index for that piece.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.fit\n \n \nMethod\n.\n\n\nfit(model :: FeedForward, optimizer, data; kwargs...)\n\n\n\n\nTrain the \nmodel\n on \ndata\n with the \noptimizer\n.\n\n\n\n\nmodel::FeedForward\n: the model to be trained.\n\n\noptimizer::AbstractOptimizer\n: the optimization algorithm to use.\n\n\ndata::AbstractDataProvider\n: the training data provider.\n\n\nn_epoch::Int\n: default 10, the number of full data-passes to run.\n\n\neval_data::AbstractDataProvider\n: keyword argument, default \nnothing\n. The data provider for         the validation set.\n\n\neval_metric::AbstractEvalMetric\n: keyword argument, default \nAccuracy()\n. The metric used         to evaluate the training performance. If \neval_data\n is provided, the same metric is also         calculated on the validation set.\n\n\nkvstore\n: keyword argument, default \n:local\n. The key-value store used to synchronize gradients         and parameters when multiple devices are used for training.  :type kvstore: \nKVStore\n or \nBase.Symbol\n\n\ninitializer::AbstractInitializer\n: keyword argument, default \nUniformInitializer(0.01)\n.\n\n\nforce_init::Bool\n: keyword argument, default false. By default, the random initialization using the         provided \ninitializer\n will be skipped if the model weights already exists, maybe from a previous         call to \ntrain\n or an explicit call to \ninit_model\n or \nload_checkpoint\n. When         this option is set, it will always do random initialization at the begining of training.\n\n\ncallbacks::Vector{AbstractCallback}\n: keyword argument, default \n[]\n. Callbacks to be invoked at each epoch or mini-batch,         see \nAbstractCallback\n.\n\n\nverbosity::Int\n: Determines the verbosity of the print messages. Higher numbers         leads to more verbose printing. Acceptable values are         - \n0\n: Do not print anything during training         - \n1\n: Print starting and final messages         - \n2\n: Print one time messages and a message at the start of each epoch         - \n3\n: Print a summary of the training and validation accuracy for each epoch\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.init_model\n \n \nMethod\n.\n\n\ninit_model(self, initializer; overwrite=false, input_shapes...)\n\n\n\n\nInitialize the weights in the model.\n\n\nThis method will be called automatically when training a model. So there is usually no need to call this method unless one needs to inspect a model with only randomly initialized weights.\n\n\nArguments:\n\n\n\n\nself::FeedForward\n: the model to be initialized.\n\n\ninitializer::AbstractInitializer\n: an initializer describing how the weights should be initialized.\n\n\noverwrite::Bool\n: keyword argument, force initialization even when weights already exists.\n\n\ninput_shapes\n: the shape of all data and label inputs to this model, given as keyword arguments.                 For example, \ndata=(28,28,1,100), label=(100,)\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.predict\n \n \nMethod\n.\n\n\npredict(self, data; overwrite=false, callback=nothing)\n\n\n\n\nPredict using an existing model. The model should be already initialized, or trained or loaded from a checkpoint. There is an overloaded function that allows to pass the callback as the first argument, so it is possible to do\n\n\npredict(model, data) do batch_output\n  # consume or write batch_output to file\nend\n\n\n\n\nArguments:\n\n\n\n\nself::FeedForward\n:  the model.\n\n\ndata::AbstractDataProvider\n: the data to perform prediction on.\n\n\noverwrite::Bool\n: an \nExecutor\n is initialized the first time predict is called. The memory                    allocation of the \nExecutor\n depends on the mini-batch size of the test                    data provider. If you call predict twice with data provider of the same batch-size,                    then the executor can be potentially be re-used. So, if \noverwrite\n is false,                    we will try to re-use, and raise an error if batch-size changed. If \noverwrite\n                    is true (the default), a new \nExecutor\n will be created to replace the old one.\n\n\n\n\n\n\nNote\n\n\nPrediction is computationally much less costly than training, so the bottleneck sometimes becomes the IO for copying mini-batches of data. Since there is no concern about convergence in prediction, it is better to set the mini-batch size as large as possible (limited by your device memory) if prediction speed is a concern.\n\n\nFor the same reason, currently prediction will only use the first device even if multiple devices are provided to construct the model.\n\n\n\n\n\n\nNote\n\n\nIf you perform further after prediction. The weights are not automatically synchronized if \noverwrite\n is set to false and the old predictor is re-used. In this case setting \noverwrite\n to true (the default) will re-initialize the predictor the next time you call predict and synchronize the weights again.\n\n\n\n\nSee also \ntrain\n, \nfit\n, \ninit_model\n, and \nload_checkpoint\n\n\nsource\n\n\n#\n\n\nMXNet.mx.train\n \n \nMethod\n.\n\n\ntrain(model :: FeedForward, ...)\n\n\n\n\nAlias to \nfit\n.\n\n\nsource", 
            "title": "Models"
        }, 
        {
            "location": "/api/model/#model", 
            "text": "The model API provides convenient high-level interface to do training and predicting on a network described using the symbolic API.  #  MXNet.mx.AbstractModel     Type .  AbstractModel  The abstract super type of all models in MXNet.jl.  source  #  MXNet.mx.FeedForward     Type .  FeedForward  The feedforward model provides convenient interface to train and predict on feedforward architectures like multi-layer MLP, ConvNets, etc. There is no explicitly handling of  time index , but it is relatively easy to implement unrolled RNN / LSTM under this framework ( TODO : add example). For models that handles sequential data explicitly, please use  TODO ...  source  #  MXNet.mx.FeedForward     Method .  FeedForward(arch :: SymbolicNode, ctx)  Arguments:   arch : the architecture of the network constructed using the symbolic API.  ctx : the devices on which this model should do computation. It could be a single  Context         or a list of  Context  objects. In the latter case, data parallelization will be used        for training. If no context is provided, the default context  cpu()  will be used.   source  #  MXNet.mx._split_inputs     Method .  Get a split of  batch_size  into  n_split  pieces for data parallelization. Returns a vector of length  n_split , with each entry a  UnitRange{Int}  indicating the slice index for that piece.  source  #  MXNet.mx.fit     Method .  fit(model :: FeedForward, optimizer, data; kwargs...)  Train the  model  on  data  with the  optimizer .   model::FeedForward : the model to be trained.  optimizer::AbstractOptimizer : the optimization algorithm to use.  data::AbstractDataProvider : the training data provider.  n_epoch::Int : default 10, the number of full data-passes to run.  eval_data::AbstractDataProvider : keyword argument, default  nothing . The data provider for         the validation set.  eval_metric::AbstractEvalMetric : keyword argument, default  Accuracy() . The metric used         to evaluate the training performance. If  eval_data  is provided, the same metric is also         calculated on the validation set.  kvstore : keyword argument, default  :local . The key-value store used to synchronize gradients         and parameters when multiple devices are used for training.  :type kvstore:  KVStore  or  Base.Symbol  initializer::AbstractInitializer : keyword argument, default  UniformInitializer(0.01) .  force_init::Bool : keyword argument, default false. By default, the random initialization using the         provided  initializer  will be skipped if the model weights already exists, maybe from a previous         call to  train  or an explicit call to  init_model  or  load_checkpoint . When         this option is set, it will always do random initialization at the begining of training.  callbacks::Vector{AbstractCallback} : keyword argument, default  [] . Callbacks to be invoked at each epoch or mini-batch,         see  AbstractCallback .  verbosity::Int : Determines the verbosity of the print messages. Higher numbers         leads to more verbose printing. Acceptable values are         -  0 : Do not print anything during training         -  1 : Print starting and final messages         -  2 : Print one time messages and a message at the start of each epoch         -  3 : Print a summary of the training and validation accuracy for each epoch   source  #  MXNet.mx.init_model     Method .  init_model(self, initializer; overwrite=false, input_shapes...)  Initialize the weights in the model.  This method will be called automatically when training a model. So there is usually no need to call this method unless one needs to inspect a model with only randomly initialized weights.  Arguments:   self::FeedForward : the model to be initialized.  initializer::AbstractInitializer : an initializer describing how the weights should be initialized.  overwrite::Bool : keyword argument, force initialization even when weights already exists.  input_shapes : the shape of all data and label inputs to this model, given as keyword arguments.                 For example,  data=(28,28,1,100), label=(100,) .   source  #  MXNet.mx.predict     Method .  predict(self, data; overwrite=false, callback=nothing)  Predict using an existing model. The model should be already initialized, or trained or loaded from a checkpoint. There is an overloaded function that allows to pass the callback as the first argument, so it is possible to do  predict(model, data) do batch_output\n  # consume or write batch_output to file\nend  Arguments:   self::FeedForward :  the model.  data::AbstractDataProvider : the data to perform prediction on.  overwrite::Bool : an  Executor  is initialized the first time predict is called. The memory                    allocation of the  Executor  depends on the mini-batch size of the test                    data provider. If you call predict twice with data provider of the same batch-size,                    then the executor can be potentially be re-used. So, if  overwrite  is false,                    we will try to re-use, and raise an error if batch-size changed. If  overwrite                     is true (the default), a new  Executor  will be created to replace the old one.    Note  Prediction is computationally much less costly than training, so the bottleneck sometimes becomes the IO for copying mini-batches of data. Since there is no concern about convergence in prediction, it is better to set the mini-batch size as large as possible (limited by your device memory) if prediction speed is a concern.  For the same reason, currently prediction will only use the first device even if multiple devices are provided to construct the model.    Note  If you perform further after prediction. The weights are not automatically synchronized if  overwrite  is set to false and the old predictor is re-used. In this case setting  overwrite  to true (the default) will re-initialize the predictor the next time you call predict and synchronize the weights again.   See also  train ,  fit ,  init_model , and  load_checkpoint  source  #  MXNet.mx.train     Method .  train(model :: FeedForward, ...)  Alias to  fit .  source", 
            "title": "Model"
        }, 
        {
            "location": "/api/initializer/", 
            "text": "Initializer\n\n\n#\n\n\nMXNet.mx.AbstractInitializer\n \n \nType\n.\n\n\nAbstractInitializer\n\n\n\n\nThe abstract base class for all initializers.\n\n\nTo define a new initializer, it is enough to derive a new type, and implement one or more of the following methods:\n\n\n_init_weight(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_bias(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_gamma(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_beta(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n\n\n\n\nOr, if full behavior customization is needed, override the following function\n\n\ninit(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.NormalInitializer\n \n \nType\n.\n\n\nNormalInitializer\n\n\n\n\nInitialize weights according to a univariate Gaussian distribution.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.NormalInitializer\n \n \nMethod\n.\n\n\nNormalIninitializer(; mu=0, sigma=0.01)\n\n\n\n\nConstruct a \nNormalInitializer\n with mean \nmu\n and variance \nsigma\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.UniformInitializer\n \n \nType\n.\n\n\nUniformInitializer\n\n\n\n\nInitialize weights according to a uniform distribution within the provided scale.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.UniformInitializer\n \n \nMethod\n.\n\n\nUniformInitializer(scale=0.07)\n\n\n\n\nConstruct a \nUniformInitializer\n with the specified scale.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.XavierDistribution\n \n \nType\n.\n\n\nXavierInitializer\n\n\n\n\nThe initializer documented in the paper [Bengio and Glorot 2010]: \nUnderstanding the difficulty of training deep feedforward neuralnetworks\n.\n\n\nThere are several different version of the XavierInitializer used in the wild. The general idea is that the variance of the initialization distribution is controlled by the dimensionality of the input and output. As a distribution one can either choose a normal distribution with \u03bc = 0 and \u03c3\u00b2 or a uniform distribution from -\u03c3 to \u03c3.\n\n\nSeveral different ways of calculating the variance are given in the literature or are used by various libraries.\n\n\n\n\n[Bengio and Glorot 2010]: \nmx.XavierInitializer(distribution = mx.xv_uniform, regularization = mx.xv_avg, magnitude = 1)\n\n\n[K. He, X. Zhang, S. Ren, and J. Sun 2015]: \nmx.XavierInitializer(distribution = mx.xv_gaussian, regularization = mx.xv_in, magnitude = 2)\n\n\ncaffe_avg: \nmx.XavierInitializer(distribution = mx.xv_uniform, regularization = mx.xv_avg, magnitude = 3)\n\n\n\n\nsource", 
            "title": "Initializers"
        }, 
        {
            "location": "/api/initializer/#initializer", 
            "text": "#  MXNet.mx.AbstractInitializer     Type .  AbstractInitializer  The abstract base class for all initializers.  To define a new initializer, it is enough to derive a new type, and implement one or more of the following methods:  _init_weight(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_bias(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_gamma(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_beta(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)  Or, if full behavior customization is needed, override the following function  init(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)  source  #  MXNet.mx.NormalInitializer     Type .  NormalInitializer  Initialize weights according to a univariate Gaussian distribution.  source  #  MXNet.mx.NormalInitializer     Method .  NormalIninitializer(; mu=0, sigma=0.01)  Construct a  NormalInitializer  with mean  mu  and variance  sigma .  source  #  MXNet.mx.UniformInitializer     Type .  UniformInitializer  Initialize weights according to a uniform distribution within the provided scale.  source  #  MXNet.mx.UniformInitializer     Method .  UniformInitializer(scale=0.07)  Construct a  UniformInitializer  with the specified scale.  source  #  MXNet.mx.XavierDistribution     Type .  XavierInitializer  The initializer documented in the paper [Bengio and Glorot 2010]:  Understanding the difficulty of training deep feedforward neuralnetworks .  There are several different version of the XavierInitializer used in the wild. The general idea is that the variance of the initialization distribution is controlled by the dimensionality of the input and output. As a distribution one can either choose a normal distribution with \u03bc = 0 and \u03c3\u00b2 or a uniform distribution from -\u03c3 to \u03c3.  Several different ways of calculating the variance are given in the literature or are used by various libraries.   [Bengio and Glorot 2010]:  mx.XavierInitializer(distribution = mx.xv_uniform, regularization = mx.xv_avg, magnitude = 1)  [K. He, X. Zhang, S. Ren, and J. Sun 2015]:  mx.XavierInitializer(distribution = mx.xv_gaussian, regularization = mx.xv_in, magnitude = 2)  caffe_avg:  mx.XavierInitializer(distribution = mx.xv_uniform, regularization = mx.xv_avg, magnitude = 3)   source", 
            "title": "Initializer"
        }, 
        {
            "location": "/api/optimizer/", 
            "text": "Optimizers\n\n\n#\n\n\nMXNet.mx.AbstractLearningRateScheduler\n \n \nType\n.\n\n\nAbstractLearningRateScheduler\n\n\n\n\nBase type for all learning rate scheduler.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractMomentumScheduler\n \n \nType\n.\n\n\nAbstractMomentumScheduler\n\n\n\n\nBase type for all momentum scheduler.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractOptimizer\n \n \nType\n.\n\n\nAbstractOptimizer\n\n\n\n\nBase type for all optimizers.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractOptimizerOptions\n \n \nType\n.\n\n\nAbstractOptimizerOptions\n\n\n\n\nBase class for all optimizer options.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.OptimizationState\n \n \nType\n.\n\n\nOptimizationState\n\n\n\n\nAttributes:\n\n\n\n\nbatch_size\n: The size of the mini-batch used in stochastic training.\n\n\ncurr_epoch\n: The current epoch count. Epoch 0 means no training yet, during the first pass through the data, the epoch will be 1; during the second pass, the epoch count will be 1, and so on.\n\n\ncurr_batch\n: The current mini-batch count. The batch count is reset during every epoch. The batch count 0 means the beginning of each epoch, with no mini-batch seen yet. During the first mini-batch, the mini-batch count will be 1.\n\n\ncurr_iter\n: The current iteration count. One iteration corresponds to one mini-batch, but unlike the mini-batch count, the iteration count does \nnot\n reset in each epoch. So it track the \ntotal\n number of mini-batches seen so far.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_learning_rate\n \n \nFunction\n.\n\n\nget_learning_rate(scheduler, state)\n\n\n\n\nArguments\n\n\n\n\nscheduler::AbstractLearningRateScheduler\n: a learning rate scheduler.\n\n\nstate::OptimizationState\n: the current state about epoch, mini-batch and iteration count.\n\n\n\n\nReturns the current learning rate.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_momentum\n \n \nFunction\n.\n\n\nget_momentum(scheduler, state)\n\n\n\n\n\n\nscheduler::AbstractMomentumScheduler\n: the momentum scheduler.\n\n\nstate::OptimizationState\n: the state about current epoch, mini-batch and iteration count.\n\n\n\n\nReturns the current momentum.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_updater\n \n \nMethod\n.\n\n\nget_updater(optimizer)\n\n\n\n\nA utility function to create an updater function, that uses its closure to store all the states needed for each weights.\n\n\n\n\noptimizer::AbstractOptimizer\n: the underlying optimizer.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.normalized_gradient\n \n \nMethod\n.\n\n\nnormalized_gradient(opts, state, weight, grad)\n\n\n\n\n\n\nopts::AbstractOptimizerOptions\n: options for the optimizer, should contain the field\n\n\n\n\ngrad_clip\n and \nweight_decay\n.\n\n\n\n\nstate::OptimizationState\n: the current optimization state.\n\n\nweight::NDArray\n: the trainable weights.\n\n\n\n\ngrad::NDArray\n: the original gradient of the weights.\n\n\nGet the properly normalized gradient (re-scaled and clipped if necessary).\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LearningRate.Exp\n \n \nType\n.\n\n\nLearningRate.Exp\n\n\n\n\n$\u001bta_t = \u001bta_0gamma^t$. Here $t$ is the epoch count, or the iteration count if \ndecay_on_iteration\n is set to true.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LearningRate.Fixed\n \n \nType\n.\n\n\nLearningRate.Fixed\n\n\n\n\nFixed learning rate scheduler always return the same learning rate.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LearningRate.Inv\n \n \nType\n.\n\n\nLearningRate.Inv\n\n\n\n\n$\u001bta_t = \u001bta_0 * (1 + gamma * t)^(-power)$. Here $t$ is the epoch count, or the iteration count if \ndecay_on_iteration\n is set to true.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Momentum.Fixed\n \n \nType\n.\n\n\nMomentum.Fixed\n\n\n\n\nFixed momentum scheduler always returns the same value.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Momentum.NadamScheduler\n \n \nType\n.\n\n\nMomentum.NadamScheduler\n\n\n\n\nNesterov-accelerated adaptive momentum scheduler.\n\n\nDescription in \"Incorporating Nesterov Momentum into Adam.\" \nhttp://cs229.stanford.edu/proj2015/054_report.pdf\n\n\n$mu_t = mu_0 * (1 - gamma * \u0007lpha^{t * delta})$. Here\n\n\n\n\n$t$ is the iteration count\n\n\n$delta$: default \n0.004\n is scheduler decay,\n\n\n$gamma$: default \n0.5\n\n\n$\u0007lpha$: default \n0.96\n\n\n$mu_0$: default \n0.99\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Momentum.Null\n \n \nType\n.\n\n\nMomentum.Null\n\n\n\n\nThe null momentum scheduler always returns 0 for momentum. It is also used to explicitly indicate momentum should not be used.\n\n\nsource\n\n\n\n\nBuilt-in optimizers\n\n\n\n\nStochastic Gradient Descent\n\n\n#\n\n\nMXNet.mx.SGD\n \n \nType\n.\n\n\nSGD\n\n\n\n\nStochastic gradient descent optimizer.\n\n\nSGD(; kwargs...)\n\n\n\n\nArguments:\n\n\n\n\nlr::Real\n: default \n0.01\n, learning rate.\n\n\nlr_scheduler::AbstractLearningRateScheduler\n: default \nnothing\n, a      dynamic learning rate scheduler. If set, will overwrite the \nlr\n      parameter.\n\n\nmomentum::Real\n: default \n0.0\n, the momentum.\n\n\nmomentum_scheduler::AbstractMomentumScheduler\n: default \nnothing\n,      a dynamic momentum scheduler. If set, will overwrite the \nmomentum\n      parameter.\n\n\ngrad_clip::Real\n: default \n0\n, if positive, will clip the gradient      into the bounded range \n[-grad_clip, grad_clip]\n.\n\n\nweight_decay::Real\n: default \n0.0001\n, weight decay is equivalent to      adding a global l2 regularizer to the parameters.\n\n\n\n\nsource\n\n\n\n\nADAM\n\n\n#\n\n\nMXNet.mx.ADAM\n \n \nType\n.\n\n\n ADAM\n\n\n\n\nThe solver described in Diederik Kingma, Jimmy Ba: \nAdam: A Method for Stochastic Optimization\n. arXiv:1412.6980 [cs.LG].\n\n\nADAM(; kwargs...)\n\n\n\n\n\n\nlr::Real\n: default \n0.001\n, learning rate.\n\n\nlr_scheduler::AbstractLearningRateScheduler\n: default \nnothing\n, a      dynamic learning rate scheduler. If set, will overwrite the \nlr\n      parameter.\n\n\nbeta1::Real\n: default \n0.9\n.\n\n\nbeta2::Real\n: default \n0.999\n.\n\n\nepsilon::Real\n: default \n1e-8\n.\n\n\ngrad_clip::Real\n: default \n0\n, if positive, will clip the gradient      into the range \n[-grad_clip, grad_clip]\n.\n\n\nweight_decay::Real\n: default \n0.00001\n, weight decay is equivalent      to adding a global l2 regularizer for all the parameters.\n\n\n\n\nsource\n\n\n\n\nAdaGrad\n\n\n#\n\n\nMXNet.mx.AdaGrad\n \n \nType\n.\n\n\nAdaGrad\n\n\n\n\nScale learning rates by dividing with the square root of accumulated squared gradients. See [1] for further description.\n\n\nAdaGrad(; kwargs...)\n\n\n\n\nAttributes\n\n\n\n\nlr::Real\n: default \n0.1\n, the learning rate controlling the size of update steps\n\n\nepsilon::Real\n: default \n1e-6\n, small value added for numerical stability\n\n\ngrad_clip::Real\n: default \n0\n, if positive, will clip the gradient into the range \n[-grad_clip, grad_clip]\n.\n\n\nweight_decay::Real\n: default \n0.00001\n, weight decay is equivalent to adding a global l2 regularizer for all the parameters.\n\n\n\n\nNotes\n\n\nUsing step size lr AdaGrad calculates the learning rate for feature i at time step t as: $\u03b7_{t,i} = \frac{lr}{sqrt{sum^t_{t^prime} g^2_{t^prime,i} + \u03f5}} g_{t,i}$ as such the learning rate is monotonically decreasing. Epsilon is not included in the typical formula, see [2].\n\n\nReferences\n\n\n\n\n[1]: Duchi, J., Hazan, E., \n Singer, Y. (2011): Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12:2121-2159.\n\n\n[2]: Chris Dyer: Notes on AdaGrad. \nhttp://www.ark.cs.cmu.edu/cdyer/adagrad.pdf\n\n\n\n\nsource\n\n\n\n\nAdaDelta\n\n\n#\n\n\nMXNet.mx.AdaDelta\n \n \nType\n.\n\n\nAdaDelta\n\n\n\n\nScale learning rates by the ratio of accumulated gradients to accumulated updates, see [1] and notes for further description.\n\n\nAdaDelta(; kwargs...)\n\n\n\n\nAttributes\n\n\n\n\nlr::Real\n: default \n1.0\n, the learning rate controlling the size of update steps\n\n\nrho::Real\n: default \n0.9\n, squared gradient moving average decay factor\n\n\nepsilon::Real\n: default \n1e-6\n, small value added for numerical stability\n\n\ngrad_clip::Real\n: default \n0\n, if positive, will clip the gradient into the range \n[-grad_clip, grad_clip]\n.\n\n\nweight_decay::Real\n: default \n0.00001\n, weight decay is equivalent to adding a global l2 regularizer for all the parameters.\n\n\n\n\nNotes\n\n\nrho\n should be between 0 and 1. A value of \nrho\n close to 1 will decay the moving average slowly and a value close to 0 will decay the moving average fast.\n\n\nrho\n = 0.95 and \nepsilon\n = 1e-6 are suggested in the paper and reported to work for multiple datasets (MNIST, speech). In the paper, no learning rate is considered (so \nlr\n = 1.0). Probably best to keep it at this value.\n\n\nepsilon\n is important for the very first update (so the numerator does not become 0).\n\n\nUsing the step size \nlr\n and a decay factor \nrho\n the learning rate is calculated as: \nr_t \n=  ho r_{t-1} + (1- ho)*g^2\n\u001bta_t \n= \u001bta \frac{sqrt{s_{t-1} + \u001bpsilon}} {sqrt{r_t + \u001bpsilon}}\ns_t \n=  ho s_{t-1} + (1- ho)*(\u001bta_t*g)^2\n\n\nReferences\n\n\n\n\n[1]: Zeiler, M. D. (2012): ADADELTA: An Adaptive Learning Rate Method. arXiv Preprint arXiv:1212.5701.\n\n\n\n\nsource\n\n\n\n\nAdaMax\n\n\n#\n\n\nMXNet.mx.AdaMax\n \n \nType\n.\n\n\nAdaMax\n\n\n\n\nThis is a variant of of the Adam algorithm based on the infinity norm. See [1] for further description.\n\n\nAdaMax(; kwargs...)\n\n\n\n\nAttributes\n\n\n\n\nlr::Real\n: default \n0.002\n, the learning rate controlling the size of update steps\n\n\nbeta1::Real\n: default \n0.9\n, exponential decay rate for the first moment estimates\n\n\nbeta2::Real\n: default \n0.999\n, exponential decay rate for the weighted infinity norm estimates\n\n\nepsilon::Real\n: default \n1e-8\n, small value added for numerical stability\n\n\ngrad_clip::Real\n: default \n0\n, if positive, will clip the gradient into the range \n[-grad_clip, grad_clip]\n.\n\n\nweight_decay::Real\n: default \n0.00001\n, weight decay is equivalent to adding a global l2 regularizer for all the parameters.\n\n\n\n\nReferences\n\n\n\n\n[1]: Kingma, Diederik, and Jimmy Ba (2014): Adam: A Method for Stochastic Optimization. \nhttp://arxiv.org/abs/1412.6980v8\n.\n\n\n\n\nsource\n\n\n\n\nRMSProp\n\n\n#\n\n\nMXNet.mx.RMSProp\n \n \nType\n.\n\n\nRMSProp\n\n\n\n\nScale learning rates by dividing with the moving average of the root mean squared (RMS) gradients. See [1] for further description.\n\n\nRMSProp(; kwargs...)\n\n\n\n\nAttributes\n\n\n\n\nlr::Real\n: default \n0.1\n, the learning rate controlling the size of update steps\n\n\nrho::Real\n: default \n0.9\n, gradient moving average decay factor\n\n\nepsilon::Real\n: default \n1e-6\n, small value added for numerical stability\n\n\ngrad_clip::Real\n: default \n0\n, if positive, will clip the gradient into the range \n[-grad_clip, grad_clip]\n.\n\n\nweight_decay::Real\n: default \n0.00001\n, weight decay is equivalent to adding a global l2 regularizer for all the parameters.\n\n\n\n\nNotes\n\n\nrho\n should be between 0 and 1. A value of \nrho\n close to 1 will decay the moving average slowly and a value close to 0 will decay the moving average fast.\n\n\nUsing the step size $lr$ and a decay factor $ho$ the learning rate $\u001bta_t$ is calculated as: \nr_t \n= \u03c1 r_{t-1} + (1 - \u03c1)*g^2 \n  \u03b7_t \n= \frac{lr}{sqrt{r_t + \u03f5}}\n\n\nReferences\n\n\n\n\n[1]: Tieleman, T. and Hinton, G. (2012): Neural Networks for Machine Learning, Lecture 6.5 - rmsprop. Coursera. \nhttp://www.youtube.com/watch?v=O3sxAc4hxZU\n (formula @5:20)\n\n\n\n\nsource\n\n\n\n\nNadam\n\n\n#\n\n\nMXNet.mx.Nadam\n \n \nType\n.\n\n\nNadam\n\n\n\n\nNesterov Adam optimizer: Adam RMSprop with Nesterov momentum, see [1] and notes for further description.\n\n\nNadam(; kwargs...)\n\n\n\n\nAttributes\n\n\n\n\nlr::Real\n: default \n0.001\n, learning rate.\n\n\nbeta1::Real\n: default \n0.99\n.\n\n\nbeta2::Real\n: default \n0.999\n.\n\n\nepsilon::Real\n: default \n1e-8\n, small value added for numerical stability\n\n\ngrad_clip::Real\n: default \n0\n, if positive, will clip the gradient into the range \n[-grad_clip, grad_clip]\n.\n\n\nweight_decay::Real\n: default \n0.00001\n, weight decay is equivalent to adding a global l2 regularizer for all the parameters.\n\n\nlr_scheduler::AbstractLearningRateScheduler\n: default \nnothing\n, a dynamic learning rate scheduler. If set, will overwrite the \nlr\n parameter.\n\n\nmomentum_scheduler::AbstractMomentumScheduler\n default \nNadamScheduler\n of the form $mu_t = beta1 * (1 - 0.5 * 0.96^{t * 0.004})$\n\n\n\n\nNotes\n\n\nDefault parameters follow those provided in the paper. It is recommended to leave the parameters of this optimizer at their default values.\n\n\nReferences\n\n\n\n\n[1]: Incorporating Nesterov Momentum into Adam. \nhttp://cs229.stanford.edu/proj2015/054_report.pdf\n\n\n[2]: On the importance of initialization and momentum in deep learning \nhttp://www.cs.toronto.edu/~fritz/absps/momentum.pdf\n\n\n\n\nsource", 
            "title": "Optimizers"
        }, 
        {
            "location": "/api/optimizer/#optimizers", 
            "text": "#  MXNet.mx.AbstractLearningRateScheduler     Type .  AbstractLearningRateScheduler  Base type for all learning rate scheduler.  source  #  MXNet.mx.AbstractMomentumScheduler     Type .  AbstractMomentumScheduler  Base type for all momentum scheduler.  source  #  MXNet.mx.AbstractOptimizer     Type .  AbstractOptimizer  Base type for all optimizers.  source  #  MXNet.mx.AbstractOptimizerOptions     Type .  AbstractOptimizerOptions  Base class for all optimizer options.  source  #  MXNet.mx.OptimizationState     Type .  OptimizationState  Attributes:   batch_size : The size of the mini-batch used in stochastic training.  curr_epoch : The current epoch count. Epoch 0 means no training yet, during the first pass through the data, the epoch will be 1; during the second pass, the epoch count will be 1, and so on.  curr_batch : The current mini-batch count. The batch count is reset during every epoch. The batch count 0 means the beginning of each epoch, with no mini-batch seen yet. During the first mini-batch, the mini-batch count will be 1.  curr_iter : The current iteration count. One iteration corresponds to one mini-batch, but unlike the mini-batch count, the iteration count does  not  reset in each epoch. So it track the  total  number of mini-batches seen so far.   source  #  MXNet.mx.get_learning_rate     Function .  get_learning_rate(scheduler, state)  Arguments   scheduler::AbstractLearningRateScheduler : a learning rate scheduler.  state::OptimizationState : the current state about epoch, mini-batch and iteration count.   Returns the current learning rate.  source  #  MXNet.mx.get_momentum     Function .  get_momentum(scheduler, state)   scheduler::AbstractMomentumScheduler : the momentum scheduler.  state::OptimizationState : the state about current epoch, mini-batch and iteration count.   Returns the current momentum.  source  #  MXNet.mx.get_updater     Method .  get_updater(optimizer)  A utility function to create an updater function, that uses its closure to store all the states needed for each weights.   optimizer::AbstractOptimizer : the underlying optimizer.   source  #  MXNet.mx.normalized_gradient     Method .  normalized_gradient(opts, state, weight, grad)   opts::AbstractOptimizerOptions : options for the optimizer, should contain the field   grad_clip  and  weight_decay .   state::OptimizationState : the current optimization state.  weight::NDArray : the trainable weights.   grad::NDArray : the original gradient of the weights.  Get the properly normalized gradient (re-scaled and clipped if necessary).    source  #  MXNet.mx.LearningRate.Exp     Type .  LearningRate.Exp  $\u001bta_t = \u001bta_0gamma^t$. Here $t$ is the epoch count, or the iteration count if  decay_on_iteration  is set to true.  source  #  MXNet.mx.LearningRate.Fixed     Type .  LearningRate.Fixed  Fixed learning rate scheduler always return the same learning rate.  source  #  MXNet.mx.LearningRate.Inv     Type .  LearningRate.Inv  $\u001bta_t = \u001bta_0 * (1 + gamma * t)^(-power)$. Here $t$ is the epoch count, or the iteration count if  decay_on_iteration  is set to true.  source  #  MXNet.mx.Momentum.Fixed     Type .  Momentum.Fixed  Fixed momentum scheduler always returns the same value.  source  #  MXNet.mx.Momentum.NadamScheduler     Type .  Momentum.NadamScheduler  Nesterov-accelerated adaptive momentum scheduler.  Description in \"Incorporating Nesterov Momentum into Adam.\"  http://cs229.stanford.edu/proj2015/054_report.pdf  $mu_t = mu_0 * (1 - gamma * \u0007lpha^{t * delta})$. Here   $t$ is the iteration count  $delta$: default  0.004  is scheduler decay,  $gamma$: default  0.5  $\u0007lpha$: default  0.96  $mu_0$: default  0.99   source  #  MXNet.mx.Momentum.Null     Type .  Momentum.Null  The null momentum scheduler always returns 0 for momentum. It is also used to explicitly indicate momentum should not be used.  source", 
            "title": "Optimizers"
        }, 
        {
            "location": "/api/optimizer/#built-in-optimizers", 
            "text": "", 
            "title": "Built-in optimizers"
        }, 
        {
            "location": "/api/optimizer/#stochastic-gradient-descent", 
            "text": "#  MXNet.mx.SGD     Type .  SGD  Stochastic gradient descent optimizer.  SGD(; kwargs...)  Arguments:   lr::Real : default  0.01 , learning rate.  lr_scheduler::AbstractLearningRateScheduler : default  nothing , a      dynamic learning rate scheduler. If set, will overwrite the  lr       parameter.  momentum::Real : default  0.0 , the momentum.  momentum_scheduler::AbstractMomentumScheduler : default  nothing ,      a dynamic momentum scheduler. If set, will overwrite the  momentum       parameter.  grad_clip::Real : default  0 , if positive, will clip the gradient      into the bounded range  [-grad_clip, grad_clip] .  weight_decay::Real : default  0.0001 , weight decay is equivalent to      adding a global l2 regularizer to the parameters.   source", 
            "title": "Stochastic Gradient Descent"
        }, 
        {
            "location": "/api/optimizer/#adam", 
            "text": "#  MXNet.mx.ADAM     Type .   ADAM  The solver described in Diederik Kingma, Jimmy Ba:  Adam: A Method for Stochastic Optimization . arXiv:1412.6980 [cs.LG].  ADAM(; kwargs...)   lr::Real : default  0.001 , learning rate.  lr_scheduler::AbstractLearningRateScheduler : default  nothing , a      dynamic learning rate scheduler. If set, will overwrite the  lr       parameter.  beta1::Real : default  0.9 .  beta2::Real : default  0.999 .  epsilon::Real : default  1e-8 .  grad_clip::Real : default  0 , if positive, will clip the gradient      into the range  [-grad_clip, grad_clip] .  weight_decay::Real : default  0.00001 , weight decay is equivalent      to adding a global l2 regularizer for all the parameters.   source", 
            "title": "ADAM"
        }, 
        {
            "location": "/api/optimizer/#adagrad", 
            "text": "#  MXNet.mx.AdaGrad     Type .  AdaGrad  Scale learning rates by dividing with the square root of accumulated squared gradients. See [1] for further description.  AdaGrad(; kwargs...)  Attributes   lr::Real : default  0.1 , the learning rate controlling the size of update steps  epsilon::Real : default  1e-6 , small value added for numerical stability  grad_clip::Real : default  0 , if positive, will clip the gradient into the range  [-grad_clip, grad_clip] .  weight_decay::Real : default  0.00001 , weight decay is equivalent to adding a global l2 regularizer for all the parameters.   Notes  Using step size lr AdaGrad calculates the learning rate for feature i at time step t as: $\u03b7_{t,i} = \frac{lr}{sqrt{sum^t_{t^prime} g^2_{t^prime,i} + \u03f5}} g_{t,i}$ as such the learning rate is monotonically decreasing. Epsilon is not included in the typical formula, see [2].  References   [1]: Duchi, J., Hazan, E.,   Singer, Y. (2011): Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12:2121-2159.  [2]: Chris Dyer: Notes on AdaGrad.  http://www.ark.cs.cmu.edu/cdyer/adagrad.pdf   source", 
            "title": "AdaGrad"
        }, 
        {
            "location": "/api/optimizer/#adadelta", 
            "text": "#  MXNet.mx.AdaDelta     Type .  AdaDelta  Scale learning rates by the ratio of accumulated gradients to accumulated updates, see [1] and notes for further description.  AdaDelta(; kwargs...)  Attributes   lr::Real : default  1.0 , the learning rate controlling the size of update steps  rho::Real : default  0.9 , squared gradient moving average decay factor  epsilon::Real : default  1e-6 , small value added for numerical stability  grad_clip::Real : default  0 , if positive, will clip the gradient into the range  [-grad_clip, grad_clip] .  weight_decay::Real : default  0.00001 , weight decay is equivalent to adding a global l2 regularizer for all the parameters.   Notes  rho  should be between 0 and 1. A value of  rho  close to 1 will decay the moving average slowly and a value close to 0 will decay the moving average fast.  rho  = 0.95 and  epsilon  = 1e-6 are suggested in the paper and reported to work for multiple datasets (MNIST, speech). In the paper, no learning rate is considered (so  lr  = 1.0). Probably best to keep it at this value.  epsilon  is important for the very first update (so the numerator does not become 0).  Using the step size  lr  and a decay factor  rho  the learning rate is calculated as:  r_t  =  ho r_{t-1} + (1- ho)*g^2\n\u001bta_t  = \u001bta \frac{sqrt{s_{t-1} + \u001bpsilon}} {sqrt{r_t + \u001bpsilon}}\ns_t  =  ho s_{t-1} + (1- ho)*(\u001bta_t*g)^2  References   [1]: Zeiler, M. D. (2012): ADADELTA: An Adaptive Learning Rate Method. arXiv Preprint arXiv:1212.5701.   source", 
            "title": "AdaDelta"
        }, 
        {
            "location": "/api/optimizer/#adamax", 
            "text": "#  MXNet.mx.AdaMax     Type .  AdaMax  This is a variant of of the Adam algorithm based on the infinity norm. See [1] for further description.  AdaMax(; kwargs...)  Attributes   lr::Real : default  0.002 , the learning rate controlling the size of update steps  beta1::Real : default  0.9 , exponential decay rate for the first moment estimates  beta2::Real : default  0.999 , exponential decay rate for the weighted infinity norm estimates  epsilon::Real : default  1e-8 , small value added for numerical stability  grad_clip::Real : default  0 , if positive, will clip the gradient into the range  [-grad_clip, grad_clip] .  weight_decay::Real : default  0.00001 , weight decay is equivalent to adding a global l2 regularizer for all the parameters.   References   [1]: Kingma, Diederik, and Jimmy Ba (2014): Adam: A Method for Stochastic Optimization.  http://arxiv.org/abs/1412.6980v8 .   source", 
            "title": "AdaMax"
        }, 
        {
            "location": "/api/optimizer/#rmsprop", 
            "text": "#  MXNet.mx.RMSProp     Type .  RMSProp  Scale learning rates by dividing with the moving average of the root mean squared (RMS) gradients. See [1] for further description.  RMSProp(; kwargs...)  Attributes   lr::Real : default  0.1 , the learning rate controlling the size of update steps  rho::Real : default  0.9 , gradient moving average decay factor  epsilon::Real : default  1e-6 , small value added for numerical stability  grad_clip::Real : default  0 , if positive, will clip the gradient into the range  [-grad_clip, grad_clip] .  weight_decay::Real : default  0.00001 , weight decay is equivalent to adding a global l2 regularizer for all the parameters.   Notes  rho  should be between 0 and 1. A value of  rho  close to 1 will decay the moving average slowly and a value close to 0 will decay the moving average fast.  Using the step size $lr$ and a decay factor $ho$ the learning rate $\u001bta_t$ is calculated as:  r_t  = \u03c1 r_{t-1} + (1 - \u03c1)*g^2 \n  \u03b7_t  = \frac{lr}{sqrt{r_t + \u03f5}}  References   [1]: Tieleman, T. and Hinton, G. (2012): Neural Networks for Machine Learning, Lecture 6.5 - rmsprop. Coursera.  http://www.youtube.com/watch?v=O3sxAc4hxZU  (formula @5:20)   source", 
            "title": "RMSProp"
        }, 
        {
            "location": "/api/optimizer/#nadam", 
            "text": "#  MXNet.mx.Nadam     Type .  Nadam  Nesterov Adam optimizer: Adam RMSprop with Nesterov momentum, see [1] and notes for further description.  Nadam(; kwargs...)  Attributes   lr::Real : default  0.001 , learning rate.  beta1::Real : default  0.99 .  beta2::Real : default  0.999 .  epsilon::Real : default  1e-8 , small value added for numerical stability  grad_clip::Real : default  0 , if positive, will clip the gradient into the range  [-grad_clip, grad_clip] .  weight_decay::Real : default  0.00001 , weight decay is equivalent to adding a global l2 regularizer for all the parameters.  lr_scheduler::AbstractLearningRateScheduler : default  nothing , a dynamic learning rate scheduler. If set, will overwrite the  lr  parameter.  momentum_scheduler::AbstractMomentumScheduler  default  NadamScheduler  of the form $mu_t = beta1 * (1 - 0.5 * 0.96^{t * 0.004})$   Notes  Default parameters follow those provided in the paper. It is recommended to leave the parameters of this optimizer at their default values.  References   [1]: Incorporating Nesterov Momentum into Adam.  http://cs229.stanford.edu/proj2015/054_report.pdf  [2]: On the importance of initialization and momentum in deep learning  http://www.cs.toronto.edu/~fritz/absps/momentum.pdf   source", 
            "title": "Nadam"
        }, 
        {
            "location": "/api/callback/", 
            "text": "Callback in training\n\n\n#\n\n\nMXNet.mx.AbstractBatchCallback\n \n \nType\n.\n\n\nAbstractBatchCallback\n\n\n\n\nAbstract type of callbacks to be called every mini-batch.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractCallback\n \n \nType\n.\n\n\nAbstractCallback\n\n\n\n\nAbstract type of callback functions used in training.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractEpochCallback\n \n \nType\n.\n\n\nAbstractEpochCallback\n\n\n\n\nAbstract type of callbacks to be called every epoch.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.do_checkpoint\n \n \nMethod\n.\n\n\ndo_checkpoint(prefix; frequency=1, save_epoch_0=false)\n\n\n\n\nCreate an \nAbstractEpochCallback\n that save checkpoints of the model to disk. The checkpoints can be loaded back later on.\n\n\nArguments\n\n\n\n\nprefix::AbstractString\n: the prefix of the filenames to save the model. The model         architecture will be saved to prefix-symbol.json, while the weights will be saved         to prefix-0012.params, for example, for the 12-th epoch.\n\n\nfrequency::Int\n: keyword argument, default 1. The frequency (measured in epochs) to         save checkpoints.\n\n\nsave_epoch_0::Bool\n: keyword argument, default false. Whether we should save a         checkpoint for epoch 0 (model initialized but not seen any data yet).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.every_n_batch\n \n \nMethod\n.\n\n\nevery_n_batch(callback :: Function, n :: Int; call_on_0 = false)\n\n\n\n\nA convenient function to construct a callback that runs every \nn\n mini-batches.\n\n\nArguments\n\n\n\n\ncall_on_0::Bool\n: keyword argument, default false. Unless set, the callback         will \nnot\n be run on batch 0.\n\n\n\n\nFor example, the \nspeedometer\n callback is defined as\n\n\nevery_n_iter(frequency, call_on_0=true) do state :: OptimizationState\n  if state.curr_batch == 0\n    # reset timer\n  else\n    # compute and print speed\n  end\nend\n\n\n\n\nSee also \nevery_n_epoch\n and \nspeedometer\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.every_n_epoch\n \n \nMethod\n.\n\n\nevery_n_epoch(callback :: Function, n :: Int; call_on_0 = false)\n\n\n\n\nA convenient function to construct a callback that runs every \nn\n full data-passes.\n\n\n\n\ncall_on_0::Int\n: keyword argument, default false. Unless set, the callback         will \nnot\n be run on epoch 0. Epoch 0 means no training has been performed         yet. This is useful if you want to inspect the randomly initialized model         that has not seen any data yet.\n\n\n\n\nSee also \nevery_n_iter\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.speedometer\n \n \nMethod\n.\n\n\nspeedometer(; frequency=50)\n\n\n\n\nCreate an \nAbstractBatchCallback\n that measure the training speed    (number of samples processed per second) every k mini-batches.\n\n\nArguments\n\n\n\n\nfrequency::Int\n: keyword argument, default 50. The frequency (number of         min-batches) to measure and report the speed.\n\n\n\n\nsource", 
            "title": "Callbacks in training"
        }, 
        {
            "location": "/api/callback/#callback-in-training", 
            "text": "#  MXNet.mx.AbstractBatchCallback     Type .  AbstractBatchCallback  Abstract type of callbacks to be called every mini-batch.  source  #  MXNet.mx.AbstractCallback     Type .  AbstractCallback  Abstract type of callback functions used in training.  source  #  MXNet.mx.AbstractEpochCallback     Type .  AbstractEpochCallback  Abstract type of callbacks to be called every epoch.  source  #  MXNet.mx.do_checkpoint     Method .  do_checkpoint(prefix; frequency=1, save_epoch_0=false)  Create an  AbstractEpochCallback  that save checkpoints of the model to disk. The checkpoints can be loaded back later on.  Arguments   prefix::AbstractString : the prefix of the filenames to save the model. The model         architecture will be saved to prefix-symbol.json, while the weights will be saved         to prefix-0012.params, for example, for the 12-th epoch.  frequency::Int : keyword argument, default 1. The frequency (measured in epochs) to         save checkpoints.  save_epoch_0::Bool : keyword argument, default false. Whether we should save a         checkpoint for epoch 0 (model initialized but not seen any data yet).   source  #  MXNet.mx.every_n_batch     Method .  every_n_batch(callback :: Function, n :: Int; call_on_0 = false)  A convenient function to construct a callback that runs every  n  mini-batches.  Arguments   call_on_0::Bool : keyword argument, default false. Unless set, the callback         will  not  be run on batch 0.   For example, the  speedometer  callback is defined as  every_n_iter(frequency, call_on_0=true) do state :: OptimizationState\n  if state.curr_batch == 0\n    # reset timer\n  else\n    # compute and print speed\n  end\nend  See also  every_n_epoch  and  speedometer .  source  #  MXNet.mx.every_n_epoch     Method .  every_n_epoch(callback :: Function, n :: Int; call_on_0 = false)  A convenient function to construct a callback that runs every  n  full data-passes.   call_on_0::Int : keyword argument, default false. Unless set, the callback         will  not  be run on epoch 0. Epoch 0 means no training has been performed         yet. This is useful if you want to inspect the randomly initialized model         that has not seen any data yet.   See also  every_n_iter .  source  #  MXNet.mx.speedometer     Method .  speedometer(; frequency=50)  Create an  AbstractBatchCallback  that measure the training speed    (number of samples processed per second) every k mini-batches.  Arguments   frequency::Int : keyword argument, default 50. The frequency (number of         min-batches) to measure and report the speed.   source", 
            "title": "Callback in training"
        }, 
        {
            "location": "/api/metric/", 
            "text": "Evaluation Metrics\n\n\nEvaluation metrics provide a way to evaluate the performance of a learned model. This is typically used during training to monitor performance on the validation set.\n\n\n#\n\n\nMXNet.mx.ACE\n \n \nType\n.\n\n\nACE\n\n\n\n\nCalculates the averaged cross-entropy (logloss) for classification.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractEvalMetric\n \n \nType\n.\n\n\nAbstractEvalMetric\n\n\n\n\nThe base class for all evaluation metrics. The sub-types should implement the following interfaces:\n\n\n\n\nupdate!\n\n\nreset!\n\n\nget\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Accuracy\n \n \nType\n.\n\n\nAccuracy\n\n\n\n\nMulticlass classification accuracy.\n\n\nCalculates the mean accuracy per sample for softmax in one dimension. For a multi-dimensional softmax the mean accuracy over all dimensions is calculated.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MSE\n \n \nType\n.\n\n\nMSE\n\n\n\n\nMean Squared Error. TODO: add support for multi-dimensional outputs.\n\n\nCalculates the mean squared error regression loss in one dimension.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MultiACE\n \n \nType\n.\n\n\nMultiACE\n\n\n\n\nCalculates the averaged cross-entropy per class and overall (see \nACE\n). This can be used to quantify the influence of different classes on the overall loss.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MultiMetric\n \n \nType\n.\n\n\nMultiMetric(metrics::Vector{AbstractEvalMetric})\n\n\n\n\nCombine multiple metrics in one and get a result for all of them.\n\n\nUsage\n\n\nTo calculate both mean-squared error \nAccuracy\n and log-loss \nACE\n:\n\n\n  mx.fit(..., eval_metric = mx.MultiMetric([mx.Accuracy(), mx.ACE()]))\n\n\n\n\nsource\n\n\n#\n\n\nBase.get\n \n \nMethod\n.\n\n\nget(metric)\n\n\n\n\nGet the accumulated metrics.\n\n\nReturns \nVector{Tuple{Base.Symbol, Real}}\n, a list of name-value pairs. For example, \n[(:accuracy, 0.9)]\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.reset!\n \n \nMethod\n.\n\n\nreset!(metric)\n\n\n\n\nReset the accumulation counter.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.update!\n \n \nMethod\n.\n\n\nupdate!(metric, labels, preds)\n\n\n\n\nUpdate and accumulate metrics.\n\n\nArguments:\n\n\n\n\nmetric::AbstractEvalMetric\n: the metric object.\n\n\nlabels::Vector{NDArray}\n: the labels from the data provider.\n\n\npreds::Vector{NDArray}\n: the outputs (predictions) of the network.\n\n\n\n\nsource", 
            "title": "Evaluation Metrics"
        }, 
        {
            "location": "/api/metric/#evaluation-metrics", 
            "text": "Evaluation metrics provide a way to evaluate the performance of a learned model. This is typically used during training to monitor performance on the validation set.  #  MXNet.mx.ACE     Type .  ACE  Calculates the averaged cross-entropy (logloss) for classification.  source  #  MXNet.mx.AbstractEvalMetric     Type .  AbstractEvalMetric  The base class for all evaluation metrics. The sub-types should implement the following interfaces:   update!  reset!  get   source  #  MXNet.mx.Accuracy     Type .  Accuracy  Multiclass classification accuracy.  Calculates the mean accuracy per sample for softmax in one dimension. For a multi-dimensional softmax the mean accuracy over all dimensions is calculated.  source  #  MXNet.mx.MSE     Type .  MSE  Mean Squared Error. TODO: add support for multi-dimensional outputs.  Calculates the mean squared error regression loss in one dimension.  source  #  MXNet.mx.MultiACE     Type .  MultiACE  Calculates the averaged cross-entropy per class and overall (see  ACE ). This can be used to quantify the influence of different classes on the overall loss.  source  #  MXNet.mx.MultiMetric     Type .  MultiMetric(metrics::Vector{AbstractEvalMetric})  Combine multiple metrics in one and get a result for all of them.  Usage  To calculate both mean-squared error  Accuracy  and log-loss  ACE :    mx.fit(..., eval_metric = mx.MultiMetric([mx.Accuracy(), mx.ACE()]))  source  #  Base.get     Method .  get(metric)  Get the accumulated metrics.  Returns  Vector{Tuple{Base.Symbol, Real}} , a list of name-value pairs. For example,  [(:accuracy, 0.9)] .  source  #  MXNet.mx.reset!     Method .  reset!(metric)  Reset the accumulation counter.  source  #  MXNet.mx.update!     Method .  update!(metric, labels, preds)  Update and accumulate metrics.  Arguments:   metric::AbstractEvalMetric : the metric object.  labels::Vector{NDArray} : the labels from the data provider.  preds::Vector{NDArray} : the outputs (predictions) of the network.   source", 
            "title": "Evaluation Metrics"
        }, 
        {
            "location": "/api/io/", 
            "text": "Data Providers\n\n\nData providers are wrappers that load external data, be it images, text, or general tensors, and split it into mini-batches so that the model can consume the data in a uniformed way.\n\n\n\n\nAbstractDataProvider interface\n\n\n#\n\n\nMXNet.mx.AbstractDataProvider\n \n \nType\n.\n\n\nAbstractDataProvider\n\n\n\n\nThe root type for all data provider. A data provider should implement the following interfaces:\n\n\n\n\nget_batch_size\n\n\nprovide_data\n\n\nprovide_label\n\n\n\n\nAs well as the Julia iterator interface (see \nthe Julia manual\n). Normally this involves defining:\n\n\n\n\nBase.eltype(provider) -\n AbstractDataBatch\n\n\nBase.start(provider) -\n AbstractDataProviderState\n\n\nBase.done(provider, state) -\n Bool\n\n\nBase.next(provider, state) -\n (AbstractDataBatch, AbstractDataProvider)\n\n\n\n\nsource\n\n\nThe difference between \ndata\n and \nlabel\n is that during training stage, both \ndata\n and \nlabel\n will be feeded into the model, while during prediction stage, only \ndata\n is loaded. Otherwise, they could be anything, with any names, and of any shapes. The provided data and label names here should match the input names in a target \nSymbolicNode\n.\n\n\nA data provider should also implement the Julia iteration interface, in order to allow iterating through the data set. The provider will be called in the following way:\n\n\nfor batch in eachbatch(provider)\n    data = get_data(provider, batch)\nend\n\n\n\n\nwhich will be translated by Julia compiler into\n\n\nstate = Base.start(eachbatch(provider))\nwhile !Base.done(provider, state)\n    (batch, state) = Base.next(provider, state)\n    data = get_data(provider, batch)\nend\n\n\n\n\nBy default, \neachbatch\n simply returns the provider itself, so the iterator interface is implemented on the provider type itself. But the extra layer of abstraction allows us to implement a data provider easily via a Julia \nTask\n coroutine. See the data provider defined in \nthe char-lstm example\n for an example of using coroutine to define data providers.\n\n\nThe detailed interface functions for the iterator API is listed below:\n\n\nBase.eltype(provider) -\n AbstractDataBatch\n\n\n\n\nReturns the specific subtype representing a data batch. See \nAbstractDataBatch\n.\n\n\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nBase.start(provider) -\n AbstractDataProviderState\n\n\n\n\n\n\nThis function is always called before iterating into the dataset. It should initialize the iterator, reset the index, and do data shuffling if needed.\n\n\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nBase.done(provider, state) -\n Bool\n\n\n\n\n\n\nTrue if there is no more data to iterate in this dataset.\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\n\n\nstate::AbstractDataProviderState\n: the state returned by \nBase.start\n and \nBase.next\n.\n\n\nBase.next(provider) -\n (AbstractDataBatch, AbstractDataProviderState)\n\n\n\n\n\n\nReturns the current data batch, and the state for the next iteration.\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\n\n\nNote sometimes you are wrapping an existing data iterator (e.g. the built-in libmxnet data iterator) that is built with a different convention. It might be difficult to adapt to the interfaces stated here. In this case, you can safely assume that\n\n\n\n\nBase.start\n will always be called, and called only once before the iteration starts.\n\n\nBase.done\n will always be called at the beginning of every iteration and always be called once.\n\n\nIf \nBase.done\n return true, the iteration will stop, until the next round, again, starting with a call to \nBase.start\n.\n\n\nBase.next\n will always be called only once in each iteration. It will always be called after one and only one call to \nBase.done\n; but if \nBase.done\n returns true, \nBase.next\n will not be called.\n\n\n\n\nWith those assumptions, it will be relatively easy to adapt any existing iterator. See the implementation of the built-in \nMXDataProvider\n for example.\n\n\n\n\nNote\n\n\nPlease do not use the one data provider simultaneously in two different places, either in parallel, or in a nested loop. For example, the behavior for the following code is undefined\n\n\n```julia\nfor batch in data\n    # updating the parameters\n\n\n# now let's test the performance on the training set\nfor b2 in data\n    # ...\nend\n\n\n\nend\n```\n\n\n\n\n#\n\n\nMXNet.mx.get_batch_size\n \n \nFunction\n.\n\n\nget_batch_size(provider) -\n Int\n\n\n\n\nArguments:\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\n\n\nReturns the mini-batch size of the provided data. All the provided data should have the same mini-batch size (i.e. the last dimension).\n\n\nsource\n\n\n#\n\n\nMXNet.mx.provide_data\n \n \nFunction\n.\n\n\nprovide_data(provider) -\n Vector{Tuple{Base.Symbol, Tuple}}\n\n\n\n\nArguments:\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\n\n\nReturns a vector of (name, shape) pairs describing the names of the data it provides, and the corresponding shapes.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.provide_label\n \n \nFunction\n.\n\n\nprovide_label(provider) -\n Vector{Tuple{Base.Symbol, Tuple}}\n\n\n\n\nArguments:\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\n\n\nReturns a vector of (name, shape) pairs describing the names of the labels it provides, and the corresponding shapes.\n\n\nsource\n\n\n\n\nAbstractDataBatch interface\n\n\n#\n\n\nMXNet.mx.AbstractDataProviderState\n \n \nType\n.\n\n\nAbstractDataProviderState\n\n\n\n\nBase type for data provider states.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.count_samples\n \n \nFunction\n.\n\n\ncount_samples(provider, batch) -\n Int\n\n\n\n\nArguments:\n\n\n\n\nbatch::AbstractDataBatch\n: the data batch object.\n\n\n\n\nReturns the number of samples in this batch. This number should be greater than 0, but less than or equal to the batch size. This is used to indicate at the end of the data set, there might not be enough samples for a whole mini-batch.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_data\n \n \nFunction\n.\n\n\nget_data(provider, batch) -\n Vector{NDArray}\n\n\n\n\nArguments:\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nbatch::AbstractDataBatch\n: the data batch object.\n\n\n\n\nReturns a vector of data in this batch, should be in the same order as declared in \nprovide_data() \nAbstractDataProvider.provide_data\n.\n\n\nThe last dimension of each \nNDArray\n should always match the batch_size, even when \ncount_samples\n returns a value less than the batch size. In this case,      the data provider is free to pad the remaining contents with any value.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_label\n \n \nFunction\n.\n\n\nget_label(provider, batch) -\n Vector{NDArray}\n\n\n\n\nArguments:\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nbatch::AbstractDataBatch\n: the data batch object.\n\n\n\n\nReturns a vector of labels in this batch. Similar to \nget_data\n.\n\n\nsource\n\n\n#\n\n\nBase.get\n \n \nFunction\n.\n\n\nget(metric)\n\n\n\n\nGet the accumulated metrics.\n\n\nReturns \nVector{Tuple{Base.Symbol, Real}}\n, a list of name-value pairs. For example, \n[(:accuracy, 0.9)]\n.\n\n\nsource\n\n\nget(provider, batch, name) -\n NDArray\n\n\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nbatch::AbstractDataBatch\n: the data batch object.\n\n\nname::Symbol\n: the name of the data to get, should be one of the names provided in either \nprovide_data() \nAbstractDataProvider.provide_data\n or \nprovide_label() \nAbstractDataProvider.provide_label\n.\n\n\n\n\nReturns the corresponding data array corresponding to that name.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.load_data!\n \n \nFunction\n.\n\n\nload_data!(provider, batch, targets)\n\n\n\n\nArguments:\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nbatch::AbstractDataBatch\n: the data batch object.\n\n\ntargets::Vector{Vector{SlicedNDArray}}\n: the targets to load data into.\n\n\n\n\nThe targets is a list of the same length as number of data provided by this provider. Each element in the list is a list of \nSlicedNDArray\n. This list described a spliting scheme of this data batch into different slices, each slice is specified by a slice-ndarray pair, where \nslice\n specify the range of samples in the mini-batch that should be loaded into the corresponding \nndarray\n.\n\n\nThis utility function is used in data parallelization, where a mini-batch is splited and computed on several different devices.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.load_label!\n \n \nFunction\n.\n\n\nload_label!(provider, batch, targets)\n\n\n\n\n\n\nprovider::AbstractDataProvider provider\n: the data provider.\n\n\nbatch::AbstractDataBatch batch\n: the data batch object.\n\n\ntargets::Vector{Vector{SlicedNDArray}}\n: the targets to load label into.\n\n\n\n\nThe same as \nload_data!\n, except that this is for loading labels.\n\n\nsource\n\n\n\n\nImplemented providers and other methods\n\n\n#\n\n\nMXNet.mx.AbstractDataBatch\n \n \nType\n.\n\n\nAbstractDataBatch\n\n\n\n\nBase type for a data mini-batch. It should implement the following interfaces:\n\n\n\n\ncount_samples\n\n\nget_data\n\n\nget_label\n\n\n\n\nThe following utility functions will be automatically defined:\n\n\n\n\nget\n\n\nload_data!\n\n\nload_label!\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ArrayDataProvider\n \n \nType\n.\n\n\nArrayDataProvider\n\n\n\n\nA convenient tool to iterate \nNDArray\n or Julia \nArray\n.\n\n\nArrayDataProvider(data[, label]; batch_size, shuffle, data_padding, label_padding)\n\n\n\n\nConstruct a data provider from \nNDArray\n or Julia Arrays.\n\n\nArguments:\n\n\n\n\n\n\ndata\n: the data, could be\n\n\n\n\na \nNDArray\n, or a Julia Array. This is equivalent to \n:data =\n data\n.\n\n\na name-data pair, like \n:mydata =\n array\n, where \n:mydata\n is the name of the data\n\n\nand \narray\n is an \nNDArray\n or a Julia Array.\n\n\na list of name-data pairs.\n\n\nlabel\n: the same as the \ndata\n parameter. When this argument is omitted, the constructed provider will provide no labels.\n\n\nbatch_size::Int\n: the batch size, default is 0, which means treating the whole array as a single mini-batch.\n\n\nshuffle::Bool\n: turn on if the data should be shuffled at every epoch.\n\n\ndata_padding::Real\n: when the mini-batch goes beyond the dataset boundary, there might be less samples to include than a mini-batch. This value specify a scalar to pad the contents of all the missing data points.\n\n\nlabel_padding::Real\n: the same as \ndata_padding\n, except for the labels.\n\n\n\n\n\n\n\n\nTODO: remove \ndata_padding\n and \nlabel_padding\n, and implement rollover that copies the last or first several training samples to feed the padding.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.DataBatch\n \n \nType\n.\n\n\nDataBatch\n\n\n\n\nA basic subclass of \nAbstractDataBatch\n, that implement the interface by accessing member fields.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MXDataProvider\n \n \nType\n.\n\n\nMXDataProvider\n\n\n\n\nA data provider that wrap built-in data iterators from libmxnet. See below for a list of built-in data iterators.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SlicedNDArray\n \n \nType\n.\n\n\nSlicedNDArray\n\n\n\n\nA alias type of \nTuple{UnitRange{Int},NDArray}\n.\n\n\nsource\n\n\n#\n\n\nBase.get\n \n \nMethod\n.\n\n\nget(provider, batch, name) -\n NDArray\n\n\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nbatch::AbstractDataBatch\n: the data batch object.\n\n\nname::Symbol\n: the name of the data to get, should be one of the names provided in either \nprovide_data() \nAbstractDataProvider.provide_data\n or \nprovide_label() \nAbstractDataProvider.provide_label\n.\n\n\n\n\nReturns the corresponding data array corresponding to that name.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.CSVIter\n \n \nMethod\n.\n\n\nCSVIter(data_csv, data_shape, label_csv, label_shape)\n\n\n\n\nCan also be called with the alias \nCSVProvider\n. Create iterator for dataset in csv.\n\n\nArguments:\n\n\n\n\ndata_name::Symbol\n: keyword argument, default \n:data\n. The name of the data.\n\n\nlabel_name::Symbol\n: keyword argument, default \n:softmax_label\n. The name of the label. Could be \nnothing\n if no label is presented in this dataset.\n\n\ndata_csv::string, required\n: Dataset Param: Data csv path.\n\n\ndata_shape::Shape(tuple), required\n: Dataset Param: Shape of the data.\n\n\nlabel_csv::string, optional, default='NULL'\n: Dataset Param: Label csv path. If is NULL, all labels will be returned as 0\n\n\nlabel_shape::Shape(tuple), optional, default=(1,)\n: Dataset Param: Shape of the label.\n\n\n\n\nReturns the constructed \nMXDataProvider\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ImageRecordIter\n \n \nMethod\n.\n\n\nImageRecordIter(path_imglist, path_imgrec, aug_seq, label_width, data_shape, preprocess_threads, verbose, num_parts, part_index, shuffle_chunk_size, shuffle_chunk_seed, shuffle, seed, verbose, batch_size, round_batch, prefetch_buffer, dtype, resize, rand_crop, crop_y_start, crop_x_start, max_rotate_angle, max_aspect_ratio, max_shear_ratio, max_crop_size, min_crop_size, max_random_scale, min_random_scale, max_img_size, min_img_size, random_h, random_s, random_l, rotate, fill_value, data_shape, inter_method, pad, seed, mirror, rand_mirror, mean_img, mean_r, mean_g, mean_b, mean_a, scale, max_random_contrast, max_random_illumination, verbose)\n\n\n\n\nCan also be called with the alias \nImageRecordProvider\n. Create iterator for dataset packed in recordio.\n\n\nArguments:\n\n\n\n\ndata_name::Symbol\n: keyword argument, default \n:data\n. The name of the data.\n\n\nlabel_name::Symbol\n: keyword argument, default \n:softmax_label\n. The name of the label. Could be \nnothing\n if no label is presented in this dataset.\n\n\npath_imglist::string, optional, default=''\n: Dataset Param: Path to image list.\n\n\npath_imgrec::string, optional, default='./data/imgrec.rec'\n: Dataset Param: Path to image record file.\n\n\naug_seq::string, optional, default='aug_default'\n: Augmentation Param: the augmenter names to represent sequence of augmenters to be applied, seperated by comma. Additional keyword parameters will be seen by these augmenters.\n\n\nlabel_width::int, optional, default='1'\n: Dataset Param: How many labels for an image.\n\n\ndata_shape::Shape(tuple), required\n: Dataset Param: Shape of each instance generated by the DataIter.\n\n\npreprocess_threads::int, optional, default='4'\n: Backend Param: Number of thread to do preprocessing.\n\n\nverbose::boolean, optional, default=True\n: Auxiliary Param: Whether to output parser information.\n\n\nnum_parts::int, optional, default='1'\n: partition the data into multiple parts\n\n\npart_index::int, optional, default='0'\n: the index of the part will read\n\n\nshuffle_chunk_size::long (non-negative), optional, default=0\n: the size(MB) of the shuffle chunk, used with shuffle=True, it can enable global shuffling\n\n\nshuffle_chunk_seed::int, optional, default='0'\n: the seed for chunk shuffling\n\n\nshuffle::boolean, optional, default=False\n: Augmentation Param: Whether to shuffle data.\n\n\nseed::int, optional, default='0'\n: Augmentation Param: Random Seed.\n\n\nbatch_size::int (non-negative), required\n: Batch Param: Batch size.\n\n\nround_batch::boolean, optional, default=True\n: Batch Param: Use round robin to handle overflow batch.\n\n\nprefetch_buffer::long (non-negative), optional, default=4\n: Backend Param: Number of prefetched parameters\n\n\ndtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None'\n: Output data type. Leave as None to useinternal data iterator's output type\n\n\nresize::int, optional, default='-1'\n: Augmentation Param: scale shorter edge to size before applying other augmentations.\n\n\nrand_crop::boolean, optional, default=False\n: Augmentation Param: Whether to random crop on the image\n\n\ncrop_y_start::int, optional, default='-1'\n: Augmentation Param: Where to nonrandom crop on y.\n\n\ncrop_x_start::int, optional, default='-1'\n: Augmentation Param: Where to nonrandom crop on x.\n\n\nmax_rotate_angle::int, optional, default='0'\n: Augmentation Param: rotated randomly in [-max_rotate_angle, max_rotate_angle].\n\n\nmax_aspect_ratio::float, optional, default=0\n: Augmentation Param: denotes the max ratio of random aspect ratio augmentation.\n\n\nmax_shear_ratio::float, optional, default=0\n: Augmentation Param: denotes the max random shearing ratio.\n\n\nmax_crop_size::int, optional, default='-1'\n: Augmentation Param: Maximum crop size.\n\n\nmin_crop_size::int, optional, default='-1'\n: Augmentation Param: Minimum crop size.\n\n\nmax_random_scale::float, optional, default=1\n: Augmentation Param: Maximum scale ratio.\n\n\nmin_random_scale::float, optional, default=1\n: Augmentation Param: Minimum scale ratio.\n\n\nmax_img_size::float, optional, default=1e+10\n: Augmentation Param: Maximum image size after resizing.\n\n\nmin_img_size::float, optional, default=0\n: Augmentation Param: Minimum image size after resizing.\n\n\nrandom_h::int, optional, default='0'\n: Augmentation Param: Maximum random value of H channel in HSL color space.\n\n\nrandom_s::int, optional, default='0'\n: Augmentation Param: Maximum random value of S channel in HSL color space.\n\n\nrandom_l::int, optional, default='0'\n: Augmentation Param: Maximum random value of L channel in HSL color space.\n\n\nrotate::int, optional, default='-1'\n: Augmentation Param: Rotate angle.\n\n\nfill_value::int, optional, default='255'\n: Augmentation Param: Filled color value while padding.\n\n\ninter_method::int, optional, default='1'\n: Augmentation Param: 0-NN 1-bilinear 2-cubic 3-area 4-lanczos4 9-auto 10-rand.\n\n\npad::int, optional, default='0'\n: Augmentation Param: Padding size.\n\n\nmirror::boolean, optional, default=False\n: Augmentation Param: Whether to mirror the image.\n\n\nrand_mirror::boolean, optional, default=False\n: Augmentation Param: Whether to mirror the image randomly.\n\n\nmean_img::string, optional, default=''\n: Augmentation Param: Mean Image to be subtracted.\n\n\nmean_r::float, optional, default=0\n: Augmentation Param: Mean value on R channel.\n\n\nmean_g::float, optional, default=0\n: Augmentation Param: Mean value on G channel.\n\n\nmean_b::float, optional, default=0\n: Augmentation Param: Mean value on B channel.\n\n\nmean_a::float, optional, default=0\n: Augmentation Param: Mean value on Alpha channel.\n\n\nscale::float, optional, default=1\n: Augmentation Param: Scale in color space.\n\n\nmax_random_contrast::float, optional, default=0\n: Augmentation Param: Maximum ratio of contrast variation.\n\n\nmax_random_illumination::float, optional, default=0\n: Augmentation Param: Maximum value of illumination variation.\n\n\n\n\nReturns the constructed \nMXDataProvider\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ImageRecordUInt8Iter\n \n \nMethod\n.\n\n\nImageRecordUInt8Iter(path_imglist, path_imgrec, aug_seq, label_width, data_shape, preprocess_threads, verbose, num_parts, part_index, shuffle_chunk_size, shuffle_chunk_seed, shuffle, seed, verbose, batch_size, round_batch, prefetch_buffer, dtype, resize, rand_crop, crop_y_start, crop_x_start, max_rotate_angle, max_aspect_ratio, max_shear_ratio, max_crop_size, min_crop_size, max_random_scale, min_random_scale, max_img_size, min_img_size, random_h, random_s, random_l, rotate, fill_value, data_shape, inter_method, pad)\n\n\n\n\nCan also be called with the alias \nImageRecordUInt8Provider\n. Create iterator for dataset packed in recordio.\n\n\nArguments:\n\n\n\n\ndata_name::Symbol\n: keyword argument, default \n:data\n. The name of the data.\n\n\nlabel_name::Symbol\n: keyword argument, default \n:softmax_label\n. The name of the label. Could be \nnothing\n if no label is presented in this dataset.\n\n\npath_imglist::string, optional, default=''\n: Dataset Param: Path to image list.\n\n\npath_imgrec::string, optional, default='./data/imgrec.rec'\n: Dataset Param: Path to image record file.\n\n\naug_seq::string, optional, default='aug_default'\n: Augmentation Param: the augmenter names to represent sequence of augmenters to be applied, seperated by comma. Additional keyword parameters will be seen by these augmenters.\n\n\nlabel_width::int, optional, default='1'\n: Dataset Param: How many labels for an image.\n\n\ndata_shape::Shape(tuple), required\n: Dataset Param: Shape of each instance generated by the DataIter.\n\n\npreprocess_threads::int, optional, default='4'\n: Backend Param: Number of thread to do preprocessing.\n\n\nverbose::boolean, optional, default=True\n: Auxiliary Param: Whether to output parser information.\n\n\nnum_parts::int, optional, default='1'\n: partition the data into multiple parts\n\n\npart_index::int, optional, default='0'\n: the index of the part will read\n\n\nshuffle_chunk_size::long (non-negative), optional, default=0\n: the size(MB) of the shuffle chunk, used with shuffle=True, it can enable global shuffling\n\n\nshuffle_chunk_seed::int, optional, default='0'\n: the seed for chunk shuffling\n\n\nshuffle::boolean, optional, default=False\n: Augmentation Param: Whether to shuffle data.\n\n\nseed::int, optional, default='0'\n: Augmentation Param: Random Seed.\n\n\nbatch_size::int (non-negative), required\n: Batch Param: Batch size.\n\n\nround_batch::boolean, optional, default=True\n: Batch Param: Use round robin to handle overflow batch.\n\n\nprefetch_buffer::long (non-negative), optional, default=4\n: Backend Param: Number of prefetched parameters\n\n\ndtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None'\n: Output data type. Leave as None to useinternal data iterator's output type\n\n\nresize::int, optional, default='-1'\n: Augmentation Param: scale shorter edge to size before applying other augmentations.\n\n\nrand_crop::boolean, optional, default=False\n: Augmentation Param: Whether to random crop on the image\n\n\ncrop_y_start::int, optional, default='-1'\n: Augmentation Param: Where to nonrandom crop on y.\n\n\ncrop_x_start::int, optional, default='-1'\n: Augmentation Param: Where to nonrandom crop on x.\n\n\nmax_rotate_angle::int, optional, default='0'\n: Augmentation Param: rotated randomly in [-max_rotate_angle, max_rotate_angle].\n\n\nmax_aspect_ratio::float, optional, default=0\n: Augmentation Param: denotes the max ratio of random aspect ratio augmentation.\n\n\nmax_shear_ratio::float, optional, default=0\n: Augmentation Param: denotes the max random shearing ratio.\n\n\nmax_crop_size::int, optional, default='-1'\n: Augmentation Param: Maximum crop size.\n\n\nmin_crop_size::int, optional, default='-1'\n: Augmentation Param: Minimum crop size.\n\n\nmax_random_scale::float, optional, default=1\n: Augmentation Param: Maximum scale ratio.\n\n\nmin_random_scale::float, optional, default=1\n: Augmentation Param: Minimum scale ratio.\n\n\nmax_img_size::float, optional, default=1e+10\n: Augmentation Param: Maximum image size after resizing.\n\n\nmin_img_size::float, optional, default=0\n: Augmentation Param: Minimum image size after resizing.\n\n\nrandom_h::int, optional, default='0'\n: Augmentation Param: Maximum random value of H channel in HSL color space.\n\n\nrandom_s::int, optional, default='0'\n: Augmentation Param: Maximum random value of S channel in HSL color space.\n\n\nrandom_l::int, optional, default='0'\n: Augmentation Param: Maximum random value of L channel in HSL color space.\n\n\nrotate::int, optional, default='-1'\n: Augmentation Param: Rotate angle.\n\n\nfill_value::int, optional, default='255'\n: Augmentation Param: Filled color value while padding.\n\n\ninter_method::int, optional, default='1'\n: Augmentation Param: 0-NN 1-bilinear 2-cubic 3-area 4-lanczos4 9-auto 10-rand.\n\n\npad::int, optional, default='0'\n: Augmentation Param: Padding size.\n\n\n\n\nReturns the constructed \nMXDataProvider\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MNISTIter\n \n \nMethod\n.\n\n\nMNISTIter(image, label, batch_size, shuffle, flat, seed, silent, num_parts, part_index, prefetch_buffer, dtype)\n\n\n\n\nCan also be called with the alias \nMNISTProvider\n. Create iterator for MNIST hand-written digit number recognition dataset.\n\n\nArguments:\n\n\n\n\ndata_name::Symbol\n: keyword argument, default \n:data\n. The name of the data.\n\n\nlabel_name::Symbol\n: keyword argument, default \n:softmax_label\n. The name of the label. Could be \nnothing\n if no label is presented in this dataset.\n\n\nimage::string, optional, default='./train-images-idx3-ubyte'\n: Dataset Param: Mnist image path.\n\n\nlabel::string, optional, default='./train-labels-idx1-ubyte'\n: Dataset Param: Mnist label path.\n\n\nbatch_size::int, optional, default='128'\n: Batch Param: Batch Size.\n\n\nshuffle::boolean, optional, default=True\n: Augmentation Param: Whether to shuffle data.\n\n\nflat::boolean, optional, default=False\n: Augmentation Param: Whether to flat the data into 1D.\n\n\nseed::int, optional, default='0'\n: Augmentation Param: Random Seed.\n\n\nsilent::boolean, optional, default=False\n: Auxiliary Param: Whether to print out data info.\n\n\nnum_parts::int, optional, default='1'\n: partition the data into multiple parts\n\n\npart_index::int, optional, default='0'\n: the index of the part will read\n\n\nprefetch_buffer::long (non-negative), optional, default=4\n: Backend Param: Number of prefetched parameters\n\n\ndtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None'\n: Output data type. Leave as None to useinternal data iterator's output type\n\n\n\n\nReturns the constructed \nMXDataProvider\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.eachbatch\n \n \nMethod\n.\n\n\neachbatch(provider::AbstractDataProvider)\n\n\n\n\nAllows you to perform operations on data every epoch. This is especially useful when you need to perform real-time augmentation of the data. \n\n\nArguments:\n\n\n\n\nprovider\n: an instance of the custom DataProvider type. You must return this\n\n\n\n\ninstance after modifying its fields.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.load_data!\n \n \nMethod\n.\n\n\nload_data!(provider, batch, targets)\n\n\n\n\nArguments:\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nbatch::AbstractDataBatch\n: the data batch object.\n\n\ntargets::Vector{Vector{SlicedNDArray}}\n: the targets to load data into.\n\n\n\n\nThe targets is a list of the same length as number of data provided by this provider. Each element in the list is a list of \nSlicedNDArray\n. This list described a spliting scheme of this data batch into different slices, each slice is specified by a slice-ndarray pair, where \nslice\n specify the range of samples in the mini-batch that should be loaded into the corresponding \nndarray\n.\n\n\nThis utility function is used in data parallelization, where a mini-batch is splited and computed on several different devices.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.load_label!\n \n \nMethod\n.\n\n\nload_label!(provider, batch, targets)\n\n\n\n\n\n\nprovider::AbstractDataProvider provider\n: the data provider.\n\n\nbatch::AbstractDataBatch batch\n: the data batch object.\n\n\ntargets::Vector{Vector{SlicedNDArray}}\n: the targets to load label into.\n\n\n\n\nThe same as \nload_data!\n, except that this is for loading labels.\n\n\nsource", 
            "title": "Data Providers"
        }, 
        {
            "location": "/api/io/#data-providers", 
            "text": "Data providers are wrappers that load external data, be it images, text, or general tensors, and split it into mini-batches so that the model can consume the data in a uniformed way.", 
            "title": "Data Providers"
        }, 
        {
            "location": "/api/io/#abstractdataprovider-interface", 
            "text": "#  MXNet.mx.AbstractDataProvider     Type .  AbstractDataProvider  The root type for all data provider. A data provider should implement the following interfaces:   get_batch_size  provide_data  provide_label   As well as the Julia iterator interface (see  the Julia manual ). Normally this involves defining:   Base.eltype(provider) -  AbstractDataBatch  Base.start(provider) -  AbstractDataProviderState  Base.done(provider, state) -  Bool  Base.next(provider, state) -  (AbstractDataBatch, AbstractDataProvider)   source  The difference between  data  and  label  is that during training stage, both  data  and  label  will be feeded into the model, while during prediction stage, only  data  is loaded. Otherwise, they could be anything, with any names, and of any shapes. The provided data and label names here should match the input names in a target  SymbolicNode .  A data provider should also implement the Julia iteration interface, in order to allow iterating through the data set. The provider will be called in the following way:  for batch in eachbatch(provider)\n    data = get_data(provider, batch)\nend  which will be translated by Julia compiler into  state = Base.start(eachbatch(provider))\nwhile !Base.done(provider, state)\n    (batch, state) = Base.next(provider, state)\n    data = get_data(provider, batch)\nend  By default,  eachbatch  simply returns the provider itself, so the iterator interface is implemented on the provider type itself. But the extra layer of abstraction allows us to implement a data provider easily via a Julia  Task  coroutine. See the data provider defined in  the char-lstm example  for an example of using coroutine to define data providers.  The detailed interface functions for the iterator API is listed below:  Base.eltype(provider) -  AbstractDataBatch  Returns the specific subtype representing a data batch. See  AbstractDataBatch .    provider::AbstractDataProvider : the data provider.  Base.start(provider) -  AbstractDataProviderState    This function is always called before iterating into the dataset. It should initialize the iterator, reset the index, and do data shuffling if needed.    provider::AbstractDataProvider : the data provider.  Base.done(provider, state) -  Bool    True if there is no more data to iterate in this dataset.   provider::AbstractDataProvider : the data provider.   state::AbstractDataProviderState : the state returned by  Base.start  and  Base.next .  Base.next(provider) -  (AbstractDataBatch, AbstractDataProviderState)    Returns the current data batch, and the state for the next iteration.   provider::AbstractDataProvider : the data provider.   Note sometimes you are wrapping an existing data iterator (e.g. the built-in libmxnet data iterator) that is built with a different convention. It might be difficult to adapt to the interfaces stated here. In this case, you can safely assume that   Base.start  will always be called, and called only once before the iteration starts.  Base.done  will always be called at the beginning of every iteration and always be called once.  If  Base.done  return true, the iteration will stop, until the next round, again, starting with a call to  Base.start .  Base.next  will always be called only once in each iteration. It will always be called after one and only one call to  Base.done ; but if  Base.done  returns true,  Base.next  will not be called.   With those assumptions, it will be relatively easy to adapt any existing iterator. See the implementation of the built-in  MXDataProvider  for example.   Note  Please do not use the one data provider simultaneously in two different places, either in parallel, or in a nested loop. For example, the behavior for the following code is undefined  ```julia\nfor batch in data\n    # updating the parameters  # now let's test the performance on the training set\nfor b2 in data\n    # ...\nend  end\n```   #  MXNet.mx.get_batch_size     Function .  get_batch_size(provider) -  Int  Arguments:   provider::AbstractDataProvider : the data provider.   Returns the mini-batch size of the provided data. All the provided data should have the same mini-batch size (i.e. the last dimension).  source  #  MXNet.mx.provide_data     Function .  provide_data(provider) -  Vector{Tuple{Base.Symbol, Tuple}}  Arguments:   provider::AbstractDataProvider : the data provider.   Returns a vector of (name, shape) pairs describing the names of the data it provides, and the corresponding shapes.  source  #  MXNet.mx.provide_label     Function .  provide_label(provider) -  Vector{Tuple{Base.Symbol, Tuple}}  Arguments:   provider::AbstractDataProvider : the data provider.   Returns a vector of (name, shape) pairs describing the names of the labels it provides, and the corresponding shapes.  source", 
            "title": "AbstractDataProvider interface"
        }, 
        {
            "location": "/api/io/#abstractdatabatch-interface", 
            "text": "#  MXNet.mx.AbstractDataProviderState     Type .  AbstractDataProviderState  Base type for data provider states.  source  #  MXNet.mx.count_samples     Function .  count_samples(provider, batch) -  Int  Arguments:   batch::AbstractDataBatch : the data batch object.   Returns the number of samples in this batch. This number should be greater than 0, but less than or equal to the batch size. This is used to indicate at the end of the data set, there might not be enough samples for a whole mini-batch.  source  #  MXNet.mx.get_data     Function .  get_data(provider, batch) -  Vector{NDArray}  Arguments:   provider::AbstractDataProvider : the data provider.  batch::AbstractDataBatch : the data batch object.   Returns a vector of data in this batch, should be in the same order as declared in  provide_data()  AbstractDataProvider.provide_data .  The last dimension of each  NDArray  should always match the batch_size, even when  count_samples  returns a value less than the batch size. In this case,      the data provider is free to pad the remaining contents with any value.  source  #  MXNet.mx.get_label     Function .  get_label(provider, batch) -  Vector{NDArray}  Arguments:   provider::AbstractDataProvider : the data provider.  batch::AbstractDataBatch : the data batch object.   Returns a vector of labels in this batch. Similar to  get_data .  source  #  Base.get     Function .  get(metric)  Get the accumulated metrics.  Returns  Vector{Tuple{Base.Symbol, Real}} , a list of name-value pairs. For example,  [(:accuracy, 0.9)] .  source  get(provider, batch, name) -  NDArray   provider::AbstractDataProvider : the data provider.  batch::AbstractDataBatch : the data batch object.  name::Symbol : the name of the data to get, should be one of the names provided in either  provide_data()  AbstractDataProvider.provide_data  or  provide_label()  AbstractDataProvider.provide_label .   Returns the corresponding data array corresponding to that name.  source  #  MXNet.mx.load_data!     Function .  load_data!(provider, batch, targets)  Arguments:   provider::AbstractDataProvider : the data provider.  batch::AbstractDataBatch : the data batch object.  targets::Vector{Vector{SlicedNDArray}} : the targets to load data into.   The targets is a list of the same length as number of data provided by this provider. Each element in the list is a list of  SlicedNDArray . This list described a spliting scheme of this data batch into different slices, each slice is specified by a slice-ndarray pair, where  slice  specify the range of samples in the mini-batch that should be loaded into the corresponding  ndarray .  This utility function is used in data parallelization, where a mini-batch is splited and computed on several different devices.  source  #  MXNet.mx.load_label!     Function .  load_label!(provider, batch, targets)   provider::AbstractDataProvider provider : the data provider.  batch::AbstractDataBatch batch : the data batch object.  targets::Vector{Vector{SlicedNDArray}} : the targets to load label into.   The same as  load_data! , except that this is for loading labels.  source", 
            "title": "AbstractDataBatch interface"
        }, 
        {
            "location": "/api/io/#implemented-providers-and-other-methods", 
            "text": "#  MXNet.mx.AbstractDataBatch     Type .  AbstractDataBatch  Base type for a data mini-batch. It should implement the following interfaces:   count_samples  get_data  get_label   The following utility functions will be automatically defined:   get  load_data!  load_label!   source  #  MXNet.mx.ArrayDataProvider     Type .  ArrayDataProvider  A convenient tool to iterate  NDArray  or Julia  Array .  ArrayDataProvider(data[, label]; batch_size, shuffle, data_padding, label_padding)  Construct a data provider from  NDArray  or Julia Arrays.  Arguments:    data : the data, could be   a  NDArray , or a Julia Array. This is equivalent to  :data =  data .  a name-data pair, like  :mydata =  array , where  :mydata  is the name of the data  and  array  is an  NDArray  or a Julia Array.  a list of name-data pairs.  label : the same as the  data  parameter. When this argument is omitted, the constructed provider will provide no labels.  batch_size::Int : the batch size, default is 0, which means treating the whole array as a single mini-batch.  shuffle::Bool : turn on if the data should be shuffled at every epoch.  data_padding::Real : when the mini-batch goes beyond the dataset boundary, there might be less samples to include than a mini-batch. This value specify a scalar to pad the contents of all the missing data points.  label_padding::Real : the same as  data_padding , except for the labels.     TODO: remove  data_padding  and  label_padding , and implement rollover that copies the last or first several training samples to feed the padding.  source  #  MXNet.mx.DataBatch     Type .  DataBatch  A basic subclass of  AbstractDataBatch , that implement the interface by accessing member fields.  source  #  MXNet.mx.MXDataProvider     Type .  MXDataProvider  A data provider that wrap built-in data iterators from libmxnet. See below for a list of built-in data iterators.  source  #  MXNet.mx.SlicedNDArray     Type .  SlicedNDArray  A alias type of  Tuple{UnitRange{Int},NDArray} .  source  #  Base.get     Method .  get(provider, batch, name) -  NDArray   provider::AbstractDataProvider : the data provider.  batch::AbstractDataBatch : the data batch object.  name::Symbol : the name of the data to get, should be one of the names provided in either  provide_data()  AbstractDataProvider.provide_data  or  provide_label()  AbstractDataProvider.provide_label .   Returns the corresponding data array corresponding to that name.  source  #  MXNet.mx.CSVIter     Method .  CSVIter(data_csv, data_shape, label_csv, label_shape)  Can also be called with the alias  CSVProvider . Create iterator for dataset in csv.  Arguments:   data_name::Symbol : keyword argument, default  :data . The name of the data.  label_name::Symbol : keyword argument, default  :softmax_label . The name of the label. Could be  nothing  if no label is presented in this dataset.  data_csv::string, required : Dataset Param: Data csv path.  data_shape::Shape(tuple), required : Dataset Param: Shape of the data.  label_csv::string, optional, default='NULL' : Dataset Param: Label csv path. If is NULL, all labels will be returned as 0  label_shape::Shape(tuple), optional, default=(1,) : Dataset Param: Shape of the label.   Returns the constructed  MXDataProvider .  source  #  MXNet.mx.ImageRecordIter     Method .  ImageRecordIter(path_imglist, path_imgrec, aug_seq, label_width, data_shape, preprocess_threads, verbose, num_parts, part_index, shuffle_chunk_size, shuffle_chunk_seed, shuffle, seed, verbose, batch_size, round_batch, prefetch_buffer, dtype, resize, rand_crop, crop_y_start, crop_x_start, max_rotate_angle, max_aspect_ratio, max_shear_ratio, max_crop_size, min_crop_size, max_random_scale, min_random_scale, max_img_size, min_img_size, random_h, random_s, random_l, rotate, fill_value, data_shape, inter_method, pad, seed, mirror, rand_mirror, mean_img, mean_r, mean_g, mean_b, mean_a, scale, max_random_contrast, max_random_illumination, verbose)  Can also be called with the alias  ImageRecordProvider . Create iterator for dataset packed in recordio.  Arguments:   data_name::Symbol : keyword argument, default  :data . The name of the data.  label_name::Symbol : keyword argument, default  :softmax_label . The name of the label. Could be  nothing  if no label is presented in this dataset.  path_imglist::string, optional, default='' : Dataset Param: Path to image list.  path_imgrec::string, optional, default='./data/imgrec.rec' : Dataset Param: Path to image record file.  aug_seq::string, optional, default='aug_default' : Augmentation Param: the augmenter names to represent sequence of augmenters to be applied, seperated by comma. Additional keyword parameters will be seen by these augmenters.  label_width::int, optional, default='1' : Dataset Param: How many labels for an image.  data_shape::Shape(tuple), required : Dataset Param: Shape of each instance generated by the DataIter.  preprocess_threads::int, optional, default='4' : Backend Param: Number of thread to do preprocessing.  verbose::boolean, optional, default=True : Auxiliary Param: Whether to output parser information.  num_parts::int, optional, default='1' : partition the data into multiple parts  part_index::int, optional, default='0' : the index of the part will read  shuffle_chunk_size::long (non-negative), optional, default=0 : the size(MB) of the shuffle chunk, used with shuffle=True, it can enable global shuffling  shuffle_chunk_seed::int, optional, default='0' : the seed for chunk shuffling  shuffle::boolean, optional, default=False : Augmentation Param: Whether to shuffle data.  seed::int, optional, default='0' : Augmentation Param: Random Seed.  batch_size::int (non-negative), required : Batch Param: Batch size.  round_batch::boolean, optional, default=True : Batch Param: Use round robin to handle overflow batch.  prefetch_buffer::long (non-negative), optional, default=4 : Backend Param: Number of prefetched parameters  dtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None' : Output data type. Leave as None to useinternal data iterator's output type  resize::int, optional, default='-1' : Augmentation Param: scale shorter edge to size before applying other augmentations.  rand_crop::boolean, optional, default=False : Augmentation Param: Whether to random crop on the image  crop_y_start::int, optional, default='-1' : Augmentation Param: Where to nonrandom crop on y.  crop_x_start::int, optional, default='-1' : Augmentation Param: Where to nonrandom crop on x.  max_rotate_angle::int, optional, default='0' : Augmentation Param: rotated randomly in [-max_rotate_angle, max_rotate_angle].  max_aspect_ratio::float, optional, default=0 : Augmentation Param: denotes the max ratio of random aspect ratio augmentation.  max_shear_ratio::float, optional, default=0 : Augmentation Param: denotes the max random shearing ratio.  max_crop_size::int, optional, default='-1' : Augmentation Param: Maximum crop size.  min_crop_size::int, optional, default='-1' : Augmentation Param: Minimum crop size.  max_random_scale::float, optional, default=1 : Augmentation Param: Maximum scale ratio.  min_random_scale::float, optional, default=1 : Augmentation Param: Minimum scale ratio.  max_img_size::float, optional, default=1e+10 : Augmentation Param: Maximum image size after resizing.  min_img_size::float, optional, default=0 : Augmentation Param: Minimum image size after resizing.  random_h::int, optional, default='0' : Augmentation Param: Maximum random value of H channel in HSL color space.  random_s::int, optional, default='0' : Augmentation Param: Maximum random value of S channel in HSL color space.  random_l::int, optional, default='0' : Augmentation Param: Maximum random value of L channel in HSL color space.  rotate::int, optional, default='-1' : Augmentation Param: Rotate angle.  fill_value::int, optional, default='255' : Augmentation Param: Filled color value while padding.  inter_method::int, optional, default='1' : Augmentation Param: 0-NN 1-bilinear 2-cubic 3-area 4-lanczos4 9-auto 10-rand.  pad::int, optional, default='0' : Augmentation Param: Padding size.  mirror::boolean, optional, default=False : Augmentation Param: Whether to mirror the image.  rand_mirror::boolean, optional, default=False : Augmentation Param: Whether to mirror the image randomly.  mean_img::string, optional, default='' : Augmentation Param: Mean Image to be subtracted.  mean_r::float, optional, default=0 : Augmentation Param: Mean value on R channel.  mean_g::float, optional, default=0 : Augmentation Param: Mean value on G channel.  mean_b::float, optional, default=0 : Augmentation Param: Mean value on B channel.  mean_a::float, optional, default=0 : Augmentation Param: Mean value on Alpha channel.  scale::float, optional, default=1 : Augmentation Param: Scale in color space.  max_random_contrast::float, optional, default=0 : Augmentation Param: Maximum ratio of contrast variation.  max_random_illumination::float, optional, default=0 : Augmentation Param: Maximum value of illumination variation.   Returns the constructed  MXDataProvider .  source  #  MXNet.mx.ImageRecordUInt8Iter     Method .  ImageRecordUInt8Iter(path_imglist, path_imgrec, aug_seq, label_width, data_shape, preprocess_threads, verbose, num_parts, part_index, shuffle_chunk_size, shuffle_chunk_seed, shuffle, seed, verbose, batch_size, round_batch, prefetch_buffer, dtype, resize, rand_crop, crop_y_start, crop_x_start, max_rotate_angle, max_aspect_ratio, max_shear_ratio, max_crop_size, min_crop_size, max_random_scale, min_random_scale, max_img_size, min_img_size, random_h, random_s, random_l, rotate, fill_value, data_shape, inter_method, pad)  Can also be called with the alias  ImageRecordUInt8Provider . Create iterator for dataset packed in recordio.  Arguments:   data_name::Symbol : keyword argument, default  :data . The name of the data.  label_name::Symbol : keyword argument, default  :softmax_label . The name of the label. Could be  nothing  if no label is presented in this dataset.  path_imglist::string, optional, default='' : Dataset Param: Path to image list.  path_imgrec::string, optional, default='./data/imgrec.rec' : Dataset Param: Path to image record file.  aug_seq::string, optional, default='aug_default' : Augmentation Param: the augmenter names to represent sequence of augmenters to be applied, seperated by comma. Additional keyword parameters will be seen by these augmenters.  label_width::int, optional, default='1' : Dataset Param: How many labels for an image.  data_shape::Shape(tuple), required : Dataset Param: Shape of each instance generated by the DataIter.  preprocess_threads::int, optional, default='4' : Backend Param: Number of thread to do preprocessing.  verbose::boolean, optional, default=True : Auxiliary Param: Whether to output parser information.  num_parts::int, optional, default='1' : partition the data into multiple parts  part_index::int, optional, default='0' : the index of the part will read  shuffle_chunk_size::long (non-negative), optional, default=0 : the size(MB) of the shuffle chunk, used with shuffle=True, it can enable global shuffling  shuffle_chunk_seed::int, optional, default='0' : the seed for chunk shuffling  shuffle::boolean, optional, default=False : Augmentation Param: Whether to shuffle data.  seed::int, optional, default='0' : Augmentation Param: Random Seed.  batch_size::int (non-negative), required : Batch Param: Batch size.  round_batch::boolean, optional, default=True : Batch Param: Use round robin to handle overflow batch.  prefetch_buffer::long (non-negative), optional, default=4 : Backend Param: Number of prefetched parameters  dtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None' : Output data type. Leave as None to useinternal data iterator's output type  resize::int, optional, default='-1' : Augmentation Param: scale shorter edge to size before applying other augmentations.  rand_crop::boolean, optional, default=False : Augmentation Param: Whether to random crop on the image  crop_y_start::int, optional, default='-1' : Augmentation Param: Where to nonrandom crop on y.  crop_x_start::int, optional, default='-1' : Augmentation Param: Where to nonrandom crop on x.  max_rotate_angle::int, optional, default='0' : Augmentation Param: rotated randomly in [-max_rotate_angle, max_rotate_angle].  max_aspect_ratio::float, optional, default=0 : Augmentation Param: denotes the max ratio of random aspect ratio augmentation.  max_shear_ratio::float, optional, default=0 : Augmentation Param: denotes the max random shearing ratio.  max_crop_size::int, optional, default='-1' : Augmentation Param: Maximum crop size.  min_crop_size::int, optional, default='-1' : Augmentation Param: Minimum crop size.  max_random_scale::float, optional, default=1 : Augmentation Param: Maximum scale ratio.  min_random_scale::float, optional, default=1 : Augmentation Param: Minimum scale ratio.  max_img_size::float, optional, default=1e+10 : Augmentation Param: Maximum image size after resizing.  min_img_size::float, optional, default=0 : Augmentation Param: Minimum image size after resizing.  random_h::int, optional, default='0' : Augmentation Param: Maximum random value of H channel in HSL color space.  random_s::int, optional, default='0' : Augmentation Param: Maximum random value of S channel in HSL color space.  random_l::int, optional, default='0' : Augmentation Param: Maximum random value of L channel in HSL color space.  rotate::int, optional, default='-1' : Augmentation Param: Rotate angle.  fill_value::int, optional, default='255' : Augmentation Param: Filled color value while padding.  inter_method::int, optional, default='1' : Augmentation Param: 0-NN 1-bilinear 2-cubic 3-area 4-lanczos4 9-auto 10-rand.  pad::int, optional, default='0' : Augmentation Param: Padding size.   Returns the constructed  MXDataProvider .  source  #  MXNet.mx.MNISTIter     Method .  MNISTIter(image, label, batch_size, shuffle, flat, seed, silent, num_parts, part_index, prefetch_buffer, dtype)  Can also be called with the alias  MNISTProvider . Create iterator for MNIST hand-written digit number recognition dataset.  Arguments:   data_name::Symbol : keyword argument, default  :data . The name of the data.  label_name::Symbol : keyword argument, default  :softmax_label . The name of the label. Could be  nothing  if no label is presented in this dataset.  image::string, optional, default='./train-images-idx3-ubyte' : Dataset Param: Mnist image path.  label::string, optional, default='./train-labels-idx1-ubyte' : Dataset Param: Mnist label path.  batch_size::int, optional, default='128' : Batch Param: Batch Size.  shuffle::boolean, optional, default=True : Augmentation Param: Whether to shuffle data.  flat::boolean, optional, default=False : Augmentation Param: Whether to flat the data into 1D.  seed::int, optional, default='0' : Augmentation Param: Random Seed.  silent::boolean, optional, default=False : Auxiliary Param: Whether to print out data info.  num_parts::int, optional, default='1' : partition the data into multiple parts  part_index::int, optional, default='0' : the index of the part will read  prefetch_buffer::long (non-negative), optional, default=4 : Backend Param: Number of prefetched parameters  dtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None' : Output data type. Leave as None to useinternal data iterator's output type   Returns the constructed  MXDataProvider .  source  #  MXNet.mx.eachbatch     Method .  eachbatch(provider::AbstractDataProvider)  Allows you to perform operations on data every epoch. This is especially useful when you need to perform real-time augmentation of the data.   Arguments:   provider : an instance of the custom DataProvider type. You must return this   instance after modifying its fields.  source  #  MXNet.mx.load_data!     Method .  load_data!(provider, batch, targets)  Arguments:   provider::AbstractDataProvider : the data provider.  batch::AbstractDataBatch : the data batch object.  targets::Vector{Vector{SlicedNDArray}} : the targets to load data into.   The targets is a list of the same length as number of data provided by this provider. Each element in the list is a list of  SlicedNDArray . This list described a spliting scheme of this data batch into different slices, each slice is specified by a slice-ndarray pair, where  slice  specify the range of samples in the mini-batch that should be loaded into the corresponding  ndarray .  This utility function is used in data parallelization, where a mini-batch is splited and computed on several different devices.  source  #  MXNet.mx.load_label!     Method .  load_label!(provider, batch, targets)   provider::AbstractDataProvider provider : the data provider.  batch::AbstractDataBatch batch : the data batch object.  targets::Vector{Vector{SlicedNDArray}} : the targets to load label into.   The same as  load_data! , except that this is for loading labels.  source", 
            "title": "Implemented providers and other methods"
        }, 
        {
            "location": "/api/ndarray/", 
            "text": "NDArray API\n\n\n#\n\n\nBase.Flatten\n \n \nMethod\n.\n\n\nFlatten(data)\n\n\n\n\nFlatten input into 2D by collapsing all the higher dimensions. A (d1, d2, ..., dK) tensor is flatten to (d1, d2\n ... \ndK) matrix.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Input data to reshape.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.NDArray\n \n \nType\n.\n\n\nNDArray\n\n\n\n\nWrapper of the \nNDArray\n type in \nlibmxnet\n. This is the basic building block of tensor-based computation.\n\n\n\n\nNote\n\n\nsince C/C++ use row-major ordering for arrays while Julia follows a   column-major ordering. To keep things consistent, we keep the underlying data   in their original layout, but use \nlanguage-native\n convention when we talk   about shapes. For example, a mini-batch of 100 MNIST images is a tensor of   C/C++/Python shape (100,1,28,28), while in Julia, the same piece of memory   have shape (28,28,1,100).\n\n\n\n\nsource\n\n\n#\n\n\nBase.:*\n \n \nMethod\n.\n\n\n*(arg0, arg1)\n\n\n\n\nCurrently only multiplication a scalar with an \nNDArray\n is implemented. Matrix multiplication is to be added soon.\n\n\nsource\n\n\n#\n\n\nBase.:+\n \n \nMethod\n.\n\n\n+(args...)\n.+(args...)\n\n\n\n\nSummation. Multiple arguments of either scalar or \nNDArray\n could be added together. Note at least the first or second argument needs to be an \nNDArray\n to avoid ambiguity of built-in summation.\n\n\nsource\n\n\n#\n\n\nBase.:-\n \n \nMethod\n.\n\n\n-(arg0, arg1)\n-(arg0)\n.-(arg0, arg1)\n\n\n\n\nSubtraction \narg0 - arg1\n, of scalar types or \nNDArray\n. Or create the negative of \narg0\n.\n\n\nsource\n\n\n#\n\n\nBase.:.*\n \n \nMethod\n.\n\n\n.*(arg0, arg1)\n\n\n\n\nElementwise multiplication of \narg0\n and \narg\n, could be either scalar or \nNDArray\n.\n\n\nsource\n\n\n#\n\n\nBase.:./\n \n \nMethod\n.\n\n\n./(arg0 :: NDArray, arg :: Union{Real, NDArray})\n\n\n\n\nElementwise dividing an \nNDArray\n by a scalar or another \nNDArray\n of the same shape.\n\n\nsource\n\n\n#\n\n\nBase.:/\n \n \nMethod\n.\n\n\n/(arg0 :: NDArray, arg :: Real)\n\n\n\n\nDivide an \nNDArray\n by a scalar. Matrix division (solving linear systems) is not implemented yet.\n\n\nsource\n\n\n#\n\n\nBase.LinAlg.dot\n \n \nMethod\n.\n\n\ndot(lhs, rhs, transpose_a, transpose_b)\n\n\n\n\nCalculate dot product of two matrices or two vectors.\n\n\nFrom:src/operator/tensor/matrix_op.cc:231\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left input\n\n\nrhs::NDArray\n: Right input\n\n\ntranspose_a::boolean, optional, default=False\n: True if the first matrix is transposed.\n\n\ntranspose_b::boolean, optional, default=False\n: True if the second matrix is tranposed.\n\n\n\n\nsource\n\n\n#\n\n\nBase.LinAlg.norm\n \n \nMethod\n.\n\n\nnorm(src)\n\n\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase.Math.gamma\n \n \nMethod\n.\n\n\ngamma(data)\n\n\n\n\nTake the gamma function (extension of the factorial function) of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:306\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase._div\n \n \nMethod\n.\n\n\n_div(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nBase._sub\n \n \nMethod\n.\n\n\n_sub(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nBase.abs\n \n \nMethod\n.\n\n\nabs(data)\n\n\n\n\nTake absolute value of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:63\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase.ceil\n \n \nMethod\n.\n\n\nceil(data)\n\n\n\n\nTake ceil of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:87\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase.convert\n \n \nMethod\n.\n\n\nconvert(::Type{Array{T}}, arr :: NDArray)\n\n\n\n\nConvert an \nNDArray\n into a Julia \nArray\n of specific type. Data will be copied.\n\n\nsource\n\n\n#\n\n\nBase.copy!\n \n \nMethod\n.\n\n\ncopy!(dst :: Union{NDArray, Array}, src :: Union{NDArray, Array})\n\n\n\n\nCopy contents of \nsrc\n into \ndst\n.\n\n\nsource\n\n\n#\n\n\nBase.copy\n \n \nMethod\n.\n\n\ncopy(arr :: NDArray)\ncopy(arr :: NDArray, ctx :: Context)\ncopy(arr :: Array, ctx :: Context)\n\n\n\n\nCreate a copy of an array. When no \nContext\n is given, create a Julia \nArray\n. Otherwise, create an \nNDArray\n on the specified context.\n\n\nsource\n\n\n#\n\n\nBase.cos\n \n \nMethod\n.\n\n\ncos(data)\n\n\n\n\nTake cos of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:189\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase.cosh\n \n \nMethod\n.\n\n\ncosh(data)\n\n\n\n\nTake cosh of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:261\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase.eltype\n \n \nMethod\n.\n\n\neltype(arr :: NDArray)\n\n\n\n\nGet the element type of an \nNDArray\n.\n\n\nsource\n\n\n#\n\n\nBase.exp\n \n \nMethod\n.\n\n\nexp(data)\n\n\n\n\nTake exp of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:135\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase.expm1\n \n \nMethod\n.\n\n\nexpm1(data)\n\n\n\n\nTake \nexp(x) - 1\n in a numerically stable way\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:180\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase.floor\n \n \nMethod\n.\n\n\nfloor(data)\n\n\n\n\nTake floor of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:92\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase.getindex\n \n \nMethod\n.\n\n\ngetindex(arr :: NDArray, idx)\n\n\n\n\nShortcut for \nslice\n. A typical use is to write\n\n\n  arr[:] += 5\n\n\n\n\nwhich translates into\n\n\n  arr[:] = arr[:] + 5\n\n\n\n\nwhich furthur translates into\n\n\n  setindex!(getindex(arr, Colon()), 5, Colon())\n\n\n\n\n\n\nNote\n\n\nThe behavior is quite different from indexing into Julia's \nArray\n. For example, \narr[2:5]\n create a \ncopy\n of the sub-array for Julia \nArray\n, while for \nNDArray\n, this is a \nslice\n that shares the memory.\n\n\n\n\nsource\n\n\n#\n\n\nBase.getindex\n \n \nMethod\n.\n\n\nShortcut for \nslice\n. \nNOTE\n the behavior for Julia's built-in index slicing is to create a copy of the sub-array, while here we simply call \nslice\n, which shares the underlying memory.\n\n\nsource\n\n\n#\n\n\nBase.identity\n \n \nMethod\n.\n\n\nidentity(data)\n\n\n\n\nidentity is an alias of _copy.\n\n\nIdentity mapping, copy src to output\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:14\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase.length\n \n \nMethod\n.\n\n\nlength(arr :: NDArray)\n\n\n\n\nGet the number of elements in an \nNDArray\n.\n\n\nsource\n\n\n#\n\n\nBase.log\n \n \nMethod\n.\n\n\nlog(data)\n\n\n\n\nTake log of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:141\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase.log10\n \n \nMethod\n.\n\n\nlog10(data)\n\n\n\n\nTake base-10 log of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:147\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase.log1p\n \n \nMethod\n.\n\n\nlog1p(data)\n\n\n\n\nTake \nlog(1 + x)\n in a numerically stable way\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:171\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase.log2\n \n \nMethod\n.\n\n\nlog2(data)\n\n\n\n\nTake base-2 log of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:153\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase.max\n \n \nMethod\n.\n\n\nmax(data, axis, keepdims)\n\n\n\n\nCompute max along axis. If axis is empty, global reduction is performed\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:66\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nBase.mean\n \n \nMethod\n.\n\n\nmean(data, axis, keepdims)\n\n\n\n\nCompute mean src along axis. If axis is empty, global reduction is performed\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:26\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nBase.min\n \n \nMethod\n.\n\n\nmin(data, axis, keepdims)\n\n\n\n\nCompute min along axis. If axis is empty, global reduction is performed\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:76\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nBase.ndims\n \n \nMethod\n.\n\n\nndims(arr :: NDArray)\n\n\n\n\nGet the number of dimensions of an \nNDArray\n. Is equivalent to \nlength(size(arr))\n.\n\n\nsource\n\n\n#\n\n\nBase.prod\n \n \nMethod\n.\n\n\nprod(data, axis, keepdims)\n\n\n\n\nCompute product of src along axis. If axis is empty, global reduction is performed\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:36\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nBase.round\n \n \nMethod\n.\n\n\nround(data)\n\n\n\n\nTake round of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:81\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase.setindex!\n \n \nMethod\n.\n\n\nsetindex!(arr :: NDArray, val, idx)\n\n\n\n\nAssign values to an \nNDArray\n. Elementwise assignment is not implemented, only the following scenarios are supported\n\n\n\n\narr[:] = val\n: whole array assignment, \nval\n could be a scalar or an array (Julia \nArray\n or \nNDArray\n) of the same shape.\n\n\narr[start:stop] = val\n: assignment to a \nslice\n, \nval\n could be a scalar or an array of the same shape to the slice. See also \nslice\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.sign\n \n \nMethod\n.\n\n\nsign(data)\n\n\n\n\nTake sign of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:72\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase.similar\n \n \nMethod\n.\n\n\nsimilar(arr :: NDArray)\n\n\n\n\nCreate an \nNDArray\n with similar shape, data type, and context with the given one.\n\n\nsource\n\n\n#\n\n\nBase.sin\n \n \nMethod\n.\n\n\nsin(data)\n\n\n\n\nTake sin of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:162\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase.sinh\n \n \nMethod\n.\n\n\nsinh(data)\n\n\n\n\nTake sinh of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:252\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase.size\n \n \nMethod\n.\n\n\nsize(arr :: NDArray)\nsize(arr :: NDArray, dim :: Int)\n\n\n\n\nGet the shape of an \nNDArray\n. The shape is in Julia's column-major convention. See also the notes on NDArray shapes \nNDArray\n.\n\n\nsource\n\n\n#\n\n\nBase.slice\n \n \nMethod\n.\n\n\nslice(arr :: NDArray, start:stop)\n\n\n\n\nCreate a view into a sub-slice of an \nNDArray\n. Note only slicing at the slowest changing dimension is supported. In Julia's column-major perspective, this is the last dimension. For example, given an \nNDArray\n of shape (2,3,4), \nslice(array, 2:3)\n will create a \nNDArray\n of shape (2,3,2), sharing the data with the original array. This operation is used in data parallelization to split mini-batch into sub-batches for different devices.\n\n\nsource\n\n\n#\n\n\nBase.sort\n \n \nMethod\n.\n\n\nsort(src, axis, is_ascend)\n\n\n\n\nReturn a sorted copy of an array.\n\n\nFrom:src/operator/tensor/ordering_op.cc:59\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input\n\n\naxis::int or None, optional, default='-1'\n: Axis along which to choose sort the input tensor. If not given, the flattened array is used. Default is -1.\n\n\nis_ascend::boolean, optional, default=True\n: Whether sort in ascending or descending order.\n\n\n\n\nsource\n\n\n#\n\n\nBase.sqrt\n \n \nMethod\n.\n\n\nsqrt(data)\n\n\n\n\nTake square root of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:116\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase.sum\n \n \nMethod\n.\n\n\nsum(data, axis, keepdims)\n\n\n\n\nSum src along axis. If axis is empty, global reduction is performed\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:17\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nBase.take\n \n \nMethod\n.\n\n\ntake(a, indices, axis, mode)\n\n\n\n\nTake row vectors from an NDArray according to the indices For an input of index with shape (d1, ..., dK), the output shape is (d1, ..., dK, row_vector_length).All the input values should be integers in the range [0, column_vector_length).\n\n\nFrom:src/operator/tensor/indexing_op.cc:59\n\n\nArguments\n\n\n\n\na::SymbolicNode\n: The source array.\n\n\nindices::SymbolicNode\n: The indices of the values to extract.\n\n\naxis::int, optional, default='0'\n: the axis of data tensor to be taken.\n\n\nmode::{'clip', 'raise', 'wrap'},optional, default='raise'\n: specify how out-of-bound indices bahave.\n\n\n\n\nsource\n\n\n#\n\n\nBase.tan\n \n \nMethod\n.\n\n\ntan(data)\n\n\n\n\nTake tan of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:198\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase.tanh\n \n \nMethod\n.\n\n\ntanh(data)\n\n\n\n\nTake tanh of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:270\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase.transpose\n \n \nMethod\n.\n\n\ntranspose(data, axes)\n\n\n\n\nTranspose the input tensor and return a new one\n\n\nFrom:src/operator/tensor/matrix_op.cc:94\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxes::Shape(tuple), optional, default=()\n: Target axis order. By default the axes will be inverted.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Activation\n \n \nMethod\n.\n\n\nActivation(data, act_type)\n\n\n\n\nElementwise activation function.\n\n\nThe following activation types are supported (operations are applied elementwisely to each scalar of the input tensor):\n\n\n\n\nrelu\n: Rectified Linear Unit, \ny = max(x, 0)\n\n\nsigmoid\n: \ny = 1 / (1 + exp(-x))\n\n\ntanh\n: Hyperbolic tangent, \ny = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n\n\nsoftrelu\n: Soft ReLU, or SoftPlus, \ny = log(1 + exp(x))\n\n\n\n\nSee \nLeakyReLU\n for other activations with parameters.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to activation function.\n\n\nact_type::{'relu', 'sigmoid', 'softrelu', 'tanh'}, required\n: Activation function to be applied.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.BatchNorm\n \n \nMethod\n.\n\n\nBatchNorm(data, gamma, beta, eps, momentum, fix_gamma, use_global_stats, output_mean_var)\n\n\n\n\nApply batch normalization to input.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to batch normalization\n\n\ngamma::SymbolicNode\n: gamma matrix\n\n\nbeta::SymbolicNode\n: beta matrix\n\n\neps::float, optional, default=0.001\n: Epsilon to prevent div 0\n\n\nmomentum::float, optional, default=0.9\n: Momentum for moving average\n\n\nfix_gamma::boolean, optional, default=True\n: Fix gamma while training\n\n\nuse_global_stats::boolean, optional, default=False\n: Whether use global moving statistics instead of local batch-norm. This will force change batch-norm into a scale shift operator.\n\n\noutput_mean_var::boolean, optional, default=False\n: Output All,normal mean and var\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.BlockGrad\n \n \nMethod\n.\n\n\nBlockGrad(data)\n\n\n\n\nGet output from a symbol and pass 0 gradient back\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:30\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Cast\n \n \nMethod\n.\n\n\nCast(data, dtype)\n\n\n\n\nCast array to a different data type.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to cast function.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'}, required\n: Target data type.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Concat\n \n \nMethod\n.\n\n\nConcat(data, num_args, dim)\n\n\n\n\nNote\n: Concat takes variable number of positional inputs. So instead of calling as Concat([x, y, z], num_args=3), one should call via Concat(x, y, z), and num_args will be determined automatically.\n\n\nPerform a feature concat on channel dim (defaut is 1) over all\n\n\nArguments\n\n\n\n\ndata::SymbolicNode[]\n: List of tensors to concatenate\n\n\nnum_args::int, required\n: Number of inputs to be concated.\n\n\ndim::int, optional, default='1'\n: the dimension to be concated.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Convolution\n \n \nMethod\n.\n\n\nConvolution(data, weight, bias, kernel, stride, dilate, pad, num_filter, num_group, workspace, no_bias, cudnn_tune, cudnn_off, layout)\n\n\n\n\nApply convolution to input then add a bias.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the ConvolutionOp.\n\n\nweight::SymbolicNode\n: Weight matrix.\n\n\nbias::SymbolicNode\n: Bias parameter.\n\n\nkernel::Shape(tuple), required\n: convolution kernel size: (h, w) or (d, h, w)\n\n\nstride::Shape(tuple), optional, default=()\n: convolution stride: (h, w) or (d, h, w)\n\n\ndilate::Shape(tuple), optional, default=()\n: convolution dilate: (h, w) or (d, h, w)\n\n\npad::Shape(tuple), optional, default=()\n: pad for convolution: (h, w) or (d, h, w)\n\n\nnum_filter::int (non-negative), required\n: convolution filter(channel) number\n\n\nnum_group::int (non-negative), optional, default=1\n: Number of group partitions. Equivalent to slicing input into num_group   partitions, apply convolution on each, then concatenate the results\n\n\nworkspace::long (non-negative), optional, default=1024\n: Maximum tmp workspace allowed for convolution (MB).\n\n\nno_bias::boolean, optional, default=False\n: Whether to disable bias parameter.\n\n\ncudnn_tune::{None, 'fastest', 'limited_workspace', 'off'},optional, default='None'\n: Whether to pick convolution algo by running performance test.   Leads to higher startup time but may give faster speed. Options are:   'off': no tuning   'limited_workspace': run test and pick the fastest algorithm that doesn't exceed workspace limit.   'fastest': pick the fastest algorithm and ignore workspace limit.   If set to None (default), behavior is determined by environment   variable MXNET_CUDNN_AUTOTUNE_DEFAULT: 0 for off,   1 for limited workspace (default), 2 for fastest.\n\n\ncudnn_off::boolean, optional, default=False\n: Turn off cudnn for this layer.\n\n\nlayout::{None, 'NCDHW', 'NCHW', 'NDHWC', 'NHWC'},optional, default='None'\n: Set layout for input, output and weight. Empty for   default layout: NCHW for 2d and NCDHW for 3d.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Correlation\n \n \nMethod\n.\n\n\nCorrelation(data1, data2, kernel_size, max_displacement, stride1, stride2, pad_size, is_multiply)\n\n\n\n\nApply correlation to inputs\n\n\nArguments\n\n\n\n\ndata1::SymbolicNode\n: Input data1 to the correlation.\n\n\ndata2::SymbolicNode\n: Input data2 to the correlation.\n\n\nkernel_size::int (non-negative), optional, default=1\n: kernel size for Correlation must be an odd number\n\n\nmax_displacement::int (non-negative), optional, default=1\n: Max displacement of Correlation\n\n\nstride1::int (non-negative), optional, default=1\n: stride1 quantize data1 globally\n\n\nstride2::int (non-negative), optional, default=1\n: stride2 quantize data2 within the neighborhood centered around data1\n\n\npad_size::int (non-negative), optional, default=0\n: pad for Correlation\n\n\nis_multiply::boolean, optional, default=True\n: operation type is either multiplication or subduction\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Crop\n \n \nMethod\n.\n\n\nCrop(data, num_args, offset, h_w, center_crop)\n\n\n\n\nNote\n: Crop takes variable number of positional inputs. So instead of calling as Crop([x, y, z], num_args=3), one should call via Crop(x, y, z), and num_args will be determined automatically.\n\n\nCrop the 2nd and 3rd dim of input data, with the corresponding size of h_w or with width and height of the second input symbol, i.e., with one input, we need h_w to specify the crop height and width, otherwise the second input symbol's size will be used\n\n\nArguments\n\n\n\n\ndata::SymbolicNode or SymbolicNode[]\n: Tensor or List of Tensors, the second input will be used as crop_like shape reference\n\n\nnum_args::int, required\n: Number of inputs for crop, if equals one, then we will use the h_wfor crop height and width, else if equals two, then we will use the heightand width of the second input symbol, we name crop_like here\n\n\noffset::Shape(tuple), optional, default=(0,0)\n: crop offset coordinate: (y, x)\n\n\nh_w::Shape(tuple), optional, default=(0,0)\n: crop height and weight: (h, w)\n\n\ncenter_crop::boolean, optional, default=False\n: If set to true, then it will use be the center_crop,or it will crop using the shape of crop_like\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Custom\n \n \nMethod\n.\n\n\nCustom(op_type)\n\n\n\n\nCustom operator implemented in frontend.\n\n\nArguments\n\n\n\n\nop_type::string\n: Type of custom operator. Must be registered first.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Deconvolution\n \n \nMethod\n.\n\n\nDeconvolution(data, weight, bias, kernel, stride, pad, adj, target_shape, num_filter, num_group, workspace, no_bias)\n\n\n\n\nApply deconvolution to input then add a bias.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the DeconvolutionOp.\n\n\nweight::SymbolicNode\n: Weight matrix.\n\n\nbias::SymbolicNode\n: Bias parameter.\n\n\nkernel::Shape(tuple), required\n: deconvolution kernel size: (y, x)\n\n\nstride::Shape(tuple), optional, default=(1,1)\n: deconvolution stride: (y, x)\n\n\npad::Shape(tuple), optional, default=(0,0)\n: pad for deconvolution: (y, x), a good number is : (kernel-1)/2, if target_shape set, pad will be ignored and will be computed automatically\n\n\nadj::Shape(tuple), optional, default=(0,0)\n: adjustment for output shape: (y, x), if target_shape set, adj will be ignored and will be computed automatically\n\n\ntarget_shape::Shape(tuple), optional, default=(0,0)\n: output shape with targe shape : (y, x)\n\n\nnum_filter::int (non-negative), required\n: deconvolution filter(channel) number\n\n\nnum_group::int (non-negative), optional, default=1\n: number of groups partition\n\n\nworkspace::long (non-negative), optional, default=512\n: Tmp workspace for deconvolution (MB)\n\n\nno_bias::boolean, optional, default=True\n: Whether to disable bias parameter.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Dropout\n \n \nMethod\n.\n\n\nDropout(data, p)\n\n\n\n\nApply dropout to input. During training, each element of the input is randomly set to zero with probability p. And then the whole tensor is rescaled by 1/(1-p) to keep the expectation the same as before applying dropout. During the test time, this behaves as an identity map.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to dropout.\n\n\np::float, optional, default=0.5\n: Fraction of the input that gets dropped out at training time\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ElementWiseSum\n \n \nMethod\n.\n\n\nElementWiseSum(args)\n\n\n\n\nNote\n: ElementWiseSum takes variable number of positional inputs. So instead of calling as ElementWiseSum([x, y, z], num_args=3), one should call via ElementWiseSum(x, y, z), and num_args will be determined automatically.\n\n\nPerform element sum of inputs\n\n\nFrom:src/operator/tensor/elemwise_sum.cc:56\n\n\nArguments\n\n\n\n\nargs::NDArray[]\n: List of input tensors\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Embedding\n \n \nMethod\n.\n\n\nEmbedding(data, weight, input_dim, output_dim)\n\n\n\n\nMap integer index to vector representations (embeddings). Those embeddings are learnable parameters. For a input of shape (d1, ..., dK), the output shape is (d1, ..., dK, output_dim). All the input values should be integers in the range [0, input_dim).\n\n\nFrom:src/operator/tensor/indexing_op.cc:18\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the EmbeddingOp.\n\n\nweight::SymbolicNode\n: Embedding weight matrix.\n\n\ninput_dim::int, required\n: vocabulary size of the input indices.\n\n\noutput_dim::int, required\n: dimension of the embedding vectors.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.FullyConnected\n \n \nMethod\n.\n\n\nFullyConnected(data, weight, bias, num_hidden, no_bias)\n\n\n\n\nApply matrix multiplication to input then add a bias. It maps the input of shape \n(batch_size, input_dim)\n to the shape of \n(batch_size, num_hidden)\n. Learnable parameters include the weights of the linear transform and an optional bias vector.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the FullyConnectedOp.\n\n\nweight::SymbolicNode\n: Weight matrix.\n\n\nbias::SymbolicNode\n: Bias parameter.\n\n\nnum_hidden::int, required\n: Number of hidden nodes of the output.\n\n\nno_bias::boolean, optional, default=False\n: Whether to disable bias parameter.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.IdentityAttachKLSparseReg\n \n \nMethod\n.\n\n\nIdentityAttachKLSparseReg(data, sparseness_target, penalty, momentum)\n\n\n\n\nApply a sparse regularization to the output a sigmoid activation function.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data.\n\n\nsparseness_target::float, optional, default=0.1\n: The sparseness target\n\n\npenalty::float, optional, default=0.001\n: The tradeoff parameter for the sparseness penalty\n\n\nmomentum::float, optional, default=0.9\n: The momentum for running average\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.InstanceNorm\n \n \nMethod\n.\n\n\nInstanceNorm(data, gamma, beta, eps)\n\n\n\n\nAn operator taking in a n-dimensional input tensor (n \n 2), and normalizing the input by subtracting the mean and variance calculated over the spatial dimensions. This is an implemention of the operator described in \"Instance Normalization: The Missing Ingredient for Fast Stylization\", D. Ulyanov, A. Vedaldi, V. Lempitsky, 2016 (arXiv:1607.08022v2). This layer is similar to batch normalization, with two differences: first, the normalization is carried out per example ('instance'), not over a batch. Second, the same normalization is applied both at test and train time. This operation is also known as 'contrast normalization'.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: A n-dimensional tensor (n \n 2) of the form [batch, channel, spatial_dim1, spatial_dim2, ...].\n\n\ngamma::SymbolicNode\n: A vector of length 'channel', which multiplies the normalized input.\n\n\nbeta::SymbolicNode\n: A vector of length 'channel', which is added to the product of the normalized input and the weight.\n\n\neps::float, optional, default=0.001\n: Epsilon to prevent division by 0.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.L2Normalization\n \n \nMethod\n.\n\n\nL2Normalization(data, eps, mode)\n\n\n\n\nSet the l2 norm of each instance to a constant.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the L2NormalizationOp.\n\n\neps::float, optional, default=1e-10\n: Epsilon to prevent div 0\n\n\nmode::{'channel', 'instance', 'spatial'},optional, default='instance'\n: Normalization Mode. If set to instance, this operator will compute a norm for each instance in the batch; this is the default mode. If set to channel, this operator will compute a cross channel norm at each position of each instance. If set to spatial, this operator will compute a norm for each channel.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LRN\n \n \nMethod\n.\n\n\nLRN(data, alpha, beta, knorm, nsize)\n\n\n\n\nApply convolution to input then add a bias.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the ConvolutionOp.\n\n\nalpha::float, optional, default=0.0001\n: value of the alpha variance scaling parameter in the normalization formula\n\n\nbeta::float, optional, default=0.75\n: value of the beta power parameter in the normalization formula\n\n\nknorm::float, optional, default=2\n: value of the k parameter in normalization formula\n\n\nnsize::int (non-negative), required\n: normalization window width in elements.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LeakyReLU\n \n \nMethod\n.\n\n\nLeakyReLU(data, act_type, slope, lower_bound, upper_bound)\n\n\n\n\nApply activation function to input.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to activation function.\n\n\nact_type::{'elu', 'leaky', 'prelu', 'rrelu'},optional, default='leaky'\n: Activation function to be applied.\n\n\nslope::float, optional, default=0.25\n: Init slope for the activation. (For leaky and elu only)\n\n\nlower_bound::float, optional, default=0.125\n: Lower bound of random slope. (For rrelu only)\n\n\nupper_bound::float, optional, default=0.334\n: Upper bound of random slope. (For rrelu only)\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LinearRegressionOutput\n \n \nMethod\n.\n\n\nLinearRegressionOutput(data, label, grad_scale)\n\n\n\n\nUse linear regression for final output, this is used on final output of a net.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to function.\n\n\nlabel::SymbolicNode\n: Input label to function.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LogisticRegressionOutput\n \n \nMethod\n.\n\n\nLogisticRegressionOutput(data, label, grad_scale)\n\n\n\n\nUse Logistic regression for final output, this is used on final output of a net. Logistic regression is suitable for binary classification or probability prediction tasks.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to function.\n\n\nlabel::SymbolicNode\n: Input label to function.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MAERegressionOutput\n \n \nMethod\n.\n\n\nMAERegressionOutput(data, label, grad_scale)\n\n\n\n\nUse mean absolute error regression for final output, this is used on final output of a net.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to function.\n\n\nlabel::SymbolicNode\n: Input label to function.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MakeLoss\n \n \nMethod\n.\n\n\nMakeLoss(data, grad_scale, valid_thresh, normalization)\n\n\n\n\nGet output from a symbol and pass 1 gradient back. This is used as a terminal loss if unary and binary operator are used to composite a loss with no declaration of backward dependency\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data.\n\n\ngrad_scale::float, optional, default=1\n: gradient scale as a supplement to unary and binary operators\n\n\nvalid_thresh::float, optional, default=0\n: regard element valid when x \n valid_thresh, this is used only in valid normalization mode.\n\n\nnormalization::{'batch', 'null', 'valid'},optional, default='null'\n: If set to null, op will not normalize on output gradient.If set to batch, op will normalize gradient by divide batch size.If set to valid, op will normalize gradient by divide # sample marked as valid\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Pad\n \n \nMethod\n.\n\n\nPad(data, mode, pad_width, constant_value)\n\n\n\n\nPads an n-dimensional input tensor. Allows for precise control of the padding type and how much padding to apply on both sides of a given dimension.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: An n-dimensional input tensor.\n\n\nmode::{'constant', 'edge'}, required\n: Padding type to use. \"constant\" pads all values with a constant value, the value of which can be specified with the constant_value option. \"edge\" uses the boundary values of the array as padding.\n\n\npad_width::Shape(tuple), required\n: A tuple of padding widths of length 2*r, where r is the rank of the input tensor, specifying number of values padded to the edges of each axis. (before_1, after_1, ... , before_N, after_N) unique pad widths for each axis. Equivalent to pad_width in numpy.pad, but flattened.\n\n\nconstant_value::double, optional, default=0\n: This option is only used when mode is \"constant\". This value will be used as the padding value. Defaults to 0 if not specified.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Pooling\n \n \nMethod\n.\n\n\nPooling(data, global_pool, kernel, pool_type, pooling_convention, stride, pad)\n\n\n\n\nPerform spatial pooling on inputs.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the pooling operator.\n\n\nglobal_pool::boolean, optional, default=False\n: Ignore kernel size, do global pooling based on current input feature map. This is useful for input with different shape\n\n\nkernel::Shape(tuple), required\n: pooling kernel size: (y, x) or (d, y, x)\n\n\npool_type::{'avg', 'max', 'sum'}, required\n: Pooling type to be applied.\n\n\npooling_convention::{'full', 'valid'},optional, default='valid'\n: Pooling convention to be applied.kValid is default setting of Mxnet and rounds down the output pooling size.kFull is compatible with Caffe and rounds up the output pooling size.\n\n\nstride::Shape(tuple), optional, default=(1,1)\n: stride: for pooling (y, x) or (d, y, x)\n\n\npad::Shape(tuple), optional, default=(0,0)\n: pad for pooling: (y, x) or (d, y, x)\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.RNN\n \n \nMethod\n.\n\n\nRNN(data, parameters, state, state_cell, state_size, num_layers, bidirectional, mode, p, state_outputs)\n\n\n\n\nApply a recurrent layer to input.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to RNN\n\n\nparameters::SymbolicNode\n: Vector of all RNN trainable parameters concatenated\n\n\nstate::SymbolicNode\n: initial hidden state of the RNN\n\n\nstate_cell::SymbolicNode\n: initial cell state for LSTM networks (only for LSTM)\n\n\nstate_size::int (non-negative), required\n: size of the state for each layer\n\n\nnum_layers::int (non-negative), required\n: number of stacked layers\n\n\nbidirectional::boolean, optional, default=False\n: whether to use bidirectional recurrent layers\n\n\nmode::{'gru', 'lstm', 'rnn_relu', 'rnn_tanh'}, required\n: the type of RNN to compute\n\n\np::float, optional, default=0\n: Dropout probability, fraction of the input that gets dropped out at training time\n\n\nstate_outputs::boolean, optional, default=False\n: Whether to have the states as symbol outputs.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ROIPooling\n \n \nMethod\n.\n\n\nROIPooling(data, rois, pooled_size, spatial_scale)\n\n\n\n\nPerforms region-of-interest pooling on inputs. Resize bounding box coordinates by spatial_scale and crop input feature maps accordingly. The cropped feature maps are pooled by max pooling to a fixed size output indicated by pooled_size. batch_size will change to the number of region bounding boxes after ROIPooling\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the pooling operator, a 4D Feature maps\n\n\nrois::SymbolicNode\n: Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]]. (x1, y1) and (x2, y2) are top left and down right corners of designated region of interest. batch_index indicates the index of corresponding image in the input data\n\n\npooled_size::Shape(tuple), required\n: fix pooled size: (h, w)\n\n\nspatial_scale::float, required\n: Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Reshape\n \n \nMethod\n.\n\n\nReshape(data, target_shape, keep_highest, shape, reverse)\n\n\n\n\nReshape input according to a target shape spec. The target shape is a tuple and can be a simple list of dimensions such as (12,3) or it can incorporate special codes that correspond to contextual operations that refer to the input dimensions. The special codes are all expressed as integers less than 1. These codes effectively refer to a machine that pops input dims off the beginning of the input dims list and pushes resulting output dims onto the end of the output dims list, which starts empty. The codes are:   0  Copy     Pop one input dim and push it onto the output dims  -1  Infer    Push a dim that is inferred later from all other output dims  -2  CopyAll  Pop all remaining input dims and push them onto output dims  -3  Merge2   Pop two input dims, multiply them, and push result  -4  Split2   Pop one input dim, and read two next target shape specs,               push them both onto output dims (either can be -1 and will               be inferred from the other  The exact mathematical behavior of these codes is given in the description of the 'shape' parameter. All non-codes (positive integers) just pop a dim off the input dims (if any), throw it away, and then push the specified integer onto the output dims. Examples: Type     Input      Target            Output Copy     (2,3,4)    (4,0,2)           (4,3,2) Copy     (2,3,4)    (2,0,0)           (2,3,4) Infer    (2,3,4)    (6,1,-1)          (6,1,4) Infer    (2,3,4)    (3,-1,8)          (3,1,8) CopyAll  (9,8,7)    (-2)              (9,8,7) CopyAll  (9,8,7)    (9,-2)            (9,8,7) CopyAll  (9,8,7)    (-2,1,1)          (9,8,7,1,1) Merge2   (3,4)      (-3)              (12) Merge2   (3,4,5)    (-3,0)            (12,5) Merge2   (3,4,5)    (0,-3)            (3,20) Merge2   (3,4,5,6)  (-3,0,0)          (12,5,6) Merge2   (3,4,5,6)  (-3,-2)           (12,5,6) Split2   (12)       (-4,6,2)          (6,2) Split2   (12)       (-4,2,6)          (2,6) Split2   (12)       (-4,-1,6)         (2,6) Split2   (12,9)     (-4,2,6,0)        (2,6,9) Split2   (12,9,9,9) (-4,2,6,-2)       (2,6,9,9,9) Split2   (12,12)    (-4,2,-1,-4,-1,2) (2,6,6,2)\n\n\nFrom:src/operator/tensor/matrix_op.cc:62\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Input data to reshape.\n\n\ntarget_shape::Shape(tuple), optional, default=(0,0)\n: (Deprecated! Use shape instead.) Target new shape. One and only one dim can be 0, in which case it will be inferred from the rest of dims\n\n\nkeep_highest::boolean, optional, default=False\n: (Deprecated! Use shape instead.) Whether keep the highest dim unchanged.If set to true, then the first dim in target_shape is ignored,and always fixed as input\n\n\nshape::Shape(tuple), optional, default=()\n: Target shape, a tuple, t=(t_1,t_2,..,t_m).\n\n\n\n\nLet the input dims be s=(s_1,s_2,..,s_n). The output dims u=(u_1,u_2,..,u_p) are computed from s and t. The target shape tuple elements t_i are read in order, and used to  generate successive output dims u_p: t_i:       meaning:      behavior: +ve        explicit      u_p = t_i 0          copy          u_p = s_i -1         infer         u_p = (Prod s_i) / (Prod u_j | j != p) -2         copy all      u_p = s_i, u_p+1 = s_i+1, ... -3         merge two     u_p = s_i * s_i+1 -4,a,b     split two     u_p = a, u_p+1 = b | a * b = s_i The split directive (-4) in the target shape tuple is followed by two dimensions, one of which can be -1, which means it will be inferred from the other one and the original dimension. The can only be one globally inferred dimension (-1), aside from any -1 occuring in a split directive.\n\n\n\n\nreverse::boolean, optional, default=False\n: Whether to match the shapes from the backward. If reverse is true, 0 values in the \nshape\n argument will be searched from the backward. E.g the original shape is (10, 5, 4) and the shape argument is (-1, 0). If reverse is true, the new shape should be (50, 4). Otherwise it will be (40, 5).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SVMOutput\n \n \nMethod\n.\n\n\nSVMOutput(data, label, margin, regularization_coefficient, use_linear)\n\n\n\n\nSupport Vector Machine based transformation on input, backprop L2-SVM\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to svm.\n\n\nlabel::SymbolicNode\n: Label data.\n\n\nmargin::float, optional, default=1\n: Scale the DType(param_.margin) for activation size\n\n\nregularization_coefficient::float, optional, default=1\n: Scale the coefficient responsible for balacing coefficient size and error tradeoff\n\n\nuse_linear::boolean, optional, default=False\n: If set true, uses L1-SVM objective function. Default uses L2-SVM objective\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SequenceLast\n \n \nMethod\n.\n\n\nSequenceLast(data, sequence_length, use_sequence_length)\n\n\n\n\nTakes the last element of a sequence. Takes an n-dimensional tensor of the form [max sequence length, batchsize, other dims] and returns a (n-1)-dimensional tensor of the form [batchsize, other dims]. This operator takes an optional input tensor sequence_length of positive ints of dimension [batchsize] when the sequence_length option is set to true. This allows the operator to handle variable-length sequences. If sequence_length is false, then each example in the batch is assumed to have the max sequence length.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: n-dimensional input tensor of the form [max sequence length, batchsize, other dims]\n\n\nsequence_length::SymbolicNode\n: vector of sequence lengths of size batchsize\n\n\nuse_sequence_length::boolean, optional, default=False\n: If set to true, this layer takes in extra input sequence_length to specify variable length sequence\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SequenceMask\n \n \nMethod\n.\n\n\nSequenceMask(data, sequence_length, use_sequence_length, value)\n\n\n\n\nSets all elements outside the sequence to a constant value. Takes an n-dimensional tensor of the form [max sequence length, batchsize, other dims] and returns a tensor of the same shape. This operator takes an optional input tensor sequence_length of positive ints of dimension [batchsize] when the sequence_length option is set to true. This allows the operator to handle variable-length sequences. If sequence_length is false, then each example in the batch is assumed to have the max sequence length, and this operator becomes the identity operator.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: n-dimensional input tensor of the form [max sequence length, batchsize, other dims]\n\n\nsequence_length::SymbolicNode\n: vector of sequence lengths of size batchsize\n\n\nuse_sequence_length::boolean, optional, default=False\n: If set to true, this layer takes in extra input sequence_length to specify variable length sequence\n\n\nvalue::float, optional, default=0\n: The value to be used as a mask.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SequenceReverse\n \n \nMethod\n.\n\n\nSequenceReverse(data, sequence_length, use_sequence_length)\n\n\n\n\nReverses the elements of each sequence. Takes an n-dimensional tensor of the form [max sequence length, batchsize, other dims] and returns a tensor of the same shape. This operator takes an optional input tensor sequence_length of positive ints of dimension [batchsize] when the sequence_length option is set to true. This allows the operator to handle variable-length sequences. If sequence_length is false, then each example in the batch is assumed to have the max sequence length.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: n-dimensional input tensor of the form [max sequence length, batchsize, other dims]\n\n\nsequence_length::SymbolicNode\n: vector of sequence lengths of size batchsize\n\n\nuse_sequence_length::boolean, optional, default=False\n: If set to true, this layer takes in extra input sequence_length to specify variable length sequence\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SliceChannel\n \n \nMethod\n.\n\n\nSliceChannel(num_outputs, axis, squeeze_axis)\n\n\n\n\nSlice input equally along specified axis\n\n\nArguments\n\n\n\n\nnum_outputs::int, required\n: Number of outputs to be sliced.\n\n\naxis::int, optional, default='1'\n: Dimension along which to slice.\n\n\nsqueeze_axis::boolean, optional, default=False\n: If true AND the sliced dimension becomes 1, squeeze that dimension.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Softmax\n \n \nMethod\n.\n\n\nSoftmax(data, grad_scale, ignore_label, multi_output, use_ignore, preserve_shape, normalization, out_grad)\n\n\n\n\nDEPRECATED: Perform a softmax transformation on input. Please use SoftmaxOutput\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to softmax.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\nignore_label::float, optional, default=-1\n: the label value will be ignored during backward (only works if use_ignore is set to be true).\n\n\nmulti_output::boolean, optional, default=False\n: If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n\nx_1\n...*x_n output, each has k classes\n\n\nuse_ignore::boolean, optional, default=False\n: If set to true, the ignore_label value will not contribute to the backward gradient\n\n\npreserve_shape::boolean, optional, default=False\n: If true, for a (n_1, n_2, ..., n_d, k) dimensional input tensor, softmax will generate (n1, n2, ..., n_d, k) output, normalizing the k classes as the last dimension.\n\n\nnormalization::{'batch', 'null', 'valid'},optional, default='null'\n: If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored\n\n\nout_grad::boolean, optional, default=False\n: Apply weighting from output gradient\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SoftmaxActivation\n \n \nMethod\n.\n\n\nSoftmaxActivation(data, mode)\n\n\n\n\nApply softmax activation to input. This is intended for internal layers. For output (loss layer) please use SoftmaxOutput. If mode=instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If mode=channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to activation function.\n\n\nmode::{'channel', 'instance'},optional, default='instance'\n: Softmax Mode. If set to instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If set to channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SoftmaxOutput\n \n \nMethod\n.\n\n\nSoftmaxOutput(data, label, grad_scale, ignore_label, multi_output, use_ignore, preserve_shape, normalization, out_grad)\n\n\n\n\nPerform a softmax transformation on input, backprop with logloss.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to softmax.\n\n\nlabel::SymbolicNode\n: Label data, can also be probability value with same shape as data\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\nignore_label::float, optional, default=-1\n: the label value will be ignored during backward (only works if use_ignore is set to be true).\n\n\nmulti_output::boolean, optional, default=False\n: If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n\nx_1\n...*x_n output, each has k classes\n\n\nuse_ignore::boolean, optional, default=False\n: If set to true, the ignore_label value will not contribute to the backward gradient\n\n\npreserve_shape::boolean, optional, default=False\n: If true, for a (n_1, n_2, ..., n_d, k) dimensional input tensor, softmax will generate (n1, n2, ..., n_d, k) output, normalizing the k classes as the last dimension.\n\n\nnormalization::{'batch', 'null', 'valid'},optional, default='null'\n: If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored\n\n\nout_grad::boolean, optional, default=False\n: Apply weighting from output gradient\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SpatialTransformer\n \n \nMethod\n.\n\n\nSpatialTransformer(data, loc, target_shape, transform_type, sampler_type)\n\n\n\n\nApply spatial transformer to input feature map.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the SpatialTransformerOp.\n\n\nloc::SymbolicNode\n: localisation net, the output dim should be 6 when transform_type is affine, and the name of loc symbol should better starts with 'stn_loc', so that initialization it with iddentify tranform, or you shold initialize the weight and bias by yourself.\n\n\ntarget_shape::Shape(tuple), optional, default=(0,0)\n: output shape(h, w) of spatial transformer: (y, x)\n\n\ntransform_type::{'affine'}, required\n: transformation type\n\n\nsampler_type::{'bilinear'}, required\n: sampling type\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SwapAxis\n \n \nMethod\n.\n\n\nSwapAxis(data, dim1, dim2)\n\n\n\n\nApply swapaxis to input.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the SwapAxisOp.\n\n\ndim1::int (non-negative), optional, default=0\n: the first axis to be swapped.\n\n\ndim2::int (non-negative), optional, default=0\n: the second axis to be swapped.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.UpSampling\n \n \nMethod\n.\n\n\nUpSampling(data, scale, num_filter, sample_type, multi_input_mode, num_args, workspace)\n\n\n\n\nNote\n: UpSampling takes variable number of positional inputs. So instead of calling as UpSampling([x, y, z], num_args=3), one should call via UpSampling(x, y, z), and num_args will be determined automatically.\n\n\nPerform nearest neighboor/bilinear up sampling to inputs\n\n\nArguments\n\n\n\n\ndata::SymbolicNode[]\n: Array of tensors to upsample\n\n\nscale::int (non-negative), required\n: Up sampling scale\n\n\nnum_filter::int (non-negative), optional, default=0\n: Input filter. Only used by bilinear sample_type.\n\n\nsample_type::{'bilinear', 'nearest'}, required\n: upsampling method\n\n\nmulti_input_mode::{'concat', 'sum'},optional, default='concat'\n: How to handle multiple input. concat means concatenate upsampled images along the channel dimension. sum means add all images together, only available for nearest neighbor upsampling.\n\n\nnum_args::int, required\n: Number of inputs to be upsampled. For nearest neighbor upsampling, this can be 1-N; the size of output will be(scale\nh_0,scale\nw_0) and all other inputs will be upsampled to thesame size. For bilinear upsampling this must be 2; 1 input and 1 weight.\n\n\nworkspace::long (non-negative), optional, default=512\n: Tmp workspace for deconvolution (MB)\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._CrossDeviceCopy\n \n \nMethod\n.\n\n\n_CrossDeviceCopy()\n\n\n\n\nSpecial op to copy data cross device\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Div\n \n \nMethod\n.\n\n\n_Div(lhs, rhs)\n\n\n\n\n_Div is an alias of _div.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._DivScalar\n \n \nMethod\n.\n\n\n_DivScalar(data, scalar)\n\n\n\n\n_DivScalar is an alias of _div_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Equal\n \n \nMethod\n.\n\n\n_Equal(lhs, rhs)\n\n\n\n\n_Equal is an alias of _equal.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._EqualScalar\n \n \nMethod\n.\n\n\n_EqualScalar(data, scalar)\n\n\n\n\n_EqualScalar is an alias of _equal_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Greater\n \n \nMethod\n.\n\n\n_Greater(lhs, rhs)\n\n\n\n\n_Greater is an alias of _greater.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._GreaterEqualScalar\n \n \nMethod\n.\n\n\n_GreaterEqualScalar(data, scalar)\n\n\n\n\n_GreaterEqualScalar is an alias of _greater_equal_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._GreaterScalar\n \n \nMethod\n.\n\n\n_GreaterScalar(data, scalar)\n\n\n\n\n_GreaterScalar is an alias of _greater_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Greater_Equal\n \n \nMethod\n.\n\n\n_Greater_Equal(lhs, rhs)\n\n\n\n\n_Greater_Equal is an alias of _greater_equal.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Hypot\n \n \nMethod\n.\n\n\n_Hypot(lhs, rhs)\n\n\n\n\n_Hypot is an alias of _hypot.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._HypotScalar\n \n \nMethod\n.\n\n\n_HypotScalar(data, scalar)\n\n\n\n\n_HypotScalar is an alias of _hypot_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Lesser\n \n \nMethod\n.\n\n\n_Lesser(lhs, rhs)\n\n\n\n\n_Lesser is an alias of _lesser.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._LesserEqualScalar\n \n \nMethod\n.\n\n\n_LesserEqualScalar(data, scalar)\n\n\n\n\n_LesserEqualScalar is an alias of _lesser_equal_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._LesserScalar\n \n \nMethod\n.\n\n\n_LesserScalar(data, scalar)\n\n\n\n\n_LesserScalar is an alias of _lesser_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Lesser_Equal\n \n \nMethod\n.\n\n\n_Lesser_Equal(lhs, rhs)\n\n\n\n\n_Lesser_Equal is an alias of _lesser_equal.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Maximum\n \n \nMethod\n.\n\n\n_Maximum(lhs, rhs)\n\n\n\n\n_Maximum is an alias of _maximum.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MaximumScalar\n \n \nMethod\n.\n\n\n_MaximumScalar(data, scalar)\n\n\n\n\n_MaximumScalar is an alias of _maximum_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Minimum\n \n \nMethod\n.\n\n\n_Minimum(lhs, rhs)\n\n\n\n\n_Minimum is an alias of _minimum.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MinimumScalar\n \n \nMethod\n.\n\n\n_MinimumScalar(data, scalar)\n\n\n\n\n_MinimumScalar is an alias of _minimum_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Minus\n \n \nMethod\n.\n\n\n_Minus(lhs, rhs)\n\n\n\n\n_Minus is an alias of _sub.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MinusScalar\n \n \nMethod\n.\n\n\n_MinusScalar(data, scalar)\n\n\n\n\n_MinusScalar is an alias of _minus_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Mul\n \n \nMethod\n.\n\n\n_Mul(lhs, rhs)\n\n\n\n\n_Mul is an alias of _mul.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MulScalar\n \n \nMethod\n.\n\n\n_MulScalar(data, scalar)\n\n\n\n\n_MulScalar is an alias of _mul_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._NDArray\n \n \nMethod\n.\n\n\n_NDArray(info)\n\n\n\n\nStub for implementing an operator implemented in native frontend language with ndarray.\n\n\nArguments\n\n\n\n\ninfo::, required\n:\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Native\n \n \nMethod\n.\n\n\n_Native(info, need_top_grad)\n\n\n\n\nStub for implementing an operator implemented in native frontend language.\n\n\nArguments\n\n\n\n\ninfo::, required\n:\n\n\nneed_top_grad::boolean, optional, default=True\n: Whether this layer needs out grad for backward. Should be false for loss layers.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._NoGradient\n \n \nMethod\n.\n\n\n_NoGradient()\n\n\n\n\nPlace holder for variable who cannot perform gradient\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._NotEqualScalar\n \n \nMethod\n.\n\n\n_NotEqualScalar(data, scalar)\n\n\n\n\n_NotEqualScalar is an alias of _not_equal_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Not_Equal\n \n \nMethod\n.\n\n\n_Not_Equal(lhs, rhs)\n\n\n\n\n_Not_Equal is an alias of _not_equal.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Plus\n \n \nMethod\n.\n\n\n_Plus(lhs, rhs)\n\n\n\n\n_Plus is an alias of elemwise_add.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._PlusScalar\n \n \nMethod\n.\n\n\n_PlusScalar(data, scalar)\n\n\n\n\n_PlusScalar is an alias of _plus_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Power\n \n \nMethod\n.\n\n\n_Power(lhs, rhs)\n\n\n\n\n_Power is an alias of _power.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._PowerScalar\n \n \nMethod\n.\n\n\n_PowerScalar(data, scalar)\n\n\n\n\n_PowerScalar is an alias of _power_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._RDivScalar\n \n \nMethod\n.\n\n\n_RDivScalar(data, scalar)\n\n\n\n\n_RDivScalar is an alias of _rdiv_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._RMinusScalar\n \n \nMethod\n.\n\n\n_RMinusScalar(data, scalar)\n\n\n\n\n_RMinusScalar is an alias of _rminus_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._RPowerScalar\n \n \nMethod\n.\n\n\n_RPowerScalar(data, scalar)\n\n\n\n\n_RPowerScalar is an alias of _rpower_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._add\n \n \nMethod\n.\n\n\n_add(lhs, rhs)\n\n\n\n\n_add is an alias of elemwise_add.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._arange\n \n \nMethod\n.\n\n\n_arange(start, stop, step, repeat, ctx, dtype)\n\n\n\n\nReturn evenly spaced values within a given interval. Similar to Numpy\n\n\nArguments\n\n\n\n\nstart::float, required\n: Start of interval. The interval includes this value. The default start value is 0.\n\n\nstop::, optional, default=None\n: End of interval. The interval does not include this value, except in some cases where step is not an integer and floating point round-off affects the length of out.\n\n\nstep::float, optional, default=1\n: Spacing between values.\n\n\nrepeat::int, optional, default='1'\n: The repeating time of all elements. E.g repeat=3, the element a will be repeated three times \u2013\n a, a, a.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n.Only used for imperative calls.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32'\n: Target data type.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Activation\n \n \nMethod\n.\n\n\n_backward_Activation()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_BatchNorm\n \n \nMethod\n.\n\n\n_backward_BatchNorm()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Cast\n \n \nMethod\n.\n\n\n_backward_Cast()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Concat\n \n \nMethod\n.\n\n\n_backward_Concat()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Convolution\n \n \nMethod\n.\n\n\n_backward_Convolution()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Correlation\n \n \nMethod\n.\n\n\n_backward_Correlation()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Crop\n \n \nMethod\n.\n\n\n_backward_Crop()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Custom\n \n \nMethod\n.\n\n\n_backward_Custom()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Deconvolution\n \n \nMethod\n.\n\n\n_backward_Deconvolution()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Dropout\n \n \nMethod\n.\n\n\n_backward_Dropout()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Embedding\n \n \nMethod\n.\n\n\n_backward_Embedding()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_FullyConnected\n \n \nMethod\n.\n\n\n_backward_FullyConnected()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_IdentityAttachKLSparseReg\n \n \nMethod\n.\n\n\n_backward_IdentityAttachKLSparseReg()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_InstanceNorm\n \n \nMethod\n.\n\n\n_backward_InstanceNorm()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_L2Normalization\n \n \nMethod\n.\n\n\n_backward_L2Normalization()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_LRN\n \n \nMethod\n.\n\n\n_backward_LRN()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_LeakyReLU\n \n \nMethod\n.\n\n\n_backward_LeakyReLU()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_LinearRegressionOutput\n \n \nMethod\n.\n\n\n_backward_LinearRegressionOutput()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_LogisticRegressionOutput\n \n \nMethod\n.\n\n\n_backward_LogisticRegressionOutput()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_MAERegressionOutput\n \n \nMethod\n.\n\n\n_backward_MAERegressionOutput()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_MakeLoss\n \n \nMethod\n.\n\n\n_backward_MakeLoss()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Pad\n \n \nMethod\n.\n\n\n_backward_Pad()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Pooling\n \n \nMethod\n.\n\n\n_backward_Pooling()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_RNN\n \n \nMethod\n.\n\n\n_backward_RNN()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_ROIPooling\n \n \nMethod\n.\n\n\n_backward_ROIPooling()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SVMOutput\n \n \nMethod\n.\n\n\n_backward_SVMOutput()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SequenceLast\n \n \nMethod\n.\n\n\n_backward_SequenceLast()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SequenceMask\n \n \nMethod\n.\n\n\n_backward_SequenceMask()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SequenceReverse\n \n \nMethod\n.\n\n\n_backward_SequenceReverse()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SliceChannel\n \n \nMethod\n.\n\n\n_backward_SliceChannel()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Softmax\n \n \nMethod\n.\n\n\n_backward_Softmax()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SoftmaxActivation\n \n \nMethod\n.\n\n\n_backward_SoftmaxActivation()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SoftmaxOutput\n \n \nMethod\n.\n\n\n_backward_SoftmaxOutput()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SpatialTransformer\n \n \nMethod\n.\n\n\n_backward_SpatialTransformer()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SwapAxis\n \n \nMethod\n.\n\n\n_backward_SwapAxis()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_UpSampling\n \n \nMethod\n.\n\n\n_backward_UpSampling()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__CrossDeviceCopy\n \n \nMethod\n.\n\n\n_backward__CrossDeviceCopy()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__NDArray\n \n \nMethod\n.\n\n\n_backward__NDArray()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__Native\n \n \nMethod\n.\n\n\n_backward__Native()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_abs\n \n \nMethod\n.\n\n\n_backward_abs(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_add\n \n \nMethod\n.\n\n\n_backward_add()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arccos\n \n \nMethod\n.\n\n\n_backward_arccos(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arccosh\n \n \nMethod\n.\n\n\n_backward_arccosh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arcsin\n \n \nMethod\n.\n\n\n_backward_arcsin(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arcsinh\n \n \nMethod\n.\n\n\n_backward_arcsinh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arctan\n \n \nMethod\n.\n\n\n_backward_arctan(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arctanh\n \n \nMethod\n.\n\n\n_backward_arctanh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_batch_dot\n \n \nMethod\n.\n\n\n_backward_batch_dot()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_add\n \n \nMethod\n.\n\n\n_backward_broadcast_add()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_div\n \n \nMethod\n.\n\n\n_backward_broadcast_div()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_hypot\n \n \nMethod\n.\n\n\n_backward_broadcast_hypot()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_maximum\n \n \nMethod\n.\n\n\n_backward_broadcast_maximum()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_minimum\n \n \nMethod\n.\n\n\n_backward_broadcast_minimum()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_mul\n \n \nMethod\n.\n\n\n_backward_broadcast_mul()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_power\n \n \nMethod\n.\n\n\n_backward_broadcast_power()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_sub\n \n \nMethod\n.\n\n\n_backward_broadcast_sub()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_clip\n \n \nMethod\n.\n\n\n_backward_clip()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_copy\n \n \nMethod\n.\n\n\n_backward_copy()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_cos\n \n \nMethod\n.\n\n\n_backward_cos(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_cosh\n \n \nMethod\n.\n\n\n_backward_cosh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_degrees\n \n \nMethod\n.\n\n\n_backward_degrees(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_div\n \n \nMethod\n.\n\n\n_backward_div()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_dot\n \n \nMethod\n.\n\n\n_backward_dot(transpose_a, transpose_b)\n\n\n\n\nArguments\n\n\n\n\ntranspose_a::boolean, optional, default=False\n: True if the first matrix is transposed.\n\n\ntranspose_b::boolean, optional, default=False\n: True if the second matrix is tranposed.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_expm1\n \n \nMethod\n.\n\n\n_backward_expm1(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_gamma\n \n \nMethod\n.\n\n\n_backward_gamma(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_gammaln\n \n \nMethod\n.\n\n\n_backward_gammaln(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_hypot\n \n \nMethod\n.\n\n\n_backward_hypot()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_hypot_scalar\n \n \nMethod\n.\n\n\n_backward_hypot_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nscalar::float\n: scalar value\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_log\n \n \nMethod\n.\n\n\n_backward_log(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_log1p\n \n \nMethod\n.\n\n\n_backward_log1p(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_max\n \n \nMethod\n.\n\n\n_backward_max()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_maximum\n \n \nMethod\n.\n\n\n_backward_maximum()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_maximum_scalar\n \n \nMethod\n.\n\n\n_backward_maximum_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nscalar::float\n: scalar value\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_mean\n \n \nMethod\n.\n\n\n_backward_mean()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_min\n \n \nMethod\n.\n\n\n_backward_min()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_minimum\n \n \nMethod\n.\n\n\n_backward_minimum()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_minimum_scalar\n \n \nMethod\n.\n\n\n_backward_minimum_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nscalar::float\n: scalar value\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_mul\n \n \nMethod\n.\n\n\n_backward_mul()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_nanprod\n \n \nMethod\n.\n\n\n_backward_nanprod()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_nansum\n \n \nMethod\n.\n\n\n_backward_nansum()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_power\n \n \nMethod\n.\n\n\n_backward_power()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_power_scalar\n \n \nMethod\n.\n\n\n_backward_power_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nscalar::float\n: scalar value\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_prod\n \n \nMethod\n.\n\n\n_backward_prod()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_radians\n \n \nMethod\n.\n\n\n_backward_radians(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_rdiv_scalar\n \n \nMethod\n.\n\n\n_backward_rdiv_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nscalar::float\n: scalar value\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_rpower_scalar\n \n \nMethod\n.\n\n\n_backward_rpower_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nscalar::float\n: scalar value\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_rsqrt\n \n \nMethod\n.\n\n\n_backward_rsqrt(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sign\n \n \nMethod\n.\n\n\n_backward_sign(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sin\n \n \nMethod\n.\n\n\n_backward_sin(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sinh\n \n \nMethod\n.\n\n\n_backward_sinh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_slice_axis\n \n \nMethod\n.\n\n\n_backward_slice_axis()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_smooth_l1\n \n \nMethod\n.\n\n\n_backward_smooth_l1(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_softmax_cross_entropy\n \n \nMethod\n.\n\n\n_backward_softmax_cross_entropy()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sqrt\n \n \nMethod\n.\n\n\n_backward_sqrt(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_square\n \n \nMethod\n.\n\n\n_backward_square(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sub\n \n \nMethod\n.\n\n\n_backward_sub()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sum\n \n \nMethod\n.\n\n\n_backward_sum()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_take\n \n \nMethod\n.\n\n\n_backward_take()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_tan\n \n \nMethod\n.\n\n\n_backward_tan(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_tanh\n \n \nMethod\n.\n\n\n_backward_tanh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_topk\n \n \nMethod\n.\n\n\n_backward_topk()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._broadcast\n \n \nMethod\n.\n\n\n_broadcast(src, axis, size)\n\n\n\n\nBroadcast array in the given axis to the given size\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: source ndarray\n\n\naxis::int\n: axis to broadcast\n\n\nsize::int\n: size of broadcast\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._broadcast_backward\n \n \nMethod\n.\n\n\n_broadcast_backward()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._copy\n \n \nMethod\n.\n\n\n_copy(data)\n\n\n\n\nIdentity mapping, copy src to output\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:14\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._copyto\n \n \nMethod\n.\n\n\n_copyto(src)\n\n\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._crop_assign\n \n \nMethod\n.\n\n\n_crop_assign(lhs, rhs, begin, end)\n\n\n\n\n(Assign the rhs to a cropped subset of lhs.\n\n\nRequirements\n\n\n\n\noutput should be explicitly given and be the same as lhs.\n\n\nlhs and rhs are of the same data type, and on the same device.\n\n\n\n\n)\n\n\nFrom:src/operator/tensor/matrix_op.cc:159\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Source input\n\n\nrhs::NDArray\n: value to assign\n\n\nbegin::Shape(tuple), required\n: starting coordinates\n\n\nend::Shape(tuple), required\n: ending coordinates\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._crop_assign_scalar\n \n \nMethod\n.\n\n\n_crop_assign_scalar(data, scalar, begin, end)\n\n\n\n\n(Assign the scalar to a cropped subset of the input.\n\n\nRequirements\n\n\n\n\noutput should be explicitly given and be the same as input\n\n\n\n\n)\n\n\nFrom:src/operator/tensor/matrix_op.cc:183\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nscalar::float, optional, default=0\n: The scalar value for assignment.\n\n\nbegin::Shape(tuple), required\n: starting coordinates\n\n\nend::Shape(tuple), required\n: ending coordinates\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._cvcopyMakeBorder\n \n \nMethod\n.\n\n\n_cvcopyMakeBorder(src, top, bot, left, right, type, value)\n\n\n\n\nPad image border with OpenCV. \n\n\nArguments\n\n\n\n\nsrc::NDArray\n: source image\n\n\ntop::int, required\n: Top margin.\n\n\nbot::int, required\n: Bottom margin.\n\n\nleft::int, required\n: Left margin.\n\n\nright::int, required\n: Right margin.\n\n\ntype::int, optional, default='0'\n: Filling type (default=cv2.BORDER_CONSTANT).\n\n\nvalue::double, optional, default=0\n: Fill with value.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._cvimdecode\n \n \nMethod\n.\n\n\n_cvimdecode(buf, flag, to_rgb)\n\n\n\n\nDecode image with OpenCV.  Note: return image in RGB by default, instead of OpenCV's default BGR.\n\n\nArguments\n\n\n\n\nbuf::NDArray\n: Buffer containing binary encoded image\n\n\nflag::int, optional, default='1'\n: Convert decoded image to grayscale (0) or color (1).\n\n\nto_rgb::boolean, optional, default=True\n: Whether to convert decoded image to mxnet's default RGB format (instead of opencv's default BGR).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._cvimresize\n \n \nMethod\n.\n\n\n_cvimresize(src, w, h, interp)\n\n\n\n\nResize image with OpenCV. \n\n\nArguments\n\n\n\n\nsrc::NDArray\n: source image\n\n\nw::int, required\n: Width of resized image.\n\n\nh::int, required\n: Height of resized image.\n\n\ninterp::int, optional, default='1'\n: Interpolation method (default=cv2.INTER_LINEAR).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._div_scalar\n \n \nMethod\n.\n\n\n_div_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._equal\n \n \nMethod\n.\n\n\n_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._equal_scalar\n \n \nMethod\n.\n\n\n_equal_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._get_ndarray_function_def\n \n \nMethod\n.\n\n\nThe libxmnet APIs are automatically imported from \nlibmxnet.so\n. The functions listed here operate on \nNDArray\n objects. The arguments to the functions are typically ordered as\n\n\n  func_name(arg_in1, arg_in2, ..., scalar1, scalar2, ..., arg_out1, arg_out2, ...)\n\n\n\n\nunless \nNDARRAY_ARG_BEFORE_SCALAR\n is not set. In this case, the scalars are put before the input arguments:\n\n\n  func_name(scalar1, scalar2, ..., arg_in1, arg_in2, ..., arg_out1, arg_out2, ...)\n\n\n\n\nIf \nACCEPT_EMPTY_MUTATE_TARGET\n is set. An overloaded function without the output arguments will also be defined:\n\n\n  func_name(arg_in1, arg_in2, ..., scalar1, scalar2, ...)\n\n\n\n\nUpon calling, the output arguments will be automatically initialized with empty NDArrays.\n\n\nThose functions always return the output arguments. If there is only one output (the typical situation), that object (\nNDArray\n) is returned. Otherwise, a tuple containing all the outputs will be returned.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._grad_add\n \n \nMethod\n.\n\n\n_grad_add(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._greater\n \n \nMethod\n.\n\n\n_greater(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._greater_equal\n \n \nMethod\n.\n\n\n_greater_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._greater_equal_scalar\n \n \nMethod\n.\n\n\n_greater_equal_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._greater_scalar\n \n \nMethod\n.\n\n\n_greater_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._hypot\n \n \nMethod\n.\n\n\n_hypot(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._hypot_scalar\n \n \nMethod\n.\n\n\n_hypot_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._identity_with_attr_like_rhs\n \n \nMethod\n.\n\n\n_identity_with_attr_like_rhs()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._imdecode\n \n \nMethod\n.\n\n\n_imdecode(mean, index, x0, y0, x1, y1, c, size)\n\n\n\n\nDecode an image, clip to (x0, y0, x1, y1), subtract mean, and write to buffer\n\n\nArguments\n\n\n\n\nmean::NDArray\n: image mean\n\n\nindex::int\n: buffer position for output\n\n\nx0::int\n: x0\n\n\ny0::int\n: y0\n\n\nx1::int\n: x1\n\n\ny1::int\n: y1\n\n\nc::int\n: channel\n\n\nsize::int\n: length of str_img\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._lesser\n \n \nMethod\n.\n\n\n_lesser(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._lesser_equal\n \n \nMethod\n.\n\n\n_lesser_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._lesser_equal_scalar\n \n \nMethod\n.\n\n\n_lesser_equal_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._lesser_scalar\n \n \nMethod\n.\n\n\n_lesser_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._maximum\n \n \nMethod\n.\n\n\n_maximum(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._maximum_scalar\n \n \nMethod\n.\n\n\n_maximum_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minimum\n \n \nMethod\n.\n\n\n_minimum(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minimum_scalar\n \n \nMethod\n.\n\n\n_minimum_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minus\n \n \nMethod\n.\n\n\n_minus(lhs, rhs)\n\n\n\n\n_minus is an alias of _sub.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minus_scalar\n \n \nMethod\n.\n\n\n_minus_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._mul\n \n \nMethod\n.\n\n\n_mul(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._mul_scalar\n \n \nMethod\n.\n\n\n_mul_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._not_equal\n \n \nMethod\n.\n\n\n_not_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._not_equal_scalar\n \n \nMethod\n.\n\n\n_not_equal_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._onehot_encode\n \n \nMethod\n.\n\n\n_onehot_encode(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand to the function.\n\n\nrhs::NDArray\n: Right operand to the function.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._ones\n \n \nMethod\n.\n\n\n_ones(shape, ctx, dtype)\n\n\n\n\nfill target with ones\n\n\nArguments\n\n\n\n\nshape::Shape(tuple), optional, default=()\n: The shape of the output\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n.Only used for imperative calls.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32'\n: Target data type.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._plus\n \n \nMethod\n.\n\n\n_plus(lhs, rhs)\n\n\n\n\n_plus is an alias of elemwise_add.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._plus_scalar\n \n \nMethod\n.\n\n\n_plus_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._power\n \n \nMethod\n.\n\n\n_power(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._power_scalar\n \n \nMethod\n.\n\n\n_power_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._rdiv_scalar\n \n \nMethod\n.\n\n\n_rdiv_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._rminus_scalar\n \n \nMethod\n.\n\n\n_rminus_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._rpower_scalar\n \n \nMethod\n.\n\n\n_rpower_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_normal\n \n \nMethod\n.\n\n\n_sample_normal(loc, scale, shape, ctx, dtype)\n\n\n\n\n_sample_normal is an alias of normal.\n\n\nSample a normal distribution\n\n\nArguments\n\n\n\n\nloc::float, optional, default=0\n: Mean of the distribution.\n\n\nscale::float, optional, default=1\n: Standard deviation of the distribution.\n\n\nshape::Shape(tuple), optional, default=()\n: The shape of the output\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n.Only used for imperative calls.\n\n\ndtype::{'float32'},optional, default='float32'\n: DType of the output\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_uniform\n \n \nMethod\n.\n\n\n_sample_uniform(low, high, shape, ctx, dtype)\n\n\n\n\n_sample_uniform is an alias of uniform.\n\n\nSample a uniform distribution\n\n\nArguments\n\n\n\n\nlow::float, optional, default=0\n: The lower bound of distribution\n\n\nhigh::float, optional, default=1\n: The upper bound of distribution\n\n\nshape::Shape(tuple), optional, default=()\n: The shape of the output\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n.Only used for imperative calls.\n\n\ndtype::{'float32'},optional, default='float32'\n: DType of the output\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._set_value\n \n \nMethod\n.\n\n\n_set_value(src)\n\n\n\n\nArguments\n\n\n\n\nsrc::real_t\n: Source input to the function.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._zeros\n \n \nMethod\n.\n\n\n_zeros(shape, ctx, dtype)\n\n\n\n\nfill target with zeros\n\n\nArguments\n\n\n\n\nshape::Shape(tuple), optional, default=()\n: The shape of the output\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n.Only used for imperative calls.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32'\n: Target data type.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.adam_update\n \n \nMethod\n.\n\n\nadam_update()\n\n\n\n\nUpdater function for adam optimizer\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx.add_to!\n \n \nMethod\n.\n\n\nadd_to!(dst :: NDArray, args :: Union{Real, NDArray}...)\n\n\n\n\nAdd a bunch of arguments into \ndst\n. Inplace updating.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.arccos\n \n \nMethod\n.\n\n\narccos(data)\n\n\n\n\nTake arccos of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:216\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.arccosh\n \n \nMethod\n.\n\n\narccosh(data)\n\n\n\n\nTake arccosh of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:288\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.arcsin\n \n \nMethod\n.\n\n\narcsin(data)\n\n\n\n\nTake arcsin of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:207\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.arcsinh\n \n \nMethod\n.\n\n\narcsinh(data)\n\n\n\n\nTake arcsinh of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:279\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.arctan\n \n \nMethod\n.\n\n\narctan(data)\n\n\n\n\nTake arctan of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:225\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.arctanh\n \n \nMethod\n.\n\n\narctanh(data)\n\n\n\n\nTake arctanh of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:297\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.argmax\n \n \nMethod\n.\n\n\nargmax(data, axis, keepdims)\n\n\n\n\nCompute argmax\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_index.cc:11\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::int, optional, default='-1'\n: Empty or unsigned. The axis to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.argmax_channel\n \n \nMethod\n.\n\n\nargmax_channel(src)\n\n\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.argmin\n \n \nMethod\n.\n\n\nargmin(data, axis, keepdims)\n\n\n\n\nCompute argmin\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_index.cc:16\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::int, optional, default='-1'\n: Empty or unsigned. The axis to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.argsort\n \n \nMethod\n.\n\n\nargsort(src, axis, is_ascend)\n\n\n\n\nReturns the indices that would sort an array.\n\n\nFrom:src/operator/tensor/ordering_op.cc:89\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input\n\n\naxis::int or None, optional, default='-1'\n: Axis along which to sort the input tensor. If not given, the flattened array is used. Default is -1.\n\n\nis_ascend::boolean, optional, default=True\n: Whether sort in ascending or descending order.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.batch_dot\n \n \nMethod\n.\n\n\nbatch_dot(lhs, rhs, transpose_a, transpose_b)\n\n\n\n\nCalculate batched dot product of two matrices. (batch, M, K) X (batch, K, N) \u2013\n (batch, M, N).\n\n\nFrom:src/operator/tensor/matrix_op.cc:257\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left input\n\n\nrhs::NDArray\n: Right input\n\n\ntranspose_a::boolean, optional, default=False\n: True if the first matrix is transposed.\n\n\ntranspose_b::boolean, optional, default=False\n: True if the second matrix is tranposed.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.batch_take\n \n \nMethod\n.\n\n\nbatch_take(a, indices)\n\n\n\n\nTake scalar value from a batch of data vectos according to an index vector, i.e. out[i] = a[i, indices[i]]\n\n\nFrom:src/operator/tensor/indexing_op.cc:97\n\n\nArguments\n\n\n\n\na::NDArray\n: Input data array\n\n\nindices::NDArray\n: index array\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_add\n \n \nMethod\n.\n\n\nbroadcast_add(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_axis\n \n \nMethod\n.\n\n\nbroadcast_axis(data, axis, size)\n\n\n\n\nBroadcast src along axis\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:85\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: The axes to perform the broadcasting.\n\n\nsize::Shape(tuple), optional, default=()\n: Target sizes of the broadcasting axes.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_div\n \n \nMethod\n.\n\n\nbroadcast_div(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_equal\n \n \nMethod\n.\n\n\nbroadcast_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_greater\n \n \nMethod\n.\n\n\nbroadcast_greater(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_greater_equal\n \n \nMethod\n.\n\n\nbroadcast_greater_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_hypot\n \n \nMethod\n.\n\n\nbroadcast_hypot(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_lesser\n \n \nMethod\n.\n\n\nbroadcast_lesser(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_lesser_equal\n \n \nMethod\n.\n\n\nbroadcast_lesser_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_maximum\n \n \nMethod\n.\n\n\nbroadcast_maximum(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_minimum\n \n \nMethod\n.\n\n\nbroadcast_minimum(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_minus\n \n \nMethod\n.\n\n\nbroadcast_minus(lhs, rhs)\n\n\n\n\nbroadcast_minus is an alias of broadcast_sub.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_mul\n \n \nMethod\n.\n\n\nbroadcast_mul(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_not_equal\n \n \nMethod\n.\n\n\nbroadcast_not_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_plus\n \n \nMethod\n.\n\n\nbroadcast_plus(lhs, rhs)\n\n\n\n\nbroadcast_plus is an alias of broadcast_add.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_power\n \n \nMethod\n.\n\n\nbroadcast_power(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_sub\n \n \nMethod\n.\n\n\nbroadcast_sub(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_to\n \n \nMethod\n.\n\n\nbroadcast_to(data, shape)\n\n\n\n\nBroadcast src to shape\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:92\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nshape::Shape(tuple), optional, default=()\n: The shape of the desired array. We can set the dim to zero if it's same as the original. E.g \nA = broadcast_to(B, shape=(10, 0, 0))\n has the same meaning as \nA = broadcast_axis(B, axis=0, size=10)\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.choose_element_0index\n \n \nMethod\n.\n\n\nchoose_element_0index(lhs, rhs)\n\n\n\n\nChoose one element from each line(row for python, column for R/Julia) in lhs according to index indicated by rhs. This function assume rhs uses 0-based index.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand to the function.\n\n\nrhs::NDArray\n: Right operand to the function.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.clip\n \n \nMethod\n.\n\n\nclip(data, a_min, a_max)\n\n\n\n\nClip ndarray elements to range (a_min, a_max)\n\n\nFrom:src/operator/tensor/matrix_op.cc:289\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\na_min::float, required\n: Minimum value\n\n\na_max::float, required\n: Maximum value\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.context\n \n \nMethod\n.\n\n\ncontext(arr :: NDArray)\n\n\n\n\nGet the context that this \nNDArray\n lives on.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.crop\n \n \nMethod\n.\n\n\ncrop(data, begin, end)\n\n\n\n\n(Crop the input tensor and return a new one.\n\n\nRequirements\n\n\n\n\nthe input and output (if explicitly given) are of the same data type, and on the same device.\n\n\n\n\n)\n\n\nFrom:src/operator/tensor/matrix_op.cc:143\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nbegin::Shape(tuple), required\n: starting coordinates\n\n\nend::Shape(tuple), required\n: ending coordinates\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.degrees\n \n \nMethod\n.\n\n\ndegrees(data)\n\n\n\n\nTake degrees of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:234\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.div_from!\n \n \nMethod\n.\n\n\ndiv_from!(dst :: NDArray, arg :: Union{Real, NDArray})\n\n\n\n\nElementwise divide a scalar or an \nNDArray\n of the same shape from \ndst\n. Inplace updating.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.elemwise_add\n \n \nMethod\n.\n\n\nelemwise_add(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.empty\n \n \nMethod\n.\n\n\nempty(shape :: Tuple, ctx :: Context)\nempty(shape :: Tuple)\nempty(dim1, dim2, ...)\n\n\n\n\nAllocate memory for an uninitialized \nNDArray\n with specific shape of type Float32.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.empty\n \n \nMethod\n.\n\n\nempty(DType, shape :: Tuple, ctx :: Context)\nempty(DType, shape :: Tuple)\nempty(DType, dim1, dim2, ...)\n\n\n\n\nAllocate memory for an uninitialized \nNDArray\n with a specified type.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.expand_dims\n \n \nMethod\n.\n\n\nexpand_dims(data, axis)\n\n\n\n\nExpand the shape of array by inserting a new axis.\n\n\nFrom:src/operator/tensor/matrix_op.cc:122\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::int (non-negative), required\n: Position (amongst axes) where new axis is to be inserted.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.fill_element_0index\n \n \nMethod\n.\n\n\nfill_element_0index(lhs, mhs, rhs)\n\n\n\n\nFill one element of each line(row for python, column for R/Julia) in lhs according to index indicated by rhs and values indicated by mhs. This function assume rhs uses 0-based index.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand to the function.\n\n\nmhs::NDArray\n: Middle operand to the function.\n\n\nrhs::NDArray\n: Right operand to the function.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.fix\n \n \nMethod\n.\n\n\nfix(data)\n\n\n\n\nTake round of the src to integer nearest 0\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:102\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.flip\n \n \nMethod\n.\n\n\nflip(data, axis)\n\n\n\n\nFlip the input tensor along axis and return a new one.\n\n\nFrom:src/operator/tensor/matrix_op.cc:219\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::int, required\n: The dimension to flip\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.gammaln\n \n \nMethod\n.\n\n\ngammaln(data)\n\n\n\n\nTake gammaln (log of the absolute value of gamma(x)) of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:315\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.is_shared\n \n \nMethod\n.\n\n\nis_shared(j_arr, arr)\n\n\n\n\nTest whether \nj_arr\n is sharing data with \narr\n.\n\n\nArguments:\n\n\n\n\nArray j_arr: the Julia Array.\n\n\nNDArray arr: the \nNDArray\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.load\n \n \nMethod\n.\n\n\nload(filename, ::Type{NDArray})\n\n\n\n\nLoad NDArrays from binary file.\n\n\nArguments:\n\n\n\n\nfilename::String\n: the path of the file to load. It could be S3 or HDFS address.\n\n\n\n\nReturns either \nDict{Symbol, NDArray}\n or \nVector{NDArray}\n.\n\n\nfilename\n can point to \ns3\n or \nhdfs\n resources if the \nlibmxnet\n is built with the corresponding components enabled. Examples:\n\n\n\n\ns3://my-bucket/path/my-s3-ndarray\n\n\nhdfs://my-bucket/path/my-hdfs-ndarray\n\n\n/path-to/my-local-ndarray\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.max_axis\n \n \nMethod\n.\n\n\nmax_axis(data, axis, keepdims)\n\n\n\n\nmax_axis is an alias of max.\n\n\nCompute max along axis. If axis is empty, global reduction is performed\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:66\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.min_axis\n \n \nMethod\n.\n\n\nmin_axis(data, axis, keepdims)\n\n\n\n\nmin_axis is an alias of min.\n\n\nCompute min along axis. If axis is empty, global reduction is performed\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:76\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.mul_to!\n \n \nMethod\n.\n\n\nmul_to!(dst :: NDArray, arg :: Union{Real, NDArray})\n\n\n\n\nElementwise multiplication into \ndst\n of either a scalar or an \nNDArray\n of the same shape. Inplace updating.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.nanprod\n \n \nMethod\n.\n\n\nnanprod(data, axis, keepdims)\n\n\n\n\nCompute product of src along axis, ignoring NaN values. If axis is empty, global reduction is performed\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:56\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.nansum\n \n \nMethod\n.\n\n\nnansum(data, axis, keepdims)\n\n\n\n\nSum src along axis, ignoring NaN values. If axis is empty, global reduction is performed\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:46\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.negative\n \n \nMethod\n.\n\n\nnegative(data)\n\n\n\n\nNegate src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:57\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.normal\n \n \nMethod\n.\n\n\nnormal(loc, scale, shape, ctx, dtype)\n\n\n\n\nSample a normal distribution\n\n\nArguments\n\n\n\n\nloc::float, optional, default=0\n: Mean of the distribution.\n\n\nscale::float, optional, default=1\n: Standard deviation of the distribution.\n\n\nshape::Shape(tuple), optional, default=()\n: The shape of the output\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n.Only used for imperative calls.\n\n\ndtype::{'float32'},optional, default='float32'\n: DType of the output\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ones\n \n \nMethod\n.\n\n\nones(shape :: Tuple, ctx :: Context)\nones(shape :: Tuple)\nones(dim1, dim2, ...)\n\n\n\n\nCreate an \nNDArray\n with specific shape and initialize with 1.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ones\n \n \nMethod\n.\n\n\nones(DType, shape :: Tuple, ctx :: Context)\nones(DType, shape :: Tuple)\nones(DType, dim1, dim2, ...)\n\n\n\n\nCreate an \nNDArray\n with specific shape \n type, and initialize with 1.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.radians\n \n \nMethod\n.\n\n\nradians(data)\n\n\n\n\nTake radians of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:243\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.rint\n \n \nMethod\n.\n\n\nrint(data)\n\n\n\n\nTake round of the src to nearest integer\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:97\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.rsqrt\n \n \nMethod\n.\n\n\nrsqrt(data)\n\n\n\n\nTake reciprocal square root of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:125\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.save\n \n \nMethod\n.\n\n\nsave(filename :: AbstractString, data)\n\n\n\n\nSave NDarrays to binary file. Filename could be S3 or HDFS address, if \nlibmxnet\n is built with corresponding support (see \nload\n).\n\n\n\n\nfilename::String\n: path to the binary file to write to.\n\n\ndata\n: data to save to file. Data can be a\nNDArray\n, a \nVector{NDArray}\n, or a \nDict{Base.Symbol, NDArray}\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sgd_mom_update\n \n \nMethod\n.\n\n\nsgd_mom_update()\n\n\n\n\nUpdater function for sgd optimizer\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sgd_update\n \n \nMethod\n.\n\n\nsgd_update()\n\n\n\n\nUpdater function for sgd optimizer\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx.slice_axis\n \n \nMethod\n.\n\n\nslice_axis(data, axis, begin, end)\n\n\n\n\nSlice the input along certain axis and return a sliced array. The slice will be taken from [begin, end). end can be None and axis can be negative.\n\n\nFrom:src/operator/tensor/matrix_op.cc:200\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::int, required\n: The axis to be sliced. Negative axis means to count from the last to the first axis.\n\n\nbegin::int, required\n: The beginning index to be sliced. Negative values are interpreted as counting from the backward.\n\n\nend::int or None, required\n: The end index to be sliced. The end can be None, in which case all the rest elements are used. Also, negative values are interpreted as counting from the backward.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.smooth_l1\n \n \nMethod\n.\n\n\nsmooth_l1(data, scalar)\n\n\n\n\nCalculate Smooth L1 Loss(lhs, scalar)\n\n\nFrom:src/operator/tensor/elemwise_binary_scalar_op_extended.cc:63\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.softmax_cross_entropy\n \n \nMethod\n.\n\n\nsoftmax_cross_entropy(data, label)\n\n\n\n\nCalculate cross_entropy(data, one_hot(label))\n\n\nFrom:src/operator/loss_binary_op.cc:12\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Input data\n\n\nlabel::NDArray\n: Input label\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.square\n \n \nMethod\n.\n\n\nsquare(data)\n\n\n\n\nTake square of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:107\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sub_from!\n \n \nMethod\n.\n\n\nsub_from!(dst :: NDArray, args :: Union{Real, NDArray}...)\n\n\n\n\nSubtract a bunch of arguments from \ndst\n. Inplace updating.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sum_axis\n \n \nMethod\n.\n\n\nsum_axis(data, axis, keepdims)\n\n\n\n\nsum_axis is an alias of sum.\n\n\nSum src along axis. If axis is empty, global reduction is performed\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:17\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.topk\n \n \nMethod\n.\n\n\ntopk(src, axis, k, ret_typ, is_ascend)\n\n\n\n\nReturn the top k element of an input tensor along a given axis.\n\n\nFrom:src/operator/tensor/ordering_op.cc:18\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input\n\n\naxis::int or None, optional, default='-1'\n: Axis along which to choose the top k indices. If not given, the flattened array is used. Default is -1.\n\n\nk::int, optional, default='1'\n: Number of top elements to select, should be always smaller than or equal to the element number in the given axis. A global sort is performed if set k \n 1.\n\n\nret_typ::{'both', 'indices', 'mask', 'value'},optional, default='indices'\n: The return type. \"value\" means returning the top k values, \"indices\" means returning the indices of the top k values, \"mask\" means to return a mask array containing 0 and 1. 1 means the top k values. \"both\" means to return both value and indices.\n\n\nis_ascend::boolean, optional, default=False\n: Whether to choose k largest or k smallest. Top K largest elements will be chosen if set to false.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.try_get_shared\n \n \nMethod\n.\n\n\ntry_get_shared(arr)\n\n\n\n\nTry to create a Julia array by sharing the data with the underlying \nNDArray\n.\n\n\nArguments:\n\n\n\n\narr::NDArray\n: the array to be shared.\n\n\n\n\n\n\nNote\n\n\nThe returned array does not guarantee to share data with the underlying \nNDArray\n. In particular, data sharing is possible only when the \nNDArray\n lives on CPU.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.uniform\n \n \nMethod\n.\n\n\nuniform(low, high, shape, ctx, dtype)\n\n\n\n\nSample a uniform distribution\n\n\nArguments\n\n\n\n\nlow::float, optional, default=0\n: The lower bound of distribution\n\n\nhigh::float, optional, default=1\n: The upper bound of distribution\n\n\nshape::Shape(tuple), optional, default=()\n: The shape of the output\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n.Only used for imperative calls.\n\n\ndtype::{'float32'},optional, default='float32'\n: DType of the output\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.zeros\n \n \nMethod\n.\n\n\nzeros(shape :: Tuple, ctx :: Context)\nzeros(shape :: Tuple)\nzeros(dim1, dim2, ...)\n\n\n\n\nCreate zero-ed \nNDArray\n with specific shape.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.zeros\n \n \nMethod\n.\n\n\nzeros(DType, shape :: Tuple, ctx :: Context)\nzeros(DType, shape :: Tuple)\nzeros(DType, dim1, dim2, ...)\n\n\n\n\nCreate zero-ed \nNDArray\n with specific shape and type\n\n\nsource\n\n\n#\n\n\nMXNet.mx.@inplace\n \n \nMacro\n.\n\n\n@inplace\n\n\n\n\nJulia does not support re-definiton of \n+=\n operator (like \n__iadd__\n in python), When one write \na += b\n, it gets translated to \na = a+b\n. \na+b\n will allocate new memory for the results, and the newly allocated \nNDArray\n object is then assigned back to a, while the original contents in a is discarded. This is very inefficient when we want to do inplace update.\n\n\nThis macro is a simple utility to implement this behavior. Write\n\n\n  @mx.inplace a += b\n\n\n\n\nwill translate into\n\n\n  mx.add_to!(a, b)\n\n\n\n\nwhich will do inplace adding of the contents of \nb\n into \na\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.@nd_as_jl\n \n \nMacro\n.\n\n\nManipulating as Julia Arrays\n\n\n@nd_as_jl(captures..., statement)\n\n\n\n\nA convenient macro that allows to operate \nNDArray\n as Julia Arrays. For example,\n\n\n  x = mx.zeros(3,4)\n  y = mx.ones(3,4)\n  z = mx.zeros((3,4), mx.gpu())\n\n  @mx.nd_as_jl ro=(x,y) rw=z begin\n    # now x, y, z are just ordinary Julia Arrays\n    z[:,1] = y[:,2]\n    z[:,2] = 5\n  end\n\n\n\n\nUnder the hood, the macro convert all the declared captures from \nNDArray\n into Julia Arrays, by using \ntry_get_shared\n. And automatically commit the modifications back into the \nNDArray\n that is declared as \nrw\n. This is useful for fast prototyping and when implement non-critical computations, such as \nAbstractEvalMetric\n.\n\n\n\n\nNote\n\n\n\n\n\n\nMultiple \nrw\n and / or \nro\n capture declaration could be made.\n\n\nThe macro does \nnot\n check to make sure that \nro\n captures are not modified. If the original \nNDArray\n lives in CPU memory, then it is very likely the corresponding Julia Array shares data with the \nNDArray\n, so modifying the Julia Array will also modify the underlying \nNDArray\n.\n\n\nMore importantly, since the \nNDArray\n is asynchronized, we will wait for \nwriting\n for \nrw\n variables but wait only for \nreading\n in \nro\n variables. If we write into those \nro\n variables, \nand\n if the memory is shared, racing condition might happen, and the behavior is undefined.\n\n\nWhen an \nNDArray\n is declared to be captured as \nrw\n, its contents is always sync back in the end.\n\n\nThe execution results of the expanded macro is always \nnothing\n.\n\n\nThe statements are wrapped in a \nlet\n, thus locally introduced new variables will not be available after the statements. So you will need to declare the variables before calling the macro if needed.\n\n\n\n\nsource", 
            "title": "NDArray API"
        }, 
        {
            "location": "/api/ndarray/#ndarray-api", 
            "text": "#  Base.Flatten     Method .  Flatten(data)  Flatten input into 2D by collapsing all the higher dimensions. A (d1, d2, ..., dK) tensor is flatten to (d1, d2  ...  dK) matrix.  Arguments   data::NDArray : Input data to reshape.   source  #  MXNet.mx.NDArray     Type .  NDArray  Wrapper of the  NDArray  type in  libmxnet . This is the basic building block of tensor-based computation.   Note  since C/C++ use row-major ordering for arrays while Julia follows a   column-major ordering. To keep things consistent, we keep the underlying data   in their original layout, but use  language-native  convention when we talk   about shapes. For example, a mini-batch of 100 MNIST images is a tensor of   C/C++/Python shape (100,1,28,28), while in Julia, the same piece of memory   have shape (28,28,1,100).   source  #  Base.:*     Method .  *(arg0, arg1)  Currently only multiplication a scalar with an  NDArray  is implemented. Matrix multiplication is to be added soon.  source  #  Base.:+     Method .  +(args...)\n.+(args...)  Summation. Multiple arguments of either scalar or  NDArray  could be added together. Note at least the first or second argument needs to be an  NDArray  to avoid ambiguity of built-in summation.  source  #  Base.:-     Method .  -(arg0, arg1)\n-(arg0)\n.-(arg0, arg1)  Subtraction  arg0 - arg1 , of scalar types or  NDArray . Or create the negative of  arg0 .  source  #  Base.:.*     Method .  .*(arg0, arg1)  Elementwise multiplication of  arg0  and  arg , could be either scalar or  NDArray .  source  #  Base.:./     Method .  ./(arg0 :: NDArray, arg :: Union{Real, NDArray})  Elementwise dividing an  NDArray  by a scalar or another  NDArray  of the same shape.  source  #  Base.:/     Method .  /(arg0 :: NDArray, arg :: Real)  Divide an  NDArray  by a scalar. Matrix division (solving linear systems) is not implemented yet.  source  #  Base.LinAlg.dot     Method .  dot(lhs, rhs, transpose_a, transpose_b)  Calculate dot product of two matrices or two vectors.  From:src/operator/tensor/matrix_op.cc:231  Arguments   lhs::NDArray : Left input  rhs::NDArray : Right input  transpose_a::boolean, optional, default=False : True if the first matrix is transposed.  transpose_b::boolean, optional, default=False : True if the second matrix is tranposed.   source  #  Base.LinAlg.norm     Method .  norm(src)  Arguments   src::NDArray : Source input   source  #  Base.Math.gamma     Method .  gamma(data)  Take the gamma function (extension of the factorial function) of the src  From:src/operator/tensor/elemwise_unary_op.cc:306  Arguments   data::NDArray : Source input   source  #  Base._div     Method .  _div(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  Base._sub     Method .  _sub(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  Base.abs     Method .  abs(data)  Take absolute value of the src  From:src/operator/tensor/elemwise_unary_op.cc:63  Arguments   data::NDArray : Source input   source  #  Base.ceil     Method .  ceil(data)  Take ceil of the src  From:src/operator/tensor/elemwise_unary_op.cc:87  Arguments   data::NDArray : Source input   source  #  Base.convert     Method .  convert(::Type{Array{T}}, arr :: NDArray)  Convert an  NDArray  into a Julia  Array  of specific type. Data will be copied.  source  #  Base.copy!     Method .  copy!(dst :: Union{NDArray, Array}, src :: Union{NDArray, Array})  Copy contents of  src  into  dst .  source  #  Base.copy     Method .  copy(arr :: NDArray)\ncopy(arr :: NDArray, ctx :: Context)\ncopy(arr :: Array, ctx :: Context)  Create a copy of an array. When no  Context  is given, create a Julia  Array . Otherwise, create an  NDArray  on the specified context.  source  #  Base.cos     Method .  cos(data)  Take cos of the src  From:src/operator/tensor/elemwise_unary_op.cc:189  Arguments   data::NDArray : Source input   source  #  Base.cosh     Method .  cosh(data)  Take cosh of the src  From:src/operator/tensor/elemwise_unary_op.cc:261  Arguments   data::NDArray : Source input   source  #  Base.eltype     Method .  eltype(arr :: NDArray)  Get the element type of an  NDArray .  source  #  Base.exp     Method .  exp(data)  Take exp of the src  From:src/operator/tensor/elemwise_unary_op.cc:135  Arguments   data::NDArray : Source input   source  #  Base.expm1     Method .  expm1(data)  Take  exp(x) - 1  in a numerically stable way  From:src/operator/tensor/elemwise_unary_op.cc:180  Arguments   data::NDArray : Source input   source  #  Base.floor     Method .  floor(data)  Take floor of the src  From:src/operator/tensor/elemwise_unary_op.cc:92  Arguments   data::NDArray : Source input   source  #  Base.getindex     Method .  getindex(arr :: NDArray, idx)  Shortcut for  slice . A typical use is to write    arr[:] += 5  which translates into    arr[:] = arr[:] + 5  which furthur translates into    setindex!(getindex(arr, Colon()), 5, Colon())   Note  The behavior is quite different from indexing into Julia's  Array . For example,  arr[2:5]  create a  copy  of the sub-array for Julia  Array , while for  NDArray , this is a  slice  that shares the memory.   source  #  Base.getindex     Method .  Shortcut for  slice .  NOTE  the behavior for Julia's built-in index slicing is to create a copy of the sub-array, while here we simply call  slice , which shares the underlying memory.  source  #  Base.identity     Method .  identity(data)  identity is an alias of _copy.  Identity mapping, copy src to output  From:src/operator/tensor/elemwise_unary_op.cc:14  Arguments   data::NDArray : Source input   source  #  Base.length     Method .  length(arr :: NDArray)  Get the number of elements in an  NDArray .  source  #  Base.log     Method .  log(data)  Take log of the src  From:src/operator/tensor/elemwise_unary_op.cc:141  Arguments   data::NDArray : Source input   source  #  Base.log10     Method .  log10(data)  Take base-10 log of the src  From:src/operator/tensor/elemwise_unary_op.cc:147  Arguments   data::NDArray : Source input   source  #  Base.log1p     Method .  log1p(data)  Take  log(1 + x)  in a numerically stable way  From:src/operator/tensor/elemwise_unary_op.cc:171  Arguments   data::NDArray : Source input   source  #  Base.log2     Method .  log2(data)  Take base-2 log of the src  From:src/operator/tensor/elemwise_unary_op.cc:153  Arguments   data::NDArray : Source input   source  #  Base.max     Method .  max(data, axis, keepdims)  Compute max along axis. If axis is empty, global reduction is performed  From:src/operator/tensor/broadcast_reduce_op_value.cc:66  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.   source  #  Base.mean     Method .  mean(data, axis, keepdims)  Compute mean src along axis. If axis is empty, global reduction is performed  From:src/operator/tensor/broadcast_reduce_op_value.cc:26  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.   source  #  Base.min     Method .  min(data, axis, keepdims)  Compute min along axis. If axis is empty, global reduction is performed  From:src/operator/tensor/broadcast_reduce_op_value.cc:76  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.   source  #  Base.ndims     Method .  ndims(arr :: NDArray)  Get the number of dimensions of an  NDArray . Is equivalent to  length(size(arr)) .  source  #  Base.prod     Method .  prod(data, axis, keepdims)  Compute product of src along axis. If axis is empty, global reduction is performed  From:src/operator/tensor/broadcast_reduce_op_value.cc:36  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.   source  #  Base.round     Method .  round(data)  Take round of the src  From:src/operator/tensor/elemwise_unary_op.cc:81  Arguments   data::NDArray : Source input   source  #  Base.setindex!     Method .  setindex!(arr :: NDArray, val, idx)  Assign values to an  NDArray . Elementwise assignment is not implemented, only the following scenarios are supported   arr[:] = val : whole array assignment,  val  could be a scalar or an array (Julia  Array  or  NDArray ) of the same shape.  arr[start:stop] = val : assignment to a  slice ,  val  could be a scalar or an array of the same shape to the slice. See also  slice .   source  #  Base.sign     Method .  sign(data)  Take sign of the src  From:src/operator/tensor/elemwise_unary_op.cc:72  Arguments   data::NDArray : Source input   source  #  Base.similar     Method .  similar(arr :: NDArray)  Create an  NDArray  with similar shape, data type, and context with the given one.  source  #  Base.sin     Method .  sin(data)  Take sin of the src  From:src/operator/tensor/elemwise_unary_op.cc:162  Arguments   data::NDArray : Source input   source  #  Base.sinh     Method .  sinh(data)  Take sinh of the src  From:src/operator/tensor/elemwise_unary_op.cc:252  Arguments   data::NDArray : Source input   source  #  Base.size     Method .  size(arr :: NDArray)\nsize(arr :: NDArray, dim :: Int)  Get the shape of an  NDArray . The shape is in Julia's column-major convention. See also the notes on NDArray shapes  NDArray .  source  #  Base.slice     Method .  slice(arr :: NDArray, start:stop)  Create a view into a sub-slice of an  NDArray . Note only slicing at the slowest changing dimension is supported. In Julia's column-major perspective, this is the last dimension. For example, given an  NDArray  of shape (2,3,4),  slice(array, 2:3)  will create a  NDArray  of shape (2,3,2), sharing the data with the original array. This operation is used in data parallelization to split mini-batch into sub-batches for different devices.  source  #  Base.sort     Method .  sort(src, axis, is_ascend)  Return a sorted copy of an array.  From:src/operator/tensor/ordering_op.cc:59  Arguments   src::NDArray : Source input  axis::int or None, optional, default='-1' : Axis along which to choose sort the input tensor. If not given, the flattened array is used. Default is -1.  is_ascend::boolean, optional, default=True : Whether sort in ascending or descending order.   source  #  Base.sqrt     Method .  sqrt(data)  Take square root of the src  From:src/operator/tensor/elemwise_unary_op.cc:116  Arguments   data::NDArray : Source input   source  #  Base.sum     Method .  sum(data, axis, keepdims)  Sum src along axis. If axis is empty, global reduction is performed  From:src/operator/tensor/broadcast_reduce_op_value.cc:17  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.   source  #  Base.take     Method .  take(a, indices, axis, mode)  Take row vectors from an NDArray according to the indices For an input of index with shape (d1, ..., dK), the output shape is (d1, ..., dK, row_vector_length).All the input values should be integers in the range [0, column_vector_length).  From:src/operator/tensor/indexing_op.cc:59  Arguments   a::SymbolicNode : The source array.  indices::SymbolicNode : The indices of the values to extract.  axis::int, optional, default='0' : the axis of data tensor to be taken.  mode::{'clip', 'raise', 'wrap'},optional, default='raise' : specify how out-of-bound indices bahave.   source  #  Base.tan     Method .  tan(data)  Take tan of the src  From:src/operator/tensor/elemwise_unary_op.cc:198  Arguments   data::NDArray : Source input   source  #  Base.tanh     Method .  tanh(data)  Take tanh of the src  From:src/operator/tensor/elemwise_unary_op.cc:270  Arguments   data::NDArray : Source input   source  #  Base.transpose     Method .  transpose(data, axes)  Transpose the input tensor and return a new one  From:src/operator/tensor/matrix_op.cc:94  Arguments   data::NDArray : Source input  axes::Shape(tuple), optional, default=() : Target axis order. By default the axes will be inverted.   source  #  MXNet.mx.Activation     Method .  Activation(data, act_type)  Elementwise activation function.  The following activation types are supported (operations are applied elementwisely to each scalar of the input tensor):   relu : Rectified Linear Unit,  y = max(x, 0)  sigmoid :  y = 1 / (1 + exp(-x))  tanh : Hyperbolic tangent,  y = (exp(x) - exp(-x)) / (exp(x) + exp(-x))  softrelu : Soft ReLU, or SoftPlus,  y = log(1 + exp(x))   See  LeakyReLU  for other activations with parameters.  Arguments   data::SymbolicNode : Input data to activation function.  act_type::{'relu', 'sigmoid', 'softrelu', 'tanh'}, required : Activation function to be applied.   source  #  MXNet.mx.BatchNorm     Method .  BatchNorm(data, gamma, beta, eps, momentum, fix_gamma, use_global_stats, output_mean_var)  Apply batch normalization to input.  Arguments   data::SymbolicNode : Input data to batch normalization  gamma::SymbolicNode : gamma matrix  beta::SymbolicNode : beta matrix  eps::float, optional, default=0.001 : Epsilon to prevent div 0  momentum::float, optional, default=0.9 : Momentum for moving average  fix_gamma::boolean, optional, default=True : Fix gamma while training  use_global_stats::boolean, optional, default=False : Whether use global moving statistics instead of local batch-norm. This will force change batch-norm into a scale shift operator.  output_mean_var::boolean, optional, default=False : Output All,normal mean and var   source  #  MXNet.mx.BlockGrad     Method .  BlockGrad(data)  Get output from a symbol and pass 0 gradient back  From:src/operator/tensor/elemwise_unary_op.cc:30  Arguments   data::NDArray : Source input   source  #  MXNet.mx.Cast     Method .  Cast(data, dtype)  Cast array to a different data type.  Arguments   data::SymbolicNode : Input data to cast function.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'}, required : Target data type.   source  #  MXNet.mx.Concat     Method .  Concat(data, num_args, dim)  Note : Concat takes variable number of positional inputs. So instead of calling as Concat([x, y, z], num_args=3), one should call via Concat(x, y, z), and num_args will be determined automatically.  Perform a feature concat on channel dim (defaut is 1) over all  Arguments   data::SymbolicNode[] : List of tensors to concatenate  num_args::int, required : Number of inputs to be concated.  dim::int, optional, default='1' : the dimension to be concated.   source  #  MXNet.mx.Convolution     Method .  Convolution(data, weight, bias, kernel, stride, dilate, pad, num_filter, num_group, workspace, no_bias, cudnn_tune, cudnn_off, layout)  Apply convolution to input then add a bias.  Arguments   data::SymbolicNode : Input data to the ConvolutionOp.  weight::SymbolicNode : Weight matrix.  bias::SymbolicNode : Bias parameter.  kernel::Shape(tuple), required : convolution kernel size: (h, w) or (d, h, w)  stride::Shape(tuple), optional, default=() : convolution stride: (h, w) or (d, h, w)  dilate::Shape(tuple), optional, default=() : convolution dilate: (h, w) or (d, h, w)  pad::Shape(tuple), optional, default=() : pad for convolution: (h, w) or (d, h, w)  num_filter::int (non-negative), required : convolution filter(channel) number  num_group::int (non-negative), optional, default=1 : Number of group partitions. Equivalent to slicing input into num_group   partitions, apply convolution on each, then concatenate the results  workspace::long (non-negative), optional, default=1024 : Maximum tmp workspace allowed for convolution (MB).  no_bias::boolean, optional, default=False : Whether to disable bias parameter.  cudnn_tune::{None, 'fastest', 'limited_workspace', 'off'},optional, default='None' : Whether to pick convolution algo by running performance test.   Leads to higher startup time but may give faster speed. Options are:   'off': no tuning   'limited_workspace': run test and pick the fastest algorithm that doesn't exceed workspace limit.   'fastest': pick the fastest algorithm and ignore workspace limit.   If set to None (default), behavior is determined by environment   variable MXNET_CUDNN_AUTOTUNE_DEFAULT: 0 for off,   1 for limited workspace (default), 2 for fastest.  cudnn_off::boolean, optional, default=False : Turn off cudnn for this layer.  layout::{None, 'NCDHW', 'NCHW', 'NDHWC', 'NHWC'},optional, default='None' : Set layout for input, output and weight. Empty for   default layout: NCHW for 2d and NCDHW for 3d.   source  #  MXNet.mx.Correlation     Method .  Correlation(data1, data2, kernel_size, max_displacement, stride1, stride2, pad_size, is_multiply)  Apply correlation to inputs  Arguments   data1::SymbolicNode : Input data1 to the correlation.  data2::SymbolicNode : Input data2 to the correlation.  kernel_size::int (non-negative), optional, default=1 : kernel size for Correlation must be an odd number  max_displacement::int (non-negative), optional, default=1 : Max displacement of Correlation  stride1::int (non-negative), optional, default=1 : stride1 quantize data1 globally  stride2::int (non-negative), optional, default=1 : stride2 quantize data2 within the neighborhood centered around data1  pad_size::int (non-negative), optional, default=0 : pad for Correlation  is_multiply::boolean, optional, default=True : operation type is either multiplication or subduction   source  #  MXNet.mx.Crop     Method .  Crop(data, num_args, offset, h_w, center_crop)  Note : Crop takes variable number of positional inputs. So instead of calling as Crop([x, y, z], num_args=3), one should call via Crop(x, y, z), and num_args will be determined automatically.  Crop the 2nd and 3rd dim of input data, with the corresponding size of h_w or with width and height of the second input symbol, i.e., with one input, we need h_w to specify the crop height and width, otherwise the second input symbol's size will be used  Arguments   data::SymbolicNode or SymbolicNode[] : Tensor or List of Tensors, the second input will be used as crop_like shape reference  num_args::int, required : Number of inputs for crop, if equals one, then we will use the h_wfor crop height and width, else if equals two, then we will use the heightand width of the second input symbol, we name crop_like here  offset::Shape(tuple), optional, default=(0,0) : crop offset coordinate: (y, x)  h_w::Shape(tuple), optional, default=(0,0) : crop height and weight: (h, w)  center_crop::boolean, optional, default=False : If set to true, then it will use be the center_crop,or it will crop using the shape of crop_like   source  #  MXNet.mx.Custom     Method .  Custom(op_type)  Custom operator implemented in frontend.  Arguments   op_type::string : Type of custom operator. Must be registered first.   source  #  MXNet.mx.Deconvolution     Method .  Deconvolution(data, weight, bias, kernel, stride, pad, adj, target_shape, num_filter, num_group, workspace, no_bias)  Apply deconvolution to input then add a bias.  Arguments   data::SymbolicNode : Input data to the DeconvolutionOp.  weight::SymbolicNode : Weight matrix.  bias::SymbolicNode : Bias parameter.  kernel::Shape(tuple), required : deconvolution kernel size: (y, x)  stride::Shape(tuple), optional, default=(1,1) : deconvolution stride: (y, x)  pad::Shape(tuple), optional, default=(0,0) : pad for deconvolution: (y, x), a good number is : (kernel-1)/2, if target_shape set, pad will be ignored and will be computed automatically  adj::Shape(tuple), optional, default=(0,0) : adjustment for output shape: (y, x), if target_shape set, adj will be ignored and will be computed automatically  target_shape::Shape(tuple), optional, default=(0,0) : output shape with targe shape : (y, x)  num_filter::int (non-negative), required : deconvolution filter(channel) number  num_group::int (non-negative), optional, default=1 : number of groups partition  workspace::long (non-negative), optional, default=512 : Tmp workspace for deconvolution (MB)  no_bias::boolean, optional, default=True : Whether to disable bias parameter.   source  #  MXNet.mx.Dropout     Method .  Dropout(data, p)  Apply dropout to input. During training, each element of the input is randomly set to zero with probability p. And then the whole tensor is rescaled by 1/(1-p) to keep the expectation the same as before applying dropout. During the test time, this behaves as an identity map.  Arguments   data::SymbolicNode : Input data to dropout.  p::float, optional, default=0.5 : Fraction of the input that gets dropped out at training time   source  #  MXNet.mx.ElementWiseSum     Method .  ElementWiseSum(args)  Note : ElementWiseSum takes variable number of positional inputs. So instead of calling as ElementWiseSum([x, y, z], num_args=3), one should call via ElementWiseSum(x, y, z), and num_args will be determined automatically.  Perform element sum of inputs  From:src/operator/tensor/elemwise_sum.cc:56  Arguments   args::NDArray[] : List of input tensors   source  #  MXNet.mx.Embedding     Method .  Embedding(data, weight, input_dim, output_dim)  Map integer index to vector representations (embeddings). Those embeddings are learnable parameters. For a input of shape (d1, ..., dK), the output shape is (d1, ..., dK, output_dim). All the input values should be integers in the range [0, input_dim).  From:src/operator/tensor/indexing_op.cc:18  Arguments   data::SymbolicNode : Input data to the EmbeddingOp.  weight::SymbolicNode : Embedding weight matrix.  input_dim::int, required : vocabulary size of the input indices.  output_dim::int, required : dimension of the embedding vectors.   source  #  MXNet.mx.FullyConnected     Method .  FullyConnected(data, weight, bias, num_hidden, no_bias)  Apply matrix multiplication to input then add a bias. It maps the input of shape  (batch_size, input_dim)  to the shape of  (batch_size, num_hidden) . Learnable parameters include the weights of the linear transform and an optional bias vector.  Arguments   data::SymbolicNode : Input data to the FullyConnectedOp.  weight::SymbolicNode : Weight matrix.  bias::SymbolicNode : Bias parameter.  num_hidden::int, required : Number of hidden nodes of the output.  no_bias::boolean, optional, default=False : Whether to disable bias parameter.   source  #  MXNet.mx.IdentityAttachKLSparseReg     Method .  IdentityAttachKLSparseReg(data, sparseness_target, penalty, momentum)  Apply a sparse regularization to the output a sigmoid activation function.  Arguments   data::SymbolicNode : Input data.  sparseness_target::float, optional, default=0.1 : The sparseness target  penalty::float, optional, default=0.001 : The tradeoff parameter for the sparseness penalty  momentum::float, optional, default=0.9 : The momentum for running average   source  #  MXNet.mx.InstanceNorm     Method .  InstanceNorm(data, gamma, beta, eps)  An operator taking in a n-dimensional input tensor (n   2), and normalizing the input by subtracting the mean and variance calculated over the spatial dimensions. This is an implemention of the operator described in \"Instance Normalization: The Missing Ingredient for Fast Stylization\", D. Ulyanov, A. Vedaldi, V. Lempitsky, 2016 (arXiv:1607.08022v2). This layer is similar to batch normalization, with two differences: first, the normalization is carried out per example ('instance'), not over a batch. Second, the same normalization is applied both at test and train time. This operation is also known as 'contrast normalization'.  Arguments   data::SymbolicNode : A n-dimensional tensor (n   2) of the form [batch, channel, spatial_dim1, spatial_dim2, ...].  gamma::SymbolicNode : A vector of length 'channel', which multiplies the normalized input.  beta::SymbolicNode : A vector of length 'channel', which is added to the product of the normalized input and the weight.  eps::float, optional, default=0.001 : Epsilon to prevent division by 0.   source  #  MXNet.mx.L2Normalization     Method .  L2Normalization(data, eps, mode)  Set the l2 norm of each instance to a constant.  Arguments   data::SymbolicNode : Input data to the L2NormalizationOp.  eps::float, optional, default=1e-10 : Epsilon to prevent div 0  mode::{'channel', 'instance', 'spatial'},optional, default='instance' : Normalization Mode. If set to instance, this operator will compute a norm for each instance in the batch; this is the default mode. If set to channel, this operator will compute a cross channel norm at each position of each instance. If set to spatial, this operator will compute a norm for each channel.   source  #  MXNet.mx.LRN     Method .  LRN(data, alpha, beta, knorm, nsize)  Apply convolution to input then add a bias.  Arguments   data::SymbolicNode : Input data to the ConvolutionOp.  alpha::float, optional, default=0.0001 : value of the alpha variance scaling parameter in the normalization formula  beta::float, optional, default=0.75 : value of the beta power parameter in the normalization formula  knorm::float, optional, default=2 : value of the k parameter in normalization formula  nsize::int (non-negative), required : normalization window width in elements.   source  #  MXNet.mx.LeakyReLU     Method .  LeakyReLU(data, act_type, slope, lower_bound, upper_bound)  Apply activation function to input.  Arguments   data::SymbolicNode : Input data to activation function.  act_type::{'elu', 'leaky', 'prelu', 'rrelu'},optional, default='leaky' : Activation function to be applied.  slope::float, optional, default=0.25 : Init slope for the activation. (For leaky and elu only)  lower_bound::float, optional, default=0.125 : Lower bound of random slope. (For rrelu only)  upper_bound::float, optional, default=0.334 : Upper bound of random slope. (For rrelu only)   source  #  MXNet.mx.LinearRegressionOutput     Method .  LinearRegressionOutput(data, label, grad_scale)  Use linear regression for final output, this is used on final output of a net.  Arguments   data::SymbolicNode : Input data to function.  label::SymbolicNode : Input label to function.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor   source  #  MXNet.mx.LogisticRegressionOutput     Method .  LogisticRegressionOutput(data, label, grad_scale)  Use Logistic regression for final output, this is used on final output of a net. Logistic regression is suitable for binary classification or probability prediction tasks.  Arguments   data::SymbolicNode : Input data to function.  label::SymbolicNode : Input label to function.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor   source  #  MXNet.mx.MAERegressionOutput     Method .  MAERegressionOutput(data, label, grad_scale)  Use mean absolute error regression for final output, this is used on final output of a net.  Arguments   data::SymbolicNode : Input data to function.  label::SymbolicNode : Input label to function.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor   source  #  MXNet.mx.MakeLoss     Method .  MakeLoss(data, grad_scale, valid_thresh, normalization)  Get output from a symbol and pass 1 gradient back. This is used as a terminal loss if unary and binary operator are used to composite a loss with no declaration of backward dependency  Arguments   data::SymbolicNode : Input data.  grad_scale::float, optional, default=1 : gradient scale as a supplement to unary and binary operators  valid_thresh::float, optional, default=0 : regard element valid when x   valid_thresh, this is used only in valid normalization mode.  normalization::{'batch', 'null', 'valid'},optional, default='null' : If set to null, op will not normalize on output gradient.If set to batch, op will normalize gradient by divide batch size.If set to valid, op will normalize gradient by divide # sample marked as valid   source  #  MXNet.mx.Pad     Method .  Pad(data, mode, pad_width, constant_value)  Pads an n-dimensional input tensor. Allows for precise control of the padding type and how much padding to apply on both sides of a given dimension.  Arguments   data::SymbolicNode : An n-dimensional input tensor.  mode::{'constant', 'edge'}, required : Padding type to use. \"constant\" pads all values with a constant value, the value of which can be specified with the constant_value option. \"edge\" uses the boundary values of the array as padding.  pad_width::Shape(tuple), required : A tuple of padding widths of length 2*r, where r is the rank of the input tensor, specifying number of values padded to the edges of each axis. (before_1, after_1, ... , before_N, after_N) unique pad widths for each axis. Equivalent to pad_width in numpy.pad, but flattened.  constant_value::double, optional, default=0 : This option is only used when mode is \"constant\". This value will be used as the padding value. Defaults to 0 if not specified.   source  #  MXNet.mx.Pooling     Method .  Pooling(data, global_pool, kernel, pool_type, pooling_convention, stride, pad)  Perform spatial pooling on inputs.  Arguments   data::SymbolicNode : Input data to the pooling operator.  global_pool::boolean, optional, default=False : Ignore kernel size, do global pooling based on current input feature map. This is useful for input with different shape  kernel::Shape(tuple), required : pooling kernel size: (y, x) or (d, y, x)  pool_type::{'avg', 'max', 'sum'}, required : Pooling type to be applied.  pooling_convention::{'full', 'valid'},optional, default='valid' : Pooling convention to be applied.kValid is default setting of Mxnet and rounds down the output pooling size.kFull is compatible with Caffe and rounds up the output pooling size.  stride::Shape(tuple), optional, default=(1,1) : stride: for pooling (y, x) or (d, y, x)  pad::Shape(tuple), optional, default=(0,0) : pad for pooling: (y, x) or (d, y, x)   source  #  MXNet.mx.RNN     Method .  RNN(data, parameters, state, state_cell, state_size, num_layers, bidirectional, mode, p, state_outputs)  Apply a recurrent layer to input.  Arguments   data::SymbolicNode : Input data to RNN  parameters::SymbolicNode : Vector of all RNN trainable parameters concatenated  state::SymbolicNode : initial hidden state of the RNN  state_cell::SymbolicNode : initial cell state for LSTM networks (only for LSTM)  state_size::int (non-negative), required : size of the state for each layer  num_layers::int (non-negative), required : number of stacked layers  bidirectional::boolean, optional, default=False : whether to use bidirectional recurrent layers  mode::{'gru', 'lstm', 'rnn_relu', 'rnn_tanh'}, required : the type of RNN to compute  p::float, optional, default=0 : Dropout probability, fraction of the input that gets dropped out at training time  state_outputs::boolean, optional, default=False : Whether to have the states as symbol outputs.   source  #  MXNet.mx.ROIPooling     Method .  ROIPooling(data, rois, pooled_size, spatial_scale)  Performs region-of-interest pooling on inputs. Resize bounding box coordinates by spatial_scale and crop input feature maps accordingly. The cropped feature maps are pooled by max pooling to a fixed size output indicated by pooled_size. batch_size will change to the number of region bounding boxes after ROIPooling  Arguments   data::SymbolicNode : Input data to the pooling operator, a 4D Feature maps  rois::SymbolicNode : Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]]. (x1, y1) and (x2, y2) are top left and down right corners of designated region of interest. batch_index indicates the index of corresponding image in the input data  pooled_size::Shape(tuple), required : fix pooled size: (h, w)  spatial_scale::float, required : Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers   source  #  MXNet.mx.Reshape     Method .  Reshape(data, target_shape, keep_highest, shape, reverse)  Reshape input according to a target shape spec. The target shape is a tuple and can be a simple list of dimensions such as (12,3) or it can incorporate special codes that correspond to contextual operations that refer to the input dimensions. The special codes are all expressed as integers less than 1. These codes effectively refer to a machine that pops input dims off the beginning of the input dims list and pushes resulting output dims onto the end of the output dims list, which starts empty. The codes are:   0  Copy     Pop one input dim and push it onto the output dims  -1  Infer    Push a dim that is inferred later from all other output dims  -2  CopyAll  Pop all remaining input dims and push them onto output dims  -3  Merge2   Pop two input dims, multiply them, and push result  -4  Split2   Pop one input dim, and read two next target shape specs,               push them both onto output dims (either can be -1 and will               be inferred from the other  The exact mathematical behavior of these codes is given in the description of the 'shape' parameter. All non-codes (positive integers) just pop a dim off the input dims (if any), throw it away, and then push the specified integer onto the output dims. Examples: Type     Input      Target            Output Copy     (2,3,4)    (4,0,2)           (4,3,2) Copy     (2,3,4)    (2,0,0)           (2,3,4) Infer    (2,3,4)    (6,1,-1)          (6,1,4) Infer    (2,3,4)    (3,-1,8)          (3,1,8) CopyAll  (9,8,7)    (-2)              (9,8,7) CopyAll  (9,8,7)    (9,-2)            (9,8,7) CopyAll  (9,8,7)    (-2,1,1)          (9,8,7,1,1) Merge2   (3,4)      (-3)              (12) Merge2   (3,4,5)    (-3,0)            (12,5) Merge2   (3,4,5)    (0,-3)            (3,20) Merge2   (3,4,5,6)  (-3,0,0)          (12,5,6) Merge2   (3,4,5,6)  (-3,-2)           (12,5,6) Split2   (12)       (-4,6,2)          (6,2) Split2   (12)       (-4,2,6)          (2,6) Split2   (12)       (-4,-1,6)         (2,6) Split2   (12,9)     (-4,2,6,0)        (2,6,9) Split2   (12,9,9,9) (-4,2,6,-2)       (2,6,9,9,9) Split2   (12,12)    (-4,2,-1,-4,-1,2) (2,6,6,2)  From:src/operator/tensor/matrix_op.cc:62  Arguments   data::NDArray : Input data to reshape.  target_shape::Shape(tuple), optional, default=(0,0) : (Deprecated! Use shape instead.) Target new shape. One and only one dim can be 0, in which case it will be inferred from the rest of dims  keep_highest::boolean, optional, default=False : (Deprecated! Use shape instead.) Whether keep the highest dim unchanged.If set to true, then the first dim in target_shape is ignored,and always fixed as input  shape::Shape(tuple), optional, default=() : Target shape, a tuple, t=(t_1,t_2,..,t_m).   Let the input dims be s=(s_1,s_2,..,s_n). The output dims u=(u_1,u_2,..,u_p) are computed from s and t. The target shape tuple elements t_i are read in order, and used to  generate successive output dims u_p: t_i:       meaning:      behavior: +ve        explicit      u_p = t_i 0          copy          u_p = s_i -1         infer         u_p = (Prod s_i) / (Prod u_j | j != p) -2         copy all      u_p = s_i, u_p+1 = s_i+1, ... -3         merge two     u_p = s_i * s_i+1 -4,a,b     split two     u_p = a, u_p+1 = b | a * b = s_i The split directive (-4) in the target shape tuple is followed by two dimensions, one of which can be -1, which means it will be inferred from the other one and the original dimension. The can only be one globally inferred dimension (-1), aside from any -1 occuring in a split directive.   reverse::boolean, optional, default=False : Whether to match the shapes from the backward. If reverse is true, 0 values in the  shape  argument will be searched from the backward. E.g the original shape is (10, 5, 4) and the shape argument is (-1, 0). If reverse is true, the new shape should be (50, 4). Otherwise it will be (40, 5).   source  #  MXNet.mx.SVMOutput     Method .  SVMOutput(data, label, margin, regularization_coefficient, use_linear)  Support Vector Machine based transformation on input, backprop L2-SVM  Arguments   data::SymbolicNode : Input data to svm.  label::SymbolicNode : Label data.  margin::float, optional, default=1 : Scale the DType(param_.margin) for activation size  regularization_coefficient::float, optional, default=1 : Scale the coefficient responsible for balacing coefficient size and error tradeoff  use_linear::boolean, optional, default=False : If set true, uses L1-SVM objective function. Default uses L2-SVM objective   source  #  MXNet.mx.SequenceLast     Method .  SequenceLast(data, sequence_length, use_sequence_length)  Takes the last element of a sequence. Takes an n-dimensional tensor of the form [max sequence length, batchsize, other dims] and returns a (n-1)-dimensional tensor of the form [batchsize, other dims]. This operator takes an optional input tensor sequence_length of positive ints of dimension [batchsize] when the sequence_length option is set to true. This allows the operator to handle variable-length sequences. If sequence_length is false, then each example in the batch is assumed to have the max sequence length.  Arguments   data::SymbolicNode : n-dimensional input tensor of the form [max sequence length, batchsize, other dims]  sequence_length::SymbolicNode : vector of sequence lengths of size batchsize  use_sequence_length::boolean, optional, default=False : If set to true, this layer takes in extra input sequence_length to specify variable length sequence   source  #  MXNet.mx.SequenceMask     Method .  SequenceMask(data, sequence_length, use_sequence_length, value)  Sets all elements outside the sequence to a constant value. Takes an n-dimensional tensor of the form [max sequence length, batchsize, other dims] and returns a tensor of the same shape. This operator takes an optional input tensor sequence_length of positive ints of dimension [batchsize] when the sequence_length option is set to true. This allows the operator to handle variable-length sequences. If sequence_length is false, then each example in the batch is assumed to have the max sequence length, and this operator becomes the identity operator.  Arguments   data::SymbolicNode : n-dimensional input tensor of the form [max sequence length, batchsize, other dims]  sequence_length::SymbolicNode : vector of sequence lengths of size batchsize  use_sequence_length::boolean, optional, default=False : If set to true, this layer takes in extra input sequence_length to specify variable length sequence  value::float, optional, default=0 : The value to be used as a mask.   source  #  MXNet.mx.SequenceReverse     Method .  SequenceReverse(data, sequence_length, use_sequence_length)  Reverses the elements of each sequence. Takes an n-dimensional tensor of the form [max sequence length, batchsize, other dims] and returns a tensor of the same shape. This operator takes an optional input tensor sequence_length of positive ints of dimension [batchsize] when the sequence_length option is set to true. This allows the operator to handle variable-length sequences. If sequence_length is false, then each example in the batch is assumed to have the max sequence length.  Arguments   data::SymbolicNode : n-dimensional input tensor of the form [max sequence length, batchsize, other dims]  sequence_length::SymbolicNode : vector of sequence lengths of size batchsize  use_sequence_length::boolean, optional, default=False : If set to true, this layer takes in extra input sequence_length to specify variable length sequence   source  #  MXNet.mx.SliceChannel     Method .  SliceChannel(num_outputs, axis, squeeze_axis)  Slice input equally along specified axis  Arguments   num_outputs::int, required : Number of outputs to be sliced.  axis::int, optional, default='1' : Dimension along which to slice.  squeeze_axis::boolean, optional, default=False : If true AND the sliced dimension becomes 1, squeeze that dimension.   source  #  MXNet.mx.Softmax     Method .  Softmax(data, grad_scale, ignore_label, multi_output, use_ignore, preserve_shape, normalization, out_grad)  DEPRECATED: Perform a softmax transformation on input. Please use SoftmaxOutput  Arguments   data::SymbolicNode : Input data to softmax.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor  ignore_label::float, optional, default=-1 : the label value will be ignored during backward (only works if use_ignore is set to be true).  multi_output::boolean, optional, default=False : If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n x_1 ...*x_n output, each has k classes  use_ignore::boolean, optional, default=False : If set to true, the ignore_label value will not contribute to the backward gradient  preserve_shape::boolean, optional, default=False : If true, for a (n_1, n_2, ..., n_d, k) dimensional input tensor, softmax will generate (n1, n2, ..., n_d, k) output, normalizing the k classes as the last dimension.  normalization::{'batch', 'null', 'valid'},optional, default='null' : If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored  out_grad::boolean, optional, default=False : Apply weighting from output gradient   source  #  MXNet.mx.SoftmaxActivation     Method .  SoftmaxActivation(data, mode)  Apply softmax activation to input. This is intended for internal layers. For output (loss layer) please use SoftmaxOutput. If mode=instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If mode=channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.  Arguments   data::SymbolicNode : Input data to activation function.  mode::{'channel', 'instance'},optional, default='instance' : Softmax Mode. If set to instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If set to channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.   source  #  MXNet.mx.SoftmaxOutput     Method .  SoftmaxOutput(data, label, grad_scale, ignore_label, multi_output, use_ignore, preserve_shape, normalization, out_grad)  Perform a softmax transformation on input, backprop with logloss.  Arguments   data::SymbolicNode : Input data to softmax.  label::SymbolicNode : Label data, can also be probability value with same shape as data  grad_scale::float, optional, default=1 : Scale the gradient by a float factor  ignore_label::float, optional, default=-1 : the label value will be ignored during backward (only works if use_ignore is set to be true).  multi_output::boolean, optional, default=False : If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n x_1 ...*x_n output, each has k classes  use_ignore::boolean, optional, default=False : If set to true, the ignore_label value will not contribute to the backward gradient  preserve_shape::boolean, optional, default=False : If true, for a (n_1, n_2, ..., n_d, k) dimensional input tensor, softmax will generate (n1, n2, ..., n_d, k) output, normalizing the k classes as the last dimension.  normalization::{'batch', 'null', 'valid'},optional, default='null' : If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored  out_grad::boolean, optional, default=False : Apply weighting from output gradient   source  #  MXNet.mx.SpatialTransformer     Method .  SpatialTransformer(data, loc, target_shape, transform_type, sampler_type)  Apply spatial transformer to input feature map.  Arguments   data::SymbolicNode : Input data to the SpatialTransformerOp.  loc::SymbolicNode : localisation net, the output dim should be 6 when transform_type is affine, and the name of loc symbol should better starts with 'stn_loc', so that initialization it with iddentify tranform, or you shold initialize the weight and bias by yourself.  target_shape::Shape(tuple), optional, default=(0,0) : output shape(h, w) of spatial transformer: (y, x)  transform_type::{'affine'}, required : transformation type  sampler_type::{'bilinear'}, required : sampling type   source  #  MXNet.mx.SwapAxis     Method .  SwapAxis(data, dim1, dim2)  Apply swapaxis to input.  Arguments   data::SymbolicNode : Input data to the SwapAxisOp.  dim1::int (non-negative), optional, default=0 : the first axis to be swapped.  dim2::int (non-negative), optional, default=0 : the second axis to be swapped.   source  #  MXNet.mx.UpSampling     Method .  UpSampling(data, scale, num_filter, sample_type, multi_input_mode, num_args, workspace)  Note : UpSampling takes variable number of positional inputs. So instead of calling as UpSampling([x, y, z], num_args=3), one should call via UpSampling(x, y, z), and num_args will be determined automatically.  Perform nearest neighboor/bilinear up sampling to inputs  Arguments   data::SymbolicNode[] : Array of tensors to upsample  scale::int (non-negative), required : Up sampling scale  num_filter::int (non-negative), optional, default=0 : Input filter. Only used by bilinear sample_type.  sample_type::{'bilinear', 'nearest'}, required : upsampling method  multi_input_mode::{'concat', 'sum'},optional, default='concat' : How to handle multiple input. concat means concatenate upsampled images along the channel dimension. sum means add all images together, only available for nearest neighbor upsampling.  num_args::int, required : Number of inputs to be upsampled. For nearest neighbor upsampling, this can be 1-N; the size of output will be(scale h_0,scale w_0) and all other inputs will be upsampled to thesame size. For bilinear upsampling this must be 2; 1 input and 1 weight.  workspace::long (non-negative), optional, default=512 : Tmp workspace for deconvolution (MB)   source  #  MXNet.mx._CrossDeviceCopy     Method .  _CrossDeviceCopy()  Special op to copy data cross device  Arguments  source  #  MXNet.mx._Div     Method .  _Div(lhs, rhs)  _Div is an alias of _div.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._DivScalar     Method .  _DivScalar(data, scalar)  _DivScalar is an alias of _div_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._Equal     Method .  _Equal(lhs, rhs)  _Equal is an alias of _equal.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._EqualScalar     Method .  _EqualScalar(data, scalar)  _EqualScalar is an alias of _equal_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._Greater     Method .  _Greater(lhs, rhs)  _Greater is an alias of _greater.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._GreaterEqualScalar     Method .  _GreaterEqualScalar(data, scalar)  _GreaterEqualScalar is an alias of _greater_equal_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._GreaterScalar     Method .  _GreaterScalar(data, scalar)  _GreaterScalar is an alias of _greater_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._Greater_Equal     Method .  _Greater_Equal(lhs, rhs)  _Greater_Equal is an alias of _greater_equal.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._Hypot     Method .  _Hypot(lhs, rhs)  _Hypot is an alias of _hypot.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._HypotScalar     Method .  _HypotScalar(data, scalar)  _HypotScalar is an alias of _hypot_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._Lesser     Method .  _Lesser(lhs, rhs)  _Lesser is an alias of _lesser.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._LesserEqualScalar     Method .  _LesserEqualScalar(data, scalar)  _LesserEqualScalar is an alias of _lesser_equal_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._LesserScalar     Method .  _LesserScalar(data, scalar)  _LesserScalar is an alias of _lesser_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._Lesser_Equal     Method .  _Lesser_Equal(lhs, rhs)  _Lesser_Equal is an alias of _lesser_equal.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._Maximum     Method .  _Maximum(lhs, rhs)  _Maximum is an alias of _maximum.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._MaximumScalar     Method .  _MaximumScalar(data, scalar)  _MaximumScalar is an alias of _maximum_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._Minimum     Method .  _Minimum(lhs, rhs)  _Minimum is an alias of _minimum.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._MinimumScalar     Method .  _MinimumScalar(data, scalar)  _MinimumScalar is an alias of _minimum_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._Minus     Method .  _Minus(lhs, rhs)  _Minus is an alias of _sub.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._MinusScalar     Method .  _MinusScalar(data, scalar)  _MinusScalar is an alias of _minus_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._Mul     Method .  _Mul(lhs, rhs)  _Mul is an alias of _mul.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._MulScalar     Method .  _MulScalar(data, scalar)  _MulScalar is an alias of _mul_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._NDArray     Method .  _NDArray(info)  Stub for implementing an operator implemented in native frontend language with ndarray.  Arguments   info::, required :   source  #  MXNet.mx._Native     Method .  _Native(info, need_top_grad)  Stub for implementing an operator implemented in native frontend language.  Arguments   info::, required :  need_top_grad::boolean, optional, default=True : Whether this layer needs out grad for backward. Should be false for loss layers.   source  #  MXNet.mx._NoGradient     Method .  _NoGradient()  Place holder for variable who cannot perform gradient  Arguments  source  #  MXNet.mx._NotEqualScalar     Method .  _NotEqualScalar(data, scalar)  _NotEqualScalar is an alias of _not_equal_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._Not_Equal     Method .  _Not_Equal(lhs, rhs)  _Not_Equal is an alias of _not_equal.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._Plus     Method .  _Plus(lhs, rhs)  _Plus is an alias of elemwise_add.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._PlusScalar     Method .  _PlusScalar(data, scalar)  _PlusScalar is an alias of _plus_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._Power     Method .  _Power(lhs, rhs)  _Power is an alias of _power.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._PowerScalar     Method .  _PowerScalar(data, scalar)  _PowerScalar is an alias of _power_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._RDivScalar     Method .  _RDivScalar(data, scalar)  _RDivScalar is an alias of _rdiv_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._RMinusScalar     Method .  _RMinusScalar(data, scalar)  _RMinusScalar is an alias of _rminus_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._RPowerScalar     Method .  _RPowerScalar(data, scalar)  _RPowerScalar is an alias of _rpower_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._add     Method .  _add(lhs, rhs)  _add is an alias of elemwise_add.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._arange     Method .  _arange(start, stop, step, repeat, ctx, dtype)  Return evenly spaced values within a given interval. Similar to Numpy  Arguments   start::float, required : Start of interval. The interval includes this value. The default start value is 0.  stop::, optional, default=None : End of interval. The interval does not include this value, except in some cases where step is not an integer and floating point round-off affects the length of out.  step::float, optional, default=1 : Spacing between values.  repeat::int, optional, default='1' : The repeating time of all elements. E.g repeat=3, the element a will be repeated three times \u2013  a, a, a.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned .Only used for imperative calls.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32' : Target data type.   source  #  MXNet.mx._backward_Activation     Method .  _backward_Activation()  Arguments  source  #  MXNet.mx._backward_BatchNorm     Method .  _backward_BatchNorm()  Arguments  source  #  MXNet.mx._backward_Cast     Method .  _backward_Cast()  Arguments  source  #  MXNet.mx._backward_Concat     Method .  _backward_Concat()  Arguments  source  #  MXNet.mx._backward_Convolution     Method .  _backward_Convolution()  Arguments  source  #  MXNet.mx._backward_Correlation     Method .  _backward_Correlation()  Arguments  source  #  MXNet.mx._backward_Crop     Method .  _backward_Crop()  Arguments  source  #  MXNet.mx._backward_Custom     Method .  _backward_Custom()  Arguments  source  #  MXNet.mx._backward_Deconvolution     Method .  _backward_Deconvolution()  Arguments  source  #  MXNet.mx._backward_Dropout     Method .  _backward_Dropout()  Arguments  source  #  MXNet.mx._backward_Embedding     Method .  _backward_Embedding()  Arguments  source  #  MXNet.mx._backward_FullyConnected     Method .  _backward_FullyConnected()  Arguments  source  #  MXNet.mx._backward_IdentityAttachKLSparseReg     Method .  _backward_IdentityAttachKLSparseReg()  Arguments  source  #  MXNet.mx._backward_InstanceNorm     Method .  _backward_InstanceNorm()  Arguments  source  #  MXNet.mx._backward_L2Normalization     Method .  _backward_L2Normalization()  Arguments  source  #  MXNet.mx._backward_LRN     Method .  _backward_LRN()  Arguments  source  #  MXNet.mx._backward_LeakyReLU     Method .  _backward_LeakyReLU()  Arguments  source  #  MXNet.mx._backward_LinearRegressionOutput     Method .  _backward_LinearRegressionOutput()  Arguments  source  #  MXNet.mx._backward_LogisticRegressionOutput     Method .  _backward_LogisticRegressionOutput()  Arguments  source  #  MXNet.mx._backward_MAERegressionOutput     Method .  _backward_MAERegressionOutput()  Arguments  source  #  MXNet.mx._backward_MakeLoss     Method .  _backward_MakeLoss()  Arguments  source  #  MXNet.mx._backward_Pad     Method .  _backward_Pad()  Arguments  source  #  MXNet.mx._backward_Pooling     Method .  _backward_Pooling()  Arguments  source  #  MXNet.mx._backward_RNN     Method .  _backward_RNN()  Arguments  source  #  MXNet.mx._backward_ROIPooling     Method .  _backward_ROIPooling()  Arguments  source  #  MXNet.mx._backward_SVMOutput     Method .  _backward_SVMOutput()  Arguments  source  #  MXNet.mx._backward_SequenceLast     Method .  _backward_SequenceLast()  Arguments  source  #  MXNet.mx._backward_SequenceMask     Method .  _backward_SequenceMask()  Arguments  source  #  MXNet.mx._backward_SequenceReverse     Method .  _backward_SequenceReverse()  Arguments  source  #  MXNet.mx._backward_SliceChannel     Method .  _backward_SliceChannel()  Arguments  source  #  MXNet.mx._backward_Softmax     Method .  _backward_Softmax()  Arguments  source  #  MXNet.mx._backward_SoftmaxActivation     Method .  _backward_SoftmaxActivation()  Arguments  source  #  MXNet.mx._backward_SoftmaxOutput     Method .  _backward_SoftmaxOutput()  Arguments  source  #  MXNet.mx._backward_SpatialTransformer     Method .  _backward_SpatialTransformer()  Arguments  source  #  MXNet.mx._backward_SwapAxis     Method .  _backward_SwapAxis()  Arguments  source  #  MXNet.mx._backward_UpSampling     Method .  _backward_UpSampling()  Arguments  source  #  MXNet.mx._backward__CrossDeviceCopy     Method .  _backward__CrossDeviceCopy()  Arguments  source  #  MXNet.mx._backward__NDArray     Method .  _backward__NDArray()  Arguments  source  #  MXNet.mx._backward__Native     Method .  _backward__Native()  Arguments  source  #  MXNet.mx._backward_abs     Method .  _backward_abs(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_add     Method .  _backward_add()  Arguments  source  #  MXNet.mx._backward_arccos     Method .  _backward_arccos(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_arccosh     Method .  _backward_arccosh(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_arcsin     Method .  _backward_arcsin(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_arcsinh     Method .  _backward_arcsinh(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_arctan     Method .  _backward_arctan(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_arctanh     Method .  _backward_arctanh(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_batch_dot     Method .  _backward_batch_dot()  Arguments  source  #  MXNet.mx._backward_broadcast_add     Method .  _backward_broadcast_add()  Arguments  source  #  MXNet.mx._backward_broadcast_div     Method .  _backward_broadcast_div()  Arguments  source  #  MXNet.mx._backward_broadcast_hypot     Method .  _backward_broadcast_hypot()  Arguments  source  #  MXNet.mx._backward_broadcast_maximum     Method .  _backward_broadcast_maximum()  Arguments  source  #  MXNet.mx._backward_broadcast_minimum     Method .  _backward_broadcast_minimum()  Arguments  source  #  MXNet.mx._backward_broadcast_mul     Method .  _backward_broadcast_mul()  Arguments  source  #  MXNet.mx._backward_broadcast_power     Method .  _backward_broadcast_power()  Arguments  source  #  MXNet.mx._backward_broadcast_sub     Method .  _backward_broadcast_sub()  Arguments  source  #  MXNet.mx._backward_clip     Method .  _backward_clip()  Arguments  source  #  MXNet.mx._backward_copy     Method .  _backward_copy()  Arguments  source  #  MXNet.mx._backward_cos     Method .  _backward_cos(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_cosh     Method .  _backward_cosh(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_degrees     Method .  _backward_degrees(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_div     Method .  _backward_div()  Arguments  source  #  MXNet.mx._backward_dot     Method .  _backward_dot(transpose_a, transpose_b)  Arguments   transpose_a::boolean, optional, default=False : True if the first matrix is transposed.  transpose_b::boolean, optional, default=False : True if the second matrix is tranposed.   source  #  MXNet.mx._backward_expm1     Method .  _backward_expm1(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_gamma     Method .  _backward_gamma(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_gammaln     Method .  _backward_gammaln(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_hypot     Method .  _backward_hypot()  Arguments  source  #  MXNet.mx._backward_hypot_scalar     Method .  _backward_hypot_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  scalar::float : scalar value   source  #  MXNet.mx._backward_log     Method .  _backward_log(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_log1p     Method .  _backward_log1p(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_max     Method .  _backward_max()  Arguments  source  #  MXNet.mx._backward_maximum     Method .  _backward_maximum()  Arguments  source  #  MXNet.mx._backward_maximum_scalar     Method .  _backward_maximum_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  scalar::float : scalar value   source  #  MXNet.mx._backward_mean     Method .  _backward_mean()  Arguments  source  #  MXNet.mx._backward_min     Method .  _backward_min()  Arguments  source  #  MXNet.mx._backward_minimum     Method .  _backward_minimum()  Arguments  source  #  MXNet.mx._backward_minimum_scalar     Method .  _backward_minimum_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  scalar::float : scalar value   source  #  MXNet.mx._backward_mul     Method .  _backward_mul()  Arguments  source  #  MXNet.mx._backward_nanprod     Method .  _backward_nanprod()  Arguments  source  #  MXNet.mx._backward_nansum     Method .  _backward_nansum()  Arguments  source  #  MXNet.mx._backward_power     Method .  _backward_power()  Arguments  source  #  MXNet.mx._backward_power_scalar     Method .  _backward_power_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  scalar::float : scalar value   source  #  MXNet.mx._backward_prod     Method .  _backward_prod()  Arguments  source  #  MXNet.mx._backward_radians     Method .  _backward_radians(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_rdiv_scalar     Method .  _backward_rdiv_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  scalar::float : scalar value   source  #  MXNet.mx._backward_rpower_scalar     Method .  _backward_rpower_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  scalar::float : scalar value   source  #  MXNet.mx._backward_rsqrt     Method .  _backward_rsqrt(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_sign     Method .  _backward_sign(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_sin     Method .  _backward_sin(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_sinh     Method .  _backward_sinh(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_slice_axis     Method .  _backward_slice_axis()  Arguments  source  #  MXNet.mx._backward_smooth_l1     Method .  _backward_smooth_l1(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_softmax_cross_entropy     Method .  _backward_softmax_cross_entropy()  Arguments  source  #  MXNet.mx._backward_sqrt     Method .  _backward_sqrt(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_square     Method .  _backward_square(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_sub     Method .  _backward_sub()  Arguments  source  #  MXNet.mx._backward_sum     Method .  _backward_sum()  Arguments  source  #  MXNet.mx._backward_take     Method .  _backward_take()  Arguments  source  #  MXNet.mx._backward_tan     Method .  _backward_tan(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_tanh     Method .  _backward_tanh(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._backward_topk     Method .  _backward_topk()  Arguments  source  #  MXNet.mx._broadcast     Method .  _broadcast(src, axis, size)  Broadcast array in the given axis to the given size  Arguments   src::NDArray : source ndarray  axis::int : axis to broadcast  size::int : size of broadcast   source  #  MXNet.mx._broadcast_backward     Method .  _broadcast_backward()  Arguments  source  #  MXNet.mx._copy     Method .  _copy(data)  Identity mapping, copy src to output  From:src/operator/tensor/elemwise_unary_op.cc:14  Arguments   data::NDArray : Source input   source  #  MXNet.mx._copyto     Method .  _copyto(src)  Arguments   src::NDArray : Source input to the function.   source  #  MXNet.mx._crop_assign     Method .  _crop_assign(lhs, rhs, begin, end)  (Assign the rhs to a cropped subset of lhs.  Requirements   output should be explicitly given and be the same as lhs.  lhs and rhs are of the same data type, and on the same device.   )  From:src/operator/tensor/matrix_op.cc:159  Arguments   lhs::NDArray : Source input  rhs::NDArray : value to assign  begin::Shape(tuple), required : starting coordinates  end::Shape(tuple), required : ending coordinates   source  #  MXNet.mx._crop_assign_scalar     Method .  _crop_assign_scalar(data, scalar, begin, end)  (Assign the scalar to a cropped subset of the input.  Requirements   output should be explicitly given and be the same as input   )  From:src/operator/tensor/matrix_op.cc:183  Arguments   data::NDArray : Source input  scalar::float, optional, default=0 : The scalar value for assignment.  begin::Shape(tuple), required : starting coordinates  end::Shape(tuple), required : ending coordinates   source  #  MXNet.mx._cvcopyMakeBorder     Method .  _cvcopyMakeBorder(src, top, bot, left, right, type, value)  Pad image border with OpenCV.   Arguments   src::NDArray : source image  top::int, required : Top margin.  bot::int, required : Bottom margin.  left::int, required : Left margin.  right::int, required : Right margin.  type::int, optional, default='0' : Filling type (default=cv2.BORDER_CONSTANT).  value::double, optional, default=0 : Fill with value.   source  #  MXNet.mx._cvimdecode     Method .  _cvimdecode(buf, flag, to_rgb)  Decode image with OpenCV.  Note: return image in RGB by default, instead of OpenCV's default BGR.  Arguments   buf::NDArray : Buffer containing binary encoded image  flag::int, optional, default='1' : Convert decoded image to grayscale (0) or color (1).  to_rgb::boolean, optional, default=True : Whether to convert decoded image to mxnet's default RGB format (instead of opencv's default BGR).   source  #  MXNet.mx._cvimresize     Method .  _cvimresize(src, w, h, interp)  Resize image with OpenCV.   Arguments   src::NDArray : source image  w::int, required : Width of resized image.  h::int, required : Height of resized image.  interp::int, optional, default='1' : Interpolation method (default=cv2.INTER_LINEAR).   source  #  MXNet.mx._div_scalar     Method .  _div_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._equal     Method .  _equal(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._equal_scalar     Method .  _equal_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._get_ndarray_function_def     Method .  The libxmnet APIs are automatically imported from  libmxnet.so . The functions listed here operate on  NDArray  objects. The arguments to the functions are typically ordered as    func_name(arg_in1, arg_in2, ..., scalar1, scalar2, ..., arg_out1, arg_out2, ...)  unless  NDARRAY_ARG_BEFORE_SCALAR  is not set. In this case, the scalars are put before the input arguments:    func_name(scalar1, scalar2, ..., arg_in1, arg_in2, ..., arg_out1, arg_out2, ...)  If  ACCEPT_EMPTY_MUTATE_TARGET  is set. An overloaded function without the output arguments will also be defined:    func_name(arg_in1, arg_in2, ..., scalar1, scalar2, ...)  Upon calling, the output arguments will be automatically initialized with empty NDArrays.  Those functions always return the output arguments. If there is only one output (the typical situation), that object ( NDArray ) is returned. Otherwise, a tuple containing all the outputs will be returned.  source  #  MXNet.mx._grad_add     Method .  _grad_add(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._greater     Method .  _greater(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._greater_equal     Method .  _greater_equal(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._greater_equal_scalar     Method .  _greater_equal_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._greater_scalar     Method .  _greater_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._hypot     Method .  _hypot(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._hypot_scalar     Method .  _hypot_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._identity_with_attr_like_rhs     Method .  _identity_with_attr_like_rhs()  Arguments  source  #  MXNet.mx._imdecode     Method .  _imdecode(mean, index, x0, y0, x1, y1, c, size)  Decode an image, clip to (x0, y0, x1, y1), subtract mean, and write to buffer  Arguments   mean::NDArray : image mean  index::int : buffer position for output  x0::int : x0  y0::int : y0  x1::int : x1  y1::int : y1  c::int : channel  size::int : length of str_img   source  #  MXNet.mx._lesser     Method .  _lesser(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._lesser_equal     Method .  _lesser_equal(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._lesser_equal_scalar     Method .  _lesser_equal_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._lesser_scalar     Method .  _lesser_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._maximum     Method .  _maximum(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._maximum_scalar     Method .  _maximum_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._minimum     Method .  _minimum(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._minimum_scalar     Method .  _minimum_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._minus     Method .  _minus(lhs, rhs)  _minus is an alias of _sub.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._minus_scalar     Method .  _minus_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._mul     Method .  _mul(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._mul_scalar     Method .  _mul_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._not_equal     Method .  _not_equal(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._not_equal_scalar     Method .  _not_equal_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._onehot_encode     Method .  _onehot_encode(lhs, rhs)  Arguments   lhs::NDArray : Left operand to the function.  rhs::NDArray : Right operand to the function.   source  #  MXNet.mx._ones     Method .  _ones(shape, ctx, dtype)  fill target with ones  Arguments   shape::Shape(tuple), optional, default=() : The shape of the output  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned .Only used for imperative calls.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32' : Target data type.   source  #  MXNet.mx._plus     Method .  _plus(lhs, rhs)  _plus is an alias of elemwise_add.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._plus_scalar     Method .  _plus_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._power     Method .  _power(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx._power_scalar     Method .  _power_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._rdiv_scalar     Method .  _rdiv_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._rminus_scalar     Method .  _rminus_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._rpower_scalar     Method .  _rpower_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx._sample_normal     Method .  _sample_normal(loc, scale, shape, ctx, dtype)  _sample_normal is an alias of normal.  Sample a normal distribution  Arguments   loc::float, optional, default=0 : Mean of the distribution.  scale::float, optional, default=1 : Standard deviation of the distribution.  shape::Shape(tuple), optional, default=() : The shape of the output  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned .Only used for imperative calls.  dtype::{'float32'},optional, default='float32' : DType of the output   source  #  MXNet.mx._sample_uniform     Method .  _sample_uniform(low, high, shape, ctx, dtype)  _sample_uniform is an alias of uniform.  Sample a uniform distribution  Arguments   low::float, optional, default=0 : The lower bound of distribution  high::float, optional, default=1 : The upper bound of distribution  shape::Shape(tuple), optional, default=() : The shape of the output  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned .Only used for imperative calls.  dtype::{'float32'},optional, default='float32' : DType of the output   source  #  MXNet.mx._set_value     Method .  _set_value(src)  Arguments   src::real_t : Source input to the function.   source  #  MXNet.mx._zeros     Method .  _zeros(shape, ctx, dtype)  fill target with zeros  Arguments   shape::Shape(tuple), optional, default=() : The shape of the output  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned .Only used for imperative calls.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32' : Target data type.   source  #  MXNet.mx.adam_update     Method .  adam_update()  Updater function for adam optimizer  Arguments  source  #  MXNet.mx.add_to!     Method .  add_to!(dst :: NDArray, args :: Union{Real, NDArray}...)  Add a bunch of arguments into  dst . Inplace updating.  source  #  MXNet.mx.arccos     Method .  arccos(data)  Take arccos of the src  From:src/operator/tensor/elemwise_unary_op.cc:216  Arguments   data::NDArray : Source input   source  #  MXNet.mx.arccosh     Method .  arccosh(data)  Take arccosh of the src  From:src/operator/tensor/elemwise_unary_op.cc:288  Arguments   data::NDArray : Source input   source  #  MXNet.mx.arcsin     Method .  arcsin(data)  Take arcsin of the src  From:src/operator/tensor/elemwise_unary_op.cc:207  Arguments   data::NDArray : Source input   source  #  MXNet.mx.arcsinh     Method .  arcsinh(data)  Take arcsinh of the src  From:src/operator/tensor/elemwise_unary_op.cc:279  Arguments   data::NDArray : Source input   source  #  MXNet.mx.arctan     Method .  arctan(data)  Take arctan of the src  From:src/operator/tensor/elemwise_unary_op.cc:225  Arguments   data::NDArray : Source input   source  #  MXNet.mx.arctanh     Method .  arctanh(data)  Take arctanh of the src  From:src/operator/tensor/elemwise_unary_op.cc:297  Arguments   data::NDArray : Source input   source  #  MXNet.mx.argmax     Method .  argmax(data, axis, keepdims)  Compute argmax  From:src/operator/tensor/broadcast_reduce_op_index.cc:11  Arguments   data::NDArray : Source input  axis::int, optional, default='-1' : Empty or unsigned. The axis to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.   source  #  MXNet.mx.argmax_channel     Method .  argmax_channel(src)  Arguments   src::NDArray : Source input   source  #  MXNet.mx.argmin     Method .  argmin(data, axis, keepdims)  Compute argmin  From:src/operator/tensor/broadcast_reduce_op_index.cc:16  Arguments   data::NDArray : Source input  axis::int, optional, default='-1' : Empty or unsigned. The axis to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.   source  #  MXNet.mx.argsort     Method .  argsort(src, axis, is_ascend)  Returns the indices that would sort an array.  From:src/operator/tensor/ordering_op.cc:89  Arguments   src::NDArray : Source input  axis::int or None, optional, default='-1' : Axis along which to sort the input tensor. If not given, the flattened array is used. Default is -1.  is_ascend::boolean, optional, default=True : Whether sort in ascending or descending order.   source  #  MXNet.mx.batch_dot     Method .  batch_dot(lhs, rhs, transpose_a, transpose_b)  Calculate batched dot product of two matrices. (batch, M, K) X (batch, K, N) \u2013  (batch, M, N).  From:src/operator/tensor/matrix_op.cc:257  Arguments   lhs::NDArray : Left input  rhs::NDArray : Right input  transpose_a::boolean, optional, default=False : True if the first matrix is transposed.  transpose_b::boolean, optional, default=False : True if the second matrix is tranposed.   source  #  MXNet.mx.batch_take     Method .  batch_take(a, indices)  Take scalar value from a batch of data vectos according to an index vector, i.e. out[i] = a[i, indices[i]]  From:src/operator/tensor/indexing_op.cc:97  Arguments   a::NDArray : Input data array  indices::NDArray : index array   source  #  MXNet.mx.broadcast_add     Method .  broadcast_add(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx.broadcast_axis     Method .  broadcast_axis(data, axis, size)  Broadcast src along axis  From:src/operator/tensor/broadcast_reduce_op_value.cc:85  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : The axes to perform the broadcasting.  size::Shape(tuple), optional, default=() : Target sizes of the broadcasting axes.   source  #  MXNet.mx.broadcast_div     Method .  broadcast_div(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx.broadcast_equal     Method .  broadcast_equal(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx.broadcast_greater     Method .  broadcast_greater(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx.broadcast_greater_equal     Method .  broadcast_greater_equal(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx.broadcast_hypot     Method .  broadcast_hypot(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx.broadcast_lesser     Method .  broadcast_lesser(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx.broadcast_lesser_equal     Method .  broadcast_lesser_equal(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx.broadcast_maximum     Method .  broadcast_maximum(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx.broadcast_minimum     Method .  broadcast_minimum(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx.broadcast_minus     Method .  broadcast_minus(lhs, rhs)  broadcast_minus is an alias of broadcast_sub.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx.broadcast_mul     Method .  broadcast_mul(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx.broadcast_not_equal     Method .  broadcast_not_equal(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx.broadcast_plus     Method .  broadcast_plus(lhs, rhs)  broadcast_plus is an alias of broadcast_add.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx.broadcast_power     Method .  broadcast_power(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx.broadcast_sub     Method .  broadcast_sub(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx.broadcast_to     Method .  broadcast_to(data, shape)  Broadcast src to shape  From:src/operator/tensor/broadcast_reduce_op_value.cc:92  Arguments   data::NDArray : Source input  shape::Shape(tuple), optional, default=() : The shape of the desired array. We can set the dim to zero if it's same as the original. E.g  A = broadcast_to(B, shape=(10, 0, 0))  has the same meaning as  A = broadcast_axis(B, axis=0, size=10) .   source  #  MXNet.mx.choose_element_0index     Method .  choose_element_0index(lhs, rhs)  Choose one element from each line(row for python, column for R/Julia) in lhs according to index indicated by rhs. This function assume rhs uses 0-based index.  Arguments   lhs::NDArray : Left operand to the function.  rhs::NDArray : Right operand to the function.   source  #  MXNet.mx.clip     Method .  clip(data, a_min, a_max)  Clip ndarray elements to range (a_min, a_max)  From:src/operator/tensor/matrix_op.cc:289  Arguments   data::NDArray : Source input  a_min::float, required : Minimum value  a_max::float, required : Maximum value   source  #  MXNet.mx.context     Method .  context(arr :: NDArray)  Get the context that this  NDArray  lives on.  source  #  MXNet.mx.crop     Method .  crop(data, begin, end)  (Crop the input tensor and return a new one.  Requirements   the input and output (if explicitly given) are of the same data type, and on the same device.   )  From:src/operator/tensor/matrix_op.cc:143  Arguments   data::NDArray : Source input  begin::Shape(tuple), required : starting coordinates  end::Shape(tuple), required : ending coordinates   source  #  MXNet.mx.degrees     Method .  degrees(data)  Take degrees of the src  From:src/operator/tensor/elemwise_unary_op.cc:234  Arguments   data::NDArray : Source input   source  #  MXNet.mx.div_from!     Method .  div_from!(dst :: NDArray, arg :: Union{Real, NDArray})  Elementwise divide a scalar or an  NDArray  of the same shape from  dst . Inplace updating.  source  #  MXNet.mx.elemwise_add     Method .  elemwise_add(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input   source  #  MXNet.mx.empty     Method .  empty(shape :: Tuple, ctx :: Context)\nempty(shape :: Tuple)\nempty(dim1, dim2, ...)  Allocate memory for an uninitialized  NDArray  with specific shape of type Float32.  source  #  MXNet.mx.empty     Method .  empty(DType, shape :: Tuple, ctx :: Context)\nempty(DType, shape :: Tuple)\nempty(DType, dim1, dim2, ...)  Allocate memory for an uninitialized  NDArray  with a specified type.  source  #  MXNet.mx.expand_dims     Method .  expand_dims(data, axis)  Expand the shape of array by inserting a new axis.  From:src/operator/tensor/matrix_op.cc:122  Arguments   data::NDArray : Source input  axis::int (non-negative), required : Position (amongst axes) where new axis is to be inserted.   source  #  MXNet.mx.fill_element_0index     Method .  fill_element_0index(lhs, mhs, rhs)  Fill one element of each line(row for python, column for R/Julia) in lhs according to index indicated by rhs and values indicated by mhs. This function assume rhs uses 0-based index.  Arguments   lhs::NDArray : Left operand to the function.  mhs::NDArray : Middle operand to the function.  rhs::NDArray : Right operand to the function.   source  #  MXNet.mx.fix     Method .  fix(data)  Take round of the src to integer nearest 0  From:src/operator/tensor/elemwise_unary_op.cc:102  Arguments   data::NDArray : Source input   source  #  MXNet.mx.flip     Method .  flip(data, axis)  Flip the input tensor along axis and return a new one.  From:src/operator/tensor/matrix_op.cc:219  Arguments   data::NDArray : Source input  axis::int, required : The dimension to flip   source  #  MXNet.mx.gammaln     Method .  gammaln(data)  Take gammaln (log of the absolute value of gamma(x)) of the src  From:src/operator/tensor/elemwise_unary_op.cc:315  Arguments   data::NDArray : Source input   source  #  MXNet.mx.is_shared     Method .  is_shared(j_arr, arr)  Test whether  j_arr  is sharing data with  arr .  Arguments:   Array j_arr: the Julia Array.  NDArray arr: the  NDArray .   source  #  MXNet.mx.load     Method .  load(filename, ::Type{NDArray})  Load NDArrays from binary file.  Arguments:   filename::String : the path of the file to load. It could be S3 or HDFS address.   Returns either  Dict{Symbol, NDArray}  or  Vector{NDArray} .  filename  can point to  s3  or  hdfs  resources if the  libmxnet  is built with the corresponding components enabled. Examples:   s3://my-bucket/path/my-s3-ndarray  hdfs://my-bucket/path/my-hdfs-ndarray  /path-to/my-local-ndarray   source  #  MXNet.mx.max_axis     Method .  max_axis(data, axis, keepdims)  max_axis is an alias of max.  Compute max along axis. If axis is empty, global reduction is performed  From:src/operator/tensor/broadcast_reduce_op_value.cc:66  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.   source  #  MXNet.mx.min_axis     Method .  min_axis(data, axis, keepdims)  min_axis is an alias of min.  Compute min along axis. If axis is empty, global reduction is performed  From:src/operator/tensor/broadcast_reduce_op_value.cc:76  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.   source  #  MXNet.mx.mul_to!     Method .  mul_to!(dst :: NDArray, arg :: Union{Real, NDArray})  Elementwise multiplication into  dst  of either a scalar or an  NDArray  of the same shape. Inplace updating.  source  #  MXNet.mx.nanprod     Method .  nanprod(data, axis, keepdims)  Compute product of src along axis, ignoring NaN values. If axis is empty, global reduction is performed  From:src/operator/tensor/broadcast_reduce_op_value.cc:56  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.   source  #  MXNet.mx.nansum     Method .  nansum(data, axis, keepdims)  Sum src along axis, ignoring NaN values. If axis is empty, global reduction is performed  From:src/operator/tensor/broadcast_reduce_op_value.cc:46  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.   source  #  MXNet.mx.negative     Method .  negative(data)  Negate src  From:src/operator/tensor/elemwise_unary_op.cc:57  Arguments   data::NDArray : Source input   source  #  MXNet.mx.normal     Method .  normal(loc, scale, shape, ctx, dtype)  Sample a normal distribution  Arguments   loc::float, optional, default=0 : Mean of the distribution.  scale::float, optional, default=1 : Standard deviation of the distribution.  shape::Shape(tuple), optional, default=() : The shape of the output  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned .Only used for imperative calls.  dtype::{'float32'},optional, default='float32' : DType of the output   source  #  MXNet.mx.ones     Method .  ones(shape :: Tuple, ctx :: Context)\nones(shape :: Tuple)\nones(dim1, dim2, ...)  Create an  NDArray  with specific shape and initialize with 1.  source  #  MXNet.mx.ones     Method .  ones(DType, shape :: Tuple, ctx :: Context)\nones(DType, shape :: Tuple)\nones(DType, dim1, dim2, ...)  Create an  NDArray  with specific shape   type, and initialize with 1.  source  #  MXNet.mx.radians     Method .  radians(data)  Take radians of the src  From:src/operator/tensor/elemwise_unary_op.cc:243  Arguments   data::NDArray : Source input   source  #  MXNet.mx.rint     Method .  rint(data)  Take round of the src to nearest integer  From:src/operator/tensor/elemwise_unary_op.cc:97  Arguments   data::NDArray : Source input   source  #  MXNet.mx.rsqrt     Method .  rsqrt(data)  Take reciprocal square root of the src  From:src/operator/tensor/elemwise_unary_op.cc:125  Arguments   data::NDArray : Source input   source  #  MXNet.mx.save     Method .  save(filename :: AbstractString, data)  Save NDarrays to binary file. Filename could be S3 or HDFS address, if  libmxnet  is built with corresponding support (see  load ).   filename::String : path to the binary file to write to.  data : data to save to file. Data can be a NDArray , a  Vector{NDArray} , or a  Dict{Base.Symbol, NDArray} .   source  #  MXNet.mx.sgd_mom_update     Method .  sgd_mom_update()  Updater function for sgd optimizer  Arguments  source  #  MXNet.mx.sgd_update     Method .  sgd_update()  Updater function for sgd optimizer  Arguments  source  #  MXNet.mx.slice_axis     Method .  slice_axis(data, axis, begin, end)  Slice the input along certain axis and return a sliced array. The slice will be taken from [begin, end). end can be None and axis can be negative.  From:src/operator/tensor/matrix_op.cc:200  Arguments   data::NDArray : Source input  axis::int, required : The axis to be sliced. Negative axis means to count from the last to the first axis.  begin::int, required : The beginning index to be sliced. Negative values are interpreted as counting from the backward.  end::int or None, required : The end index to be sliced. The end can be None, in which case all the rest elements are used. Also, negative values are interpreted as counting from the backward.   source  #  MXNet.mx.smooth_l1     Method .  smooth_l1(data, scalar)  Calculate Smooth L1 Loss(lhs, scalar)  From:src/operator/tensor/elemwise_binary_scalar_op_extended.cc:63  Arguments   data::NDArray : source input  scalar::float : scalar input   source  #  MXNet.mx.softmax_cross_entropy     Method .  softmax_cross_entropy(data, label)  Calculate cross_entropy(data, one_hot(label))  From:src/operator/loss_binary_op.cc:12  Arguments   data::NDArray : Input data  label::NDArray : Input label   source  #  MXNet.mx.square     Method .  square(data)  Take square of the src  From:src/operator/tensor/elemwise_unary_op.cc:107  Arguments   data::NDArray : Source input   source  #  MXNet.mx.sub_from!     Method .  sub_from!(dst :: NDArray, args :: Union{Real, NDArray}...)  Subtract a bunch of arguments from  dst . Inplace updating.  source  #  MXNet.mx.sum_axis     Method .  sum_axis(data, axis, keepdims)  sum_axis is an alias of sum.  Sum src along axis. If axis is empty, global reduction is performed  From:src/operator/tensor/broadcast_reduce_op_value.cc:17  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.   source  #  MXNet.mx.topk     Method .  topk(src, axis, k, ret_typ, is_ascend)  Return the top k element of an input tensor along a given axis.  From:src/operator/tensor/ordering_op.cc:18  Arguments   src::NDArray : Source input  axis::int or None, optional, default='-1' : Axis along which to choose the top k indices. If not given, the flattened array is used. Default is -1.  k::int, optional, default='1' : Number of top elements to select, should be always smaller than or equal to the element number in the given axis. A global sort is performed if set k   1.  ret_typ::{'both', 'indices', 'mask', 'value'},optional, default='indices' : The return type. \"value\" means returning the top k values, \"indices\" means returning the indices of the top k values, \"mask\" means to return a mask array containing 0 and 1. 1 means the top k values. \"both\" means to return both value and indices.  is_ascend::boolean, optional, default=False : Whether to choose k largest or k smallest. Top K largest elements will be chosen if set to false.   source  #  MXNet.mx.try_get_shared     Method .  try_get_shared(arr)  Try to create a Julia array by sharing the data with the underlying  NDArray .  Arguments:   arr::NDArray : the array to be shared.    Note  The returned array does not guarantee to share data with the underlying  NDArray . In particular, data sharing is possible only when the  NDArray  lives on CPU.   source  #  MXNet.mx.uniform     Method .  uniform(low, high, shape, ctx, dtype)  Sample a uniform distribution  Arguments   low::float, optional, default=0 : The lower bound of distribution  high::float, optional, default=1 : The upper bound of distribution  shape::Shape(tuple), optional, default=() : The shape of the output  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned .Only used for imperative calls.  dtype::{'float32'},optional, default='float32' : DType of the output   source  #  MXNet.mx.zeros     Method .  zeros(shape :: Tuple, ctx :: Context)\nzeros(shape :: Tuple)\nzeros(dim1, dim2, ...)  Create zero-ed  NDArray  with specific shape.  source  #  MXNet.mx.zeros     Method .  zeros(DType, shape :: Tuple, ctx :: Context)\nzeros(DType, shape :: Tuple)\nzeros(DType, dim1, dim2, ...)  Create zero-ed  NDArray  with specific shape and type  source  #  MXNet.mx.@inplace     Macro .  @inplace  Julia does not support re-definiton of  +=  operator (like  __iadd__  in python), When one write  a += b , it gets translated to  a = a+b .  a+b  will allocate new memory for the results, and the newly allocated  NDArray  object is then assigned back to a, while the original contents in a is discarded. This is very inefficient when we want to do inplace update.  This macro is a simple utility to implement this behavior. Write    @mx.inplace a += b  will translate into    mx.add_to!(a, b)  which will do inplace adding of the contents of  b  into  a .  source  #  MXNet.mx.@nd_as_jl     Macro .  Manipulating as Julia Arrays  @nd_as_jl(captures..., statement)  A convenient macro that allows to operate  NDArray  as Julia Arrays. For example,    x = mx.zeros(3,4)\n  y = mx.ones(3,4)\n  z = mx.zeros((3,4), mx.gpu())\n\n  @mx.nd_as_jl ro=(x,y) rw=z begin\n    # now x, y, z are just ordinary Julia Arrays\n    z[:,1] = y[:,2]\n    z[:,2] = 5\n  end  Under the hood, the macro convert all the declared captures from  NDArray  into Julia Arrays, by using  try_get_shared . And automatically commit the modifications back into the  NDArray  that is declared as  rw . This is useful for fast prototyping and when implement non-critical computations, such as  AbstractEvalMetric .   Note    Multiple  rw  and / or  ro  capture declaration could be made.  The macro does  not  check to make sure that  ro  captures are not modified. If the original  NDArray  lives in CPU memory, then it is very likely the corresponding Julia Array shares data with the  NDArray , so modifying the Julia Array will also modify the underlying  NDArray .  More importantly, since the  NDArray  is asynchronized, we will wait for  writing  for  rw  variables but wait only for  reading  in  ro  variables. If we write into those  ro  variables,  and  if the memory is shared, racing condition might happen, and the behavior is undefined.  When an  NDArray  is declared to be captured as  rw , its contents is always sync back in the end.  The execution results of the expanded macro is always  nothing .  The statements are wrapped in a  let , thus locally introduced new variables will not be available after the statements. So you will need to declare the variables before calling the macro if needed.   source", 
            "title": "NDArray API"
        }, 
        {
            "location": "/api/symbolic-node/", 
            "text": "Symbolic API\n\n\n#\n\n\nBase.Flatten\n \n \nMethod\n.\n\n\nFlatten(data)\n\n\n\n\nFlatten input into 2D by collapsing all the higher dimensions. A (d1, d2, ..., dK) tensor is flatten to (d1, d2\n ... \ndK) matrix.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Input data to reshape.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SymbolicNode\n \n \nType\n.\n\n\nSymbolicNode\n\n\n\n\nSymbolicNode is the basic building block of the symbolic graph in MXNet.jl.\n\n\n(self :: SymbolicNode)(args :: SymbolicNode...)\n(self :: SymbolicNode)(; kwargs...)\n\n\n\n\nMake a new node by composing \nself\n with \nargs\n. Or the arguments can be specified using keyword arguments.\n\n\nsource\n\n\n#\n\n\nBase.LinAlg.dot\n \n \nMethod\n.\n\n\ndot(lhs, rhs, transpose_a, transpose_b)\n\n\n\n\nCalculate dot product of two matrices or two vectors.\n\n\nFrom:src/operator/tensor/matrix_op.cc:231\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left input\n\n\nrhs::NDArray\n: Right input\n\n\ntranspose_a::boolean, optional, default=False\n: True if the first matrix is transposed.\n\n\ntranspose_b::boolean, optional, default=False\n: True if the second matrix is tranposed.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.LinAlg.norm\n \n \nMethod\n.\n\n\nnorm(src)\n\n\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.Math.gamma\n \n \nMethod\n.\n\n\ngamma(data)\n\n\n\n\nTake the gamma function (extension of the factorial function) of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:306\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase._div\n \n \nMethod\n.\n\n\n_div(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase._sub\n \n \nMethod\n.\n\n\n_sub(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.abs\n \n \nMethod\n.\n\n\nabs(data)\n\n\n\n\nTake absolute value of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:63\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.ceil\n \n \nMethod\n.\n\n\nceil(data)\n\n\n\n\nTake ceil of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:87\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.copy\n \n \nMethod\n.\n\n\ncopy(self :: SymbolicNode)\n\n\n\n\nMake a copy of a SymbolicNode. The same as making a deep copy.\n\n\nsource\n\n\n#\n\n\nBase.cos\n \n \nMethod\n.\n\n\ncos(data)\n\n\n\n\nTake cos of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:189\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.cosh\n \n \nMethod\n.\n\n\ncosh(data)\n\n\n\n\nTake cosh of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:261\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.deepcopy\n \n \nMethod\n.\n\n\ndeepcopy(self :: SymbolicNode)\n\n\n\n\nMake a deep copy of a SymbolicNode.\n\n\nsource\n\n\n#\n\n\nBase.exp\n \n \nMethod\n.\n\n\nexp(data)\n\n\n\n\nTake exp of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:135\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.expm1\n \n \nMethod\n.\n\n\nexpm1(data)\n\n\n\n\nTake \nexp(x) - 1\n in a numerically stable way\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:180\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.floor\n \n \nMethod\n.\n\n\nfloor(data)\n\n\n\n\nTake floor of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:92\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.getindex\n \n \nMethod\n.\n\n\ngetindex(self :: SymbolicNode, idx :: Union{Int, Base.Symbol, AbstractString})\n\n\n\n\nGet a node representing the specified output of this node. The index could be a symbol or string indicating the name of the output, or a 1-based integer indicating the index, as in the list of \nlist_outputs\n.\n\n\nsource\n\n\n#\n\n\nBase.identity\n \n \nMethod\n.\n\n\nidentity(data)\n\n\n\n\nidentity is an alias of _copy.\n\n\nIdentity mapping, copy src to output\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:14\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.log\n \n \nMethod\n.\n\n\nlog(data)\n\n\n\n\nTake log of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:141\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.log10\n \n \nMethod\n.\n\n\nlog10(data)\n\n\n\n\nTake base-10 log of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:147\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.log1p\n \n \nMethod\n.\n\n\nlog1p(data)\n\n\n\n\nTake \nlog(1 + x)\n in a numerically stable way\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:171\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.log2\n \n \nMethod\n.\n\n\nlog2(data)\n\n\n\n\nTake base-2 log of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:153\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.max\n \n \nMethod\n.\n\n\nmax(data, axis, keepdims)\n\n\n\n\nCompute max along axis. If axis is empty, global reduction is performed\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:66\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.mean\n \n \nMethod\n.\n\n\nmean(data, axis, keepdims)\n\n\n\n\nCompute mean src along axis. If axis is empty, global reduction is performed\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:26\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.min\n \n \nMethod\n.\n\n\nmin(data, axis, keepdims)\n\n\n\n\nCompute min along axis. If axis is empty, global reduction is performed\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:76\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.prod\n \n \nMethod\n.\n\n\nprod(data, axis, keepdims)\n\n\n\n\nCompute product of src along axis. If axis is empty, global reduction is performed\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:36\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.round\n \n \nMethod\n.\n\n\nround(data)\n\n\n\n\nTake round of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:81\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.sign\n \n \nMethod\n.\n\n\nsign(data)\n\n\n\n\nTake sign of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:72\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.sin\n \n \nMethod\n.\n\n\nsin(data)\n\n\n\n\nTake sin of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:162\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.sinh\n \n \nMethod\n.\n\n\nsinh(data)\n\n\n\n\nTake sinh of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:252\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.sort\n \n \nMethod\n.\n\n\nsort(src, axis, is_ascend)\n\n\n\n\nReturn a sorted copy of an array.\n\n\nFrom:src/operator/tensor/ordering_op.cc:59\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input\n\n\naxis::int or None, optional, default='-1'\n: Axis along which to choose sort the input tensor. If not given, the flattened array is used. Default is -1.\n\n\nis_ascend::boolean, optional, default=True\n: Whether sort in ascending or descending order.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.sqrt\n \n \nMethod\n.\n\n\nsqrt(data)\n\n\n\n\nTake square root of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:116\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.sum\n \n \nMethod\n.\n\n\nsum(data, axis, keepdims)\n\n\n\n\nSum src along axis. If axis is empty, global reduction is performed\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:17\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.take\n \n \nMethod\n.\n\n\ntake(a, indices, axis, mode)\n\n\n\n\nTake row vectors from an NDArray according to the indices For an input of index with shape (d1, ..., dK), the output shape is (d1, ..., dK, row_vector_length).All the input values should be integers in the range [0, column_vector_length).\n\n\nFrom:src/operator/tensor/indexing_op.cc:59\n\n\nArguments\n\n\n\n\na::SymbolicNode\n: The source array.\n\n\nindices::SymbolicNode\n: The indices of the values to extract.\n\n\naxis::int, optional, default='0'\n: the axis of data tensor to be taken.\n\n\nmode::{'clip', 'raise', 'wrap'},optional, default='raise'\n: specify how out-of-bound indices bahave.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.tan\n \n \nMethod\n.\n\n\ntan(data)\n\n\n\n\nTake tan of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:198\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.tanh\n \n \nMethod\n.\n\n\ntanh(data)\n\n\n\n\nTake tanh of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:270\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.transpose\n \n \nMethod\n.\n\n\ntranspose(data, axes)\n\n\n\n\nTranspose the input tensor and return a new one\n\n\nFrom:src/operator/tensor/matrix_op.cc:94\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxes::Shape(tuple), optional, default=()\n: Target axis order. By default the axes will be inverted.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Activation\n \n \nMethod\n.\n\n\nActivation(data, act_type)\n\n\n\n\nElementwise activation function.\n\n\nThe following activation types are supported (operations are applied elementwisely to each scalar of the input tensor):\n\n\n\n\nrelu\n: Rectified Linear Unit, \ny = max(x, 0)\n\n\nsigmoid\n: \ny = 1 / (1 + exp(-x))\n\n\ntanh\n: Hyperbolic tangent, \ny = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n\n\nsoftrelu\n: Soft ReLU, or SoftPlus, \ny = log(1 + exp(x))\n\n\n\n\nSee \nLeakyReLU\n for other activations with parameters.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to activation function.\n\n\nact_type::{'relu', 'sigmoid', 'softrelu', 'tanh'}, required\n: Activation function to be applied.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.BatchNorm\n \n \nMethod\n.\n\n\nBatchNorm(data, gamma, beta, eps, momentum, fix_gamma, use_global_stats, output_mean_var)\n\n\n\n\nApply batch normalization to input.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to batch normalization\n\n\ngamma::SymbolicNode\n: gamma matrix\n\n\nbeta::SymbolicNode\n: beta matrix\n\n\neps::float, optional, default=0.001\n: Epsilon to prevent div 0\n\n\nmomentum::float, optional, default=0.9\n: Momentum for moving average\n\n\nfix_gamma::boolean, optional, default=True\n: Fix gamma while training\n\n\nuse_global_stats::boolean, optional, default=False\n: Whether use global moving statistics instead of local batch-norm. This will force change batch-norm into a scale shift operator.\n\n\noutput_mean_var::boolean, optional, default=False\n: Output All,normal mean and var\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.BlockGrad\n \n \nMethod\n.\n\n\nBlockGrad(data)\n\n\n\n\nGet output from a symbol and pass 0 gradient back\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:30\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Cast\n \n \nMethod\n.\n\n\nCast(data, dtype)\n\n\n\n\nCast array to a different data type.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to cast function.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'}, required\n: Target data type.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Concat\n \n \nMethod\n.\n\n\nConcat(data, num_args, dim)\n\n\n\n\nNote\n: Concat takes variable number of positional inputs. So instead of calling as Concat([x, y, z], num_args=3), one should call via Concat(x, y, z), and num_args will be determined automatically.\n\n\nPerform a feature concat on channel dim (defaut is 1) over all\n\n\nArguments\n\n\n\n\ndata::SymbolicNode[]\n: List of tensors to concatenate\n\n\nnum_args::int, required\n: Number of inputs to be concated.\n\n\ndim::int, optional, default='1'\n: the dimension to be concated.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Convolution\n \n \nMethod\n.\n\n\nConvolution(data, weight, bias, kernel, stride, dilate, pad, num_filter, num_group, workspace, no_bias, cudnn_tune, cudnn_off, layout)\n\n\n\n\nApply convolution to input then add a bias.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the ConvolutionOp.\n\n\nweight::SymbolicNode\n: Weight matrix.\n\n\nbias::SymbolicNode\n: Bias parameter.\n\n\nkernel::Shape(tuple), required\n: convolution kernel size: (h, w) or (d, h, w)\n\n\nstride::Shape(tuple), optional, default=()\n: convolution stride: (h, w) or (d, h, w)\n\n\ndilate::Shape(tuple), optional, default=()\n: convolution dilate: (h, w) or (d, h, w)\n\n\npad::Shape(tuple), optional, default=()\n: pad for convolution: (h, w) or (d, h, w)\n\n\nnum_filter::int (non-negative), required\n: convolution filter(channel) number\n\n\nnum_group::int (non-negative), optional, default=1\n: Number of group partitions. Equivalent to slicing input into num_group   partitions, apply convolution on each, then concatenate the results\n\n\nworkspace::long (non-negative), optional, default=1024\n: Maximum tmp workspace allowed for convolution (MB).\n\n\nno_bias::boolean, optional, default=False\n: Whether to disable bias parameter.\n\n\ncudnn_tune::{None, 'fastest', 'limited_workspace', 'off'},optional, default='None'\n: Whether to pick convolution algo by running performance test.   Leads to higher startup time but may give faster speed. Options are:   'off': no tuning   'limited_workspace': run test and pick the fastest algorithm that doesn't exceed workspace limit.   'fastest': pick the fastest algorithm and ignore workspace limit.   If set to None (default), behavior is determined by environment   variable MXNET_CUDNN_AUTOTUNE_DEFAULT: 0 for off,   1 for limited workspace (default), 2 for fastest.\n\n\ncudnn_off::boolean, optional, default=False\n: Turn off cudnn for this layer.\n\n\nlayout::{None, 'NCDHW', 'NCHW', 'NDHWC', 'NHWC'},optional, default='None'\n: Set layout for input, output and weight. Empty for   default layout: NCHW for 2d and NCDHW for 3d.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Correlation\n \n \nMethod\n.\n\n\nCorrelation(data1, data2, kernel_size, max_displacement, stride1, stride2, pad_size, is_multiply)\n\n\n\n\nApply correlation to inputs\n\n\nArguments\n\n\n\n\ndata1::SymbolicNode\n: Input data1 to the correlation.\n\n\ndata2::SymbolicNode\n: Input data2 to the correlation.\n\n\nkernel_size::int (non-negative), optional, default=1\n: kernel size for Correlation must be an odd number\n\n\nmax_displacement::int (non-negative), optional, default=1\n: Max displacement of Correlation\n\n\nstride1::int (non-negative), optional, default=1\n: stride1 quantize data1 globally\n\n\nstride2::int (non-negative), optional, default=1\n: stride2 quantize data2 within the neighborhood centered around data1\n\n\npad_size::int (non-negative), optional, default=0\n: pad for Correlation\n\n\nis_multiply::boolean, optional, default=True\n: operation type is either multiplication or subduction\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Crop\n \n \nMethod\n.\n\n\nCrop(data, num_args, offset, h_w, center_crop)\n\n\n\n\nNote\n: Crop takes variable number of positional inputs. So instead of calling as Crop([x, y, z], num_args=3), one should call via Crop(x, y, z), and num_args will be determined automatically.\n\n\nCrop the 2nd and 3rd dim of input data, with the corresponding size of h_w or with width and height of the second input symbol, i.e., with one input, we need h_w to specify the crop height and width, otherwise the second input symbol's size will be used\n\n\nArguments\n\n\n\n\ndata::SymbolicNode or SymbolicNode[]\n: Tensor or List of Tensors, the second input will be used as crop_like shape reference\n\n\nnum_args::int, required\n: Number of inputs for crop, if equals one, then we will use the h_wfor crop height and width, else if equals two, then we will use the heightand width of the second input symbol, we name crop_like here\n\n\noffset::Shape(tuple), optional, default=(0,0)\n: crop offset coordinate: (y, x)\n\n\nh_w::Shape(tuple), optional, default=(0,0)\n: crop height and weight: (h, w)\n\n\ncenter_crop::boolean, optional, default=False\n: If set to true, then it will use be the center_crop,or it will crop using the shape of crop_like\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Custom\n \n \nMethod\n.\n\n\nCustom(op_type)\n\n\n\n\nCustom operator implemented in frontend.\n\n\nArguments\n\n\n\n\nop_type::string\n: Type of custom operator. Must be registered first.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Deconvolution\n \n \nMethod\n.\n\n\nDeconvolution(data, weight, bias, kernel, stride, pad, adj, target_shape, num_filter, num_group, workspace, no_bias)\n\n\n\n\nApply deconvolution to input then add a bias.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the DeconvolutionOp.\n\n\nweight::SymbolicNode\n: Weight matrix.\n\n\nbias::SymbolicNode\n: Bias parameter.\n\n\nkernel::Shape(tuple), required\n: deconvolution kernel size: (y, x)\n\n\nstride::Shape(tuple), optional, default=(1,1)\n: deconvolution stride: (y, x)\n\n\npad::Shape(tuple), optional, default=(0,0)\n: pad for deconvolution: (y, x), a good number is : (kernel-1)/2, if target_shape set, pad will be ignored and will be computed automatically\n\n\nadj::Shape(tuple), optional, default=(0,0)\n: adjustment for output shape: (y, x), if target_shape set, adj will be ignored and will be computed automatically\n\n\ntarget_shape::Shape(tuple), optional, default=(0,0)\n: output shape with targe shape : (y, x)\n\n\nnum_filter::int (non-negative), required\n: deconvolution filter(channel) number\n\n\nnum_group::int (non-negative), optional, default=1\n: number of groups partition\n\n\nworkspace::long (non-negative), optional, default=512\n: Tmp workspace for deconvolution (MB)\n\n\nno_bias::boolean, optional, default=True\n: Whether to disable bias parameter.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Dropout\n \n \nMethod\n.\n\n\nDropout(data, p)\n\n\n\n\nApply dropout to input. During training, each element of the input is randomly set to zero with probability p. And then the whole tensor is rescaled by 1/(1-p) to keep the expectation the same as before applying dropout. During the test time, this behaves as an identity map.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to dropout.\n\n\np::float, optional, default=0.5\n: Fraction of the input that gets dropped out at training time\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ElementWiseSum\n \n \nMethod\n.\n\n\nElementWiseSum(args)\n\n\n\n\nNote\n: ElementWiseSum takes variable number of positional inputs. So instead of calling as ElementWiseSum([x, y, z], num_args=3), one should call via ElementWiseSum(x, y, z), and num_args will be determined automatically.\n\n\nPerform element sum of inputs\n\n\nFrom:src/operator/tensor/elemwise_sum.cc:56\n\n\nArguments\n\n\n\n\nargs::NDArray[]\n: List of input tensors\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Embedding\n \n \nMethod\n.\n\n\nEmbedding(data, weight, input_dim, output_dim)\n\n\n\n\nMap integer index to vector representations (embeddings). Those embeddings are learnable parameters. For a input of shape (d1, ..., dK), the output shape is (d1, ..., dK, output_dim). All the input values should be integers in the range [0, input_dim).\n\n\nFrom:src/operator/tensor/indexing_op.cc:18\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the EmbeddingOp.\n\n\nweight::SymbolicNode\n: Embedding weight matrix.\n\n\ninput_dim::int, required\n: vocabulary size of the input indices.\n\n\noutput_dim::int, required\n: dimension of the embedding vectors.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.FullyConnected\n \n \nMethod\n.\n\n\nFullyConnected(data, weight, bias, num_hidden, no_bias)\n\n\n\n\nApply matrix multiplication to input then add a bias. It maps the input of shape \n(batch_size, input_dim)\n to the shape of \n(batch_size, num_hidden)\n. Learnable parameters include the weights of the linear transform and an optional bias vector.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the FullyConnectedOp.\n\n\nweight::SymbolicNode\n: Weight matrix.\n\n\nbias::SymbolicNode\n: Bias parameter.\n\n\nnum_hidden::int, required\n: Number of hidden nodes of the output.\n\n\nno_bias::boolean, optional, default=False\n: Whether to disable bias parameter.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Group\n \n \nMethod\n.\n\n\nGroup(nodes :: SymbolicNode...)\n\n\n\n\nCreate a \nSymbolicNode\n by grouping nodes together.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.IdentityAttachKLSparseReg\n \n \nMethod\n.\n\n\nIdentityAttachKLSparseReg(data, sparseness_target, penalty, momentum)\n\n\n\n\nApply a sparse regularization to the output a sigmoid activation function.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data.\n\n\nsparseness_target::float, optional, default=0.1\n: The sparseness target\n\n\npenalty::float, optional, default=0.001\n: The tradeoff parameter for the sparseness penalty\n\n\nmomentum::float, optional, default=0.9\n: The momentum for running average\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.InstanceNorm\n \n \nMethod\n.\n\n\nInstanceNorm(data, gamma, beta, eps)\n\n\n\n\nAn operator taking in a n-dimensional input tensor (n \n 2), and normalizing the input by subtracting the mean and variance calculated over the spatial dimensions. This is an implemention of the operator described in \"Instance Normalization: The Missing Ingredient for Fast Stylization\", D. Ulyanov, A. Vedaldi, V. Lempitsky, 2016 (arXiv:1607.08022v2). This layer is similar to batch normalization, with two differences: first, the normalization is carried out per example ('instance'), not over a batch. Second, the same normalization is applied both at test and train time. This operation is also known as 'contrast normalization'.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: A n-dimensional tensor (n \n 2) of the form [batch, channel, spatial_dim1, spatial_dim2, ...].\n\n\ngamma::SymbolicNode\n: A vector of length 'channel', which multiplies the normalized input.\n\n\nbeta::SymbolicNode\n: A vector of length 'channel', which is added to the product of the normalized input and the weight.\n\n\neps::float, optional, default=0.001\n: Epsilon to prevent division by 0.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.L2Normalization\n \n \nMethod\n.\n\n\nL2Normalization(data, eps, mode)\n\n\n\n\nSet the l2 norm of each instance to a constant.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the L2NormalizationOp.\n\n\neps::float, optional, default=1e-10\n: Epsilon to prevent div 0\n\n\nmode::{'channel', 'instance', 'spatial'},optional, default='instance'\n: Normalization Mode. If set to instance, this operator will compute a norm for each instance in the batch; this is the default mode. If set to channel, this operator will compute a cross channel norm at each position of each instance. If set to spatial, this operator will compute a norm for each channel.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LRN\n \n \nMethod\n.\n\n\nLRN(data, alpha, beta, knorm, nsize)\n\n\n\n\nApply convolution to input then add a bias.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the ConvolutionOp.\n\n\nalpha::float, optional, default=0.0001\n: value of the alpha variance scaling parameter in the normalization formula\n\n\nbeta::float, optional, default=0.75\n: value of the beta power parameter in the normalization formula\n\n\nknorm::float, optional, default=2\n: value of the k parameter in normalization formula\n\n\nnsize::int (non-negative), required\n: normalization window width in elements.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LeakyReLU\n \n \nMethod\n.\n\n\nLeakyReLU(data, act_type, slope, lower_bound, upper_bound)\n\n\n\n\nApply activation function to input.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to activation function.\n\n\nact_type::{'elu', 'leaky', 'prelu', 'rrelu'},optional, default='leaky'\n: Activation function to be applied.\n\n\nslope::float, optional, default=0.25\n: Init slope for the activation. (For leaky and elu only)\n\n\nlower_bound::float, optional, default=0.125\n: Lower bound of random slope. (For rrelu only)\n\n\nupper_bound::float, optional, default=0.334\n: Upper bound of random slope. (For rrelu only)\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LinearRegressionOutput\n \n \nMethod\n.\n\n\nLinearRegressionOutput(data, label, grad_scale)\n\n\n\n\nUse linear regression for final output, this is used on final output of a net.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to function.\n\n\nlabel::SymbolicNode\n: Input label to function.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LogisticRegressionOutput\n \n \nMethod\n.\n\n\nLogisticRegressionOutput(data, label, grad_scale)\n\n\n\n\nUse Logistic regression for final output, this is used on final output of a net. Logistic regression is suitable for binary classification or probability prediction tasks.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to function.\n\n\nlabel::SymbolicNode\n: Input label to function.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MAERegressionOutput\n \n \nMethod\n.\n\n\nMAERegressionOutput(data, label, grad_scale)\n\n\n\n\nUse mean absolute error regression for final output, this is used on final output of a net.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to function.\n\n\nlabel::SymbolicNode\n: Input label to function.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MakeLoss\n \n \nMethod\n.\n\n\nMakeLoss(data, grad_scale, valid_thresh, normalization)\n\n\n\n\nGet output from a symbol and pass 1 gradient back. This is used as a terminal loss if unary and binary operator are used to composite a loss with no declaration of backward dependency\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data.\n\n\ngrad_scale::float, optional, default=1\n: gradient scale as a supplement to unary and binary operators\n\n\nvalid_thresh::float, optional, default=0\n: regard element valid when x \n valid_thresh, this is used only in valid normalization mode.\n\n\nnormalization::{'batch', 'null', 'valid'},optional, default='null'\n: If set to null, op will not normalize on output gradient.If set to batch, op will normalize gradient by divide batch size.If set to valid, op will normalize gradient by divide # sample marked as valid\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Pad\n \n \nMethod\n.\n\n\nPad(data, mode, pad_width, constant_value)\n\n\n\n\nPads an n-dimensional input tensor. Allows for precise control of the padding type and how much padding to apply on both sides of a given dimension.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: An n-dimensional input tensor.\n\n\nmode::{'constant', 'edge'}, required\n: Padding type to use. \"constant\" pads all values with a constant value, the value of which can be specified with the constant_value option. \"edge\" uses the boundary values of the array as padding.\n\n\npad_width::Shape(tuple), required\n: A tuple of padding widths of length 2*r, where r is the rank of the input tensor, specifying number of values padded to the edges of each axis. (before_1, after_1, ... , before_N, after_N) unique pad widths for each axis. Equivalent to pad_width in numpy.pad, but flattened.\n\n\nconstant_value::double, optional, default=0\n: This option is only used when mode is \"constant\". This value will be used as the padding value. Defaults to 0 if not specified.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Pooling\n \n \nMethod\n.\n\n\nPooling(data, global_pool, kernel, pool_type, pooling_convention, stride, pad)\n\n\n\n\nPerform spatial pooling on inputs.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the pooling operator.\n\n\nglobal_pool::boolean, optional, default=False\n: Ignore kernel size, do global pooling based on current input feature map. This is useful for input with different shape\n\n\nkernel::Shape(tuple), required\n: pooling kernel size: (y, x) or (d, y, x)\n\n\npool_type::{'avg', 'max', 'sum'}, required\n: Pooling type to be applied.\n\n\npooling_convention::{'full', 'valid'},optional, default='valid'\n: Pooling convention to be applied.kValid is default setting of Mxnet and rounds down the output pooling size.kFull is compatible with Caffe and rounds up the output pooling size.\n\n\nstride::Shape(tuple), optional, default=(1,1)\n: stride: for pooling (y, x) or (d, y, x)\n\n\npad::Shape(tuple), optional, default=(0,0)\n: pad for pooling: (y, x) or (d, y, x)\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.RNN\n \n \nMethod\n.\n\n\nRNN(data, parameters, state, state_cell, state_size, num_layers, bidirectional, mode, p, state_outputs)\n\n\n\n\nApply a recurrent layer to input.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to RNN\n\n\nparameters::SymbolicNode\n: Vector of all RNN trainable parameters concatenated\n\n\nstate::SymbolicNode\n: initial hidden state of the RNN\n\n\nstate_cell::SymbolicNode\n: initial cell state for LSTM networks (only for LSTM)\n\n\nstate_size::int (non-negative), required\n: size of the state for each layer\n\n\nnum_layers::int (non-negative), required\n: number of stacked layers\n\n\nbidirectional::boolean, optional, default=False\n: whether to use bidirectional recurrent layers\n\n\nmode::{'gru', 'lstm', 'rnn_relu', 'rnn_tanh'}, required\n: the type of RNN to compute\n\n\np::float, optional, default=0\n: Dropout probability, fraction of the input that gets dropped out at training time\n\n\nstate_outputs::boolean, optional, default=False\n: Whether to have the states as symbol outputs.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ROIPooling\n \n \nMethod\n.\n\n\nROIPooling(data, rois, pooled_size, spatial_scale)\n\n\n\n\nPerforms region-of-interest pooling on inputs. Resize bounding box coordinates by spatial_scale and crop input feature maps accordingly. The cropped feature maps are pooled by max pooling to a fixed size output indicated by pooled_size. batch_size will change to the number of region bounding boxes after ROIPooling\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the pooling operator, a 4D Feature maps\n\n\nrois::SymbolicNode\n: Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]]. (x1, y1) and (x2, y2) are top left and down right corners of designated region of interest. batch_index indicates the index of corresponding image in the input data\n\n\npooled_size::Shape(tuple), required\n: fix pooled size: (h, w)\n\n\nspatial_scale::float, required\n: Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Reshape\n \n \nMethod\n.\n\n\nReshape(data, target_shape, keep_highest, shape, reverse)\n\n\n\n\nReshape input according to a target shape spec. The target shape is a tuple and can be a simple list of dimensions such as (12,3) or it can incorporate special codes that correspond to contextual operations that refer to the input dimensions. The special codes are all expressed as integers less than 1. These codes effectively refer to a machine that pops input dims off the beginning of the input dims list and pushes resulting output dims onto the end of the output dims list, which starts empty. The codes are:   0  Copy     Pop one input dim and push it onto the output dims  -1  Infer    Push a dim that is inferred later from all other output dims  -2  CopyAll  Pop all remaining input dims and push them onto output dims  -3  Merge2   Pop two input dims, multiply them, and push result  -4  Split2   Pop one input dim, and read two next target shape specs,               push them both onto output dims (either can be -1 and will               be inferred from the other  The exact mathematical behavior of these codes is given in the description of the 'shape' parameter. All non-codes (positive integers) just pop a dim off the input dims (if any), throw it away, and then push the specified integer onto the output dims. Examples: Type     Input      Target            Output Copy     (2,3,4)    (4,0,2)           (4,3,2) Copy     (2,3,4)    (2,0,0)           (2,3,4) Infer    (2,3,4)    (6,1,-1)          (6,1,4) Infer    (2,3,4)    (3,-1,8)          (3,1,8) CopyAll  (9,8,7)    (-2)              (9,8,7) CopyAll  (9,8,7)    (9,-2)            (9,8,7) CopyAll  (9,8,7)    (-2,1,1)          (9,8,7,1,1) Merge2   (3,4)      (-3)              (12) Merge2   (3,4,5)    (-3,0)            (12,5) Merge2   (3,4,5)    (0,-3)            (3,20) Merge2   (3,4,5,6)  (-3,0,0)          (12,5,6) Merge2   (3,4,5,6)  (-3,-2)           (12,5,6) Split2   (12)       (-4,6,2)          (6,2) Split2   (12)       (-4,2,6)          (2,6) Split2   (12)       (-4,-1,6)         (2,6) Split2   (12,9)     (-4,2,6,0)        (2,6,9) Split2   (12,9,9,9) (-4,2,6,-2)       (2,6,9,9,9) Split2   (12,12)    (-4,2,-1,-4,-1,2) (2,6,6,2)\n\n\nFrom:src/operator/tensor/matrix_op.cc:62\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Input data to reshape.\n\n\ntarget_shape::Shape(tuple), optional, default=(0,0)\n: (Deprecated! Use shape instead.) Target new shape. One and only one dim can be 0, in which case it will be inferred from the rest of dims\n\n\nkeep_highest::boolean, optional, default=False\n: (Deprecated! Use shape instead.) Whether keep the highest dim unchanged.If set to true, then the first dim in target_shape is ignored,and always fixed as input\n\n\nshape::Shape(tuple), optional, default=()\n: Target shape, a tuple, t=(t_1,t_2,..,t_m).\n\n\n\n\nLet the input dims be s=(s_1,s_2,..,s_n). The output dims u=(u_1,u_2,..,u_p) are computed from s and t. The target shape tuple elements t_i are read in order, and used to  generate successive output dims u_p: t_i:       meaning:      behavior: +ve        explicit      u_p = t_i 0          copy          u_p = s_i -1         infer         u_p = (Prod s_i) / (Prod u_j | j != p) -2         copy all      u_p = s_i, u_p+1 = s_i+1, ... -3         merge two     u_p = s_i * s_i+1 -4,a,b     split two     u_p = a, u_p+1 = b | a * b = s_i The split directive (-4) in the target shape tuple is followed by two dimensions, one of which can be -1, which means it will be inferred from the other one and the original dimension. The can only be one globally inferred dimension (-1), aside from any -1 occuring in a split directive.\n\n\n\n\nreverse::boolean, optional, default=False\n: Whether to match the shapes from the backward. If reverse is true, 0 values in the \nshape\n argument will be searched from the backward. E.g the original shape is (10, 5, 4) and the shape argument is (-1, 0). If reverse is true, the new shape should be (50, 4). Otherwise it will be (40, 5).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SVMOutput\n \n \nMethod\n.\n\n\nSVMOutput(data, label, margin, regularization_coefficient, use_linear)\n\n\n\n\nSupport Vector Machine based transformation on input, backprop L2-SVM\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to svm.\n\n\nlabel::SymbolicNode\n: Label data.\n\n\nmargin::float, optional, default=1\n: Scale the DType(param_.margin) for activation size\n\n\nregularization_coefficient::float, optional, default=1\n: Scale the coefficient responsible for balacing coefficient size and error tradeoff\n\n\nuse_linear::boolean, optional, default=False\n: If set true, uses L1-SVM objective function. Default uses L2-SVM objective\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SequenceLast\n \n \nMethod\n.\n\n\nSequenceLast(data, sequence_length, use_sequence_length)\n\n\n\n\nTakes the last element of a sequence. Takes an n-dimensional tensor of the form [max sequence length, batchsize, other dims] and returns a (n-1)-dimensional tensor of the form [batchsize, other dims]. This operator takes an optional input tensor sequence_length of positive ints of dimension [batchsize] when the sequence_length option is set to true. This allows the operator to handle variable-length sequences. If sequence_length is false, then each example in the batch is assumed to have the max sequence length.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: n-dimensional input tensor of the form [max sequence length, batchsize, other dims]\n\n\nsequence_length::SymbolicNode\n: vector of sequence lengths of size batchsize\n\n\nuse_sequence_length::boolean, optional, default=False\n: If set to true, this layer takes in extra input sequence_length to specify variable length sequence\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SequenceMask\n \n \nMethod\n.\n\n\nSequenceMask(data, sequence_length, use_sequence_length, value)\n\n\n\n\nSets all elements outside the sequence to a constant value. Takes an n-dimensional tensor of the form [max sequence length, batchsize, other dims] and returns a tensor of the same shape. This operator takes an optional input tensor sequence_length of positive ints of dimension [batchsize] when the sequence_length option is set to true. This allows the operator to handle variable-length sequences. If sequence_length is false, then each example in the batch is assumed to have the max sequence length, and this operator becomes the identity operator.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: n-dimensional input tensor of the form [max sequence length, batchsize, other dims]\n\n\nsequence_length::SymbolicNode\n: vector of sequence lengths of size batchsize\n\n\nuse_sequence_length::boolean, optional, default=False\n: If set to true, this layer takes in extra input sequence_length to specify variable length sequence\n\n\nvalue::float, optional, default=0\n: The value to be used as a mask.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SequenceReverse\n \n \nMethod\n.\n\n\nSequenceReverse(data, sequence_length, use_sequence_length)\n\n\n\n\nReverses the elements of each sequence. Takes an n-dimensional tensor of the form [max sequence length, batchsize, other dims] and returns a tensor of the same shape. This operator takes an optional input tensor sequence_length of positive ints of dimension [batchsize] when the sequence_length option is set to true. This allows the operator to handle variable-length sequences. If sequence_length is false, then each example in the batch is assumed to have the max sequence length.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: n-dimensional input tensor of the form [max sequence length, batchsize, other dims]\n\n\nsequence_length::SymbolicNode\n: vector of sequence lengths of size batchsize\n\n\nuse_sequence_length::boolean, optional, default=False\n: If set to true, this layer takes in extra input sequence_length to specify variable length sequence\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SliceChannel\n \n \nMethod\n.\n\n\nSliceChannel(num_outputs, axis, squeeze_axis)\n\n\n\n\nSlice input equally along specified axis\n\n\nArguments\n\n\n\n\nnum_outputs::int, required\n: Number of outputs to be sliced.\n\n\naxis::int, optional, default='1'\n: Dimension along which to slice.\n\n\nsqueeze_axis::boolean, optional, default=False\n: If true AND the sliced dimension becomes 1, squeeze that dimension.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Softmax\n \n \nMethod\n.\n\n\nSoftmax(data, grad_scale, ignore_label, multi_output, use_ignore, preserve_shape, normalization, out_grad)\n\n\n\n\nDEPRECATED: Perform a softmax transformation on input. Please use SoftmaxOutput\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to softmax.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\nignore_label::float, optional, default=-1\n: the label value will be ignored during backward (only works if use_ignore is set to be true).\n\n\nmulti_output::boolean, optional, default=False\n: If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n\nx_1\n...*x_n output, each has k classes\n\n\nuse_ignore::boolean, optional, default=False\n: If set to true, the ignore_label value will not contribute to the backward gradient\n\n\npreserve_shape::boolean, optional, default=False\n: If true, for a (n_1, n_2, ..., n_d, k) dimensional input tensor, softmax will generate (n1, n2, ..., n_d, k) output, normalizing the k classes as the last dimension.\n\n\nnormalization::{'batch', 'null', 'valid'},optional, default='null'\n: If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored\n\n\nout_grad::boolean, optional, default=False\n: Apply weighting from output gradient\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SoftmaxActivation\n \n \nMethod\n.\n\n\nSoftmaxActivation(data, mode)\n\n\n\n\nApply softmax activation to input. This is intended for internal layers. For output (loss layer) please use SoftmaxOutput. If mode=instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If mode=channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to activation function.\n\n\nmode::{'channel', 'instance'},optional, default='instance'\n: Softmax Mode. If set to instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If set to channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SoftmaxOutput\n \n \nMethod\n.\n\n\nSoftmaxOutput(data, label, grad_scale, ignore_label, multi_output, use_ignore, preserve_shape, normalization, out_grad)\n\n\n\n\nPerform a softmax transformation on input, backprop with logloss.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to softmax.\n\n\nlabel::SymbolicNode\n: Label data, can also be probability value with same shape as data\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\nignore_label::float, optional, default=-1\n: the label value will be ignored during backward (only works if use_ignore is set to be true).\n\n\nmulti_output::boolean, optional, default=False\n: If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n\nx_1\n...*x_n output, each has k classes\n\n\nuse_ignore::boolean, optional, default=False\n: If set to true, the ignore_label value will not contribute to the backward gradient\n\n\npreserve_shape::boolean, optional, default=False\n: If true, for a (n_1, n_2, ..., n_d, k) dimensional input tensor, softmax will generate (n1, n2, ..., n_d, k) output, normalizing the k classes as the last dimension.\n\n\nnormalization::{'batch', 'null', 'valid'},optional, default='null'\n: If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored\n\n\nout_grad::boolean, optional, default=False\n: Apply weighting from output gradient\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SpatialTransformer\n \n \nMethod\n.\n\n\nSpatialTransformer(data, loc, target_shape, transform_type, sampler_type)\n\n\n\n\nApply spatial transformer to input feature map.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the SpatialTransformerOp.\n\n\nloc::SymbolicNode\n: localisation net, the output dim should be 6 when transform_type is affine, and the name of loc symbol should better starts with 'stn_loc', so that initialization it with iddentify tranform, or you shold initialize the weight and bias by yourself.\n\n\ntarget_shape::Shape(tuple), optional, default=(0,0)\n: output shape(h, w) of spatial transformer: (y, x)\n\n\ntransform_type::{'affine'}, required\n: transformation type\n\n\nsampler_type::{'bilinear'}, required\n: sampling type\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SwapAxis\n \n \nMethod\n.\n\n\nSwapAxis(data, dim1, dim2)\n\n\n\n\nApply swapaxis to input.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the SwapAxisOp.\n\n\ndim1::int (non-negative), optional, default=0\n: the first axis to be swapped.\n\n\ndim2::int (non-negative), optional, default=0\n: the second axis to be swapped.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.UpSampling\n \n \nMethod\n.\n\n\nUpSampling(data, scale, num_filter, sample_type, multi_input_mode, num_args, workspace)\n\n\n\n\nNote\n: UpSampling takes variable number of positional inputs. So instead of calling as UpSampling([x, y, z], num_args=3), one should call via UpSampling(x, y, z), and num_args will be determined automatically.\n\n\nPerform nearest neighboor/bilinear up sampling to inputs\n\n\nArguments\n\n\n\n\ndata::SymbolicNode[]\n: Array of tensors to upsample\n\n\nscale::int (non-negative), required\n: Up sampling scale\n\n\nnum_filter::int (non-negative), optional, default=0\n: Input filter. Only used by bilinear sample_type.\n\n\nsample_type::{'bilinear', 'nearest'}, required\n: upsampling method\n\n\nmulti_input_mode::{'concat', 'sum'},optional, default='concat'\n: How to handle multiple input. concat means concatenate upsampled images along the channel dimension. sum means add all images together, only available for nearest neighbor upsampling.\n\n\nnum_args::int, required\n: Number of inputs to be upsampled. For nearest neighbor upsampling, this can be 1-N; the size of output will be(scale\nh_0,scale\nw_0) and all other inputs will be upsampled to thesame size. For bilinear upsampling this must be 2; 1 input and 1 weight.\n\n\nworkspace::long (non-negative), optional, default=512\n: Tmp workspace for deconvolution (MB)\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Variable\n \n \nMethod\n.\n\n\nVariable(name :: Union{Symbol, AbstractString})\n\n\n\n\nCreate a symbolic variable with the given name. This is typically used as a placeholder. For example, the data node, acting as the starting point of a network architecture.\n\n\nArguments\n\n\n\n\nDict{Symbol, AbstractString} attrs: The attributes associated with this \nVariable\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._CrossDeviceCopy\n \n \nMethod\n.\n\n\n_CrossDeviceCopy()\n\n\n\n\nSpecial op to copy data cross device\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Div\n \n \nMethod\n.\n\n\n_Div(lhs, rhs)\n\n\n\n\n_Div is an alias of _div.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._DivScalar\n \n \nMethod\n.\n\n\n_DivScalar(data, scalar)\n\n\n\n\n_DivScalar is an alias of _div_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Equal\n \n \nMethod\n.\n\n\n_Equal(lhs, rhs)\n\n\n\n\n_Equal is an alias of _equal.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._EqualScalar\n \n \nMethod\n.\n\n\n_EqualScalar(data, scalar)\n\n\n\n\n_EqualScalar is an alias of _equal_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Greater\n \n \nMethod\n.\n\n\n_Greater(lhs, rhs)\n\n\n\n\n_Greater is an alias of _greater.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._GreaterEqualScalar\n \n \nMethod\n.\n\n\n_GreaterEqualScalar(data, scalar)\n\n\n\n\n_GreaterEqualScalar is an alias of _greater_equal_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._GreaterScalar\n \n \nMethod\n.\n\n\n_GreaterScalar(data, scalar)\n\n\n\n\n_GreaterScalar is an alias of _greater_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Greater_Equal\n \n \nMethod\n.\n\n\n_Greater_Equal(lhs, rhs)\n\n\n\n\n_Greater_Equal is an alias of _greater_equal.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Hypot\n \n \nMethod\n.\n\n\n_Hypot(lhs, rhs)\n\n\n\n\n_Hypot is an alias of _hypot.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._HypotScalar\n \n \nMethod\n.\n\n\n_HypotScalar(data, scalar)\n\n\n\n\n_HypotScalar is an alias of _hypot_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Lesser\n \n \nMethod\n.\n\n\n_Lesser(lhs, rhs)\n\n\n\n\n_Lesser is an alias of _lesser.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._LesserEqualScalar\n \n \nMethod\n.\n\n\n_LesserEqualScalar(data, scalar)\n\n\n\n\n_LesserEqualScalar is an alias of _lesser_equal_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._LesserScalar\n \n \nMethod\n.\n\n\n_LesserScalar(data, scalar)\n\n\n\n\n_LesserScalar is an alias of _lesser_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Lesser_Equal\n \n \nMethod\n.\n\n\n_Lesser_Equal(lhs, rhs)\n\n\n\n\n_Lesser_Equal is an alias of _lesser_equal.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Maximum\n \n \nMethod\n.\n\n\n_Maximum(lhs, rhs)\n\n\n\n\n_Maximum is an alias of _maximum.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MaximumScalar\n \n \nMethod\n.\n\n\n_MaximumScalar(data, scalar)\n\n\n\n\n_MaximumScalar is an alias of _maximum_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Minimum\n \n \nMethod\n.\n\n\n_Minimum(lhs, rhs)\n\n\n\n\n_Minimum is an alias of _minimum.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MinimumScalar\n \n \nMethod\n.\n\n\n_MinimumScalar(data, scalar)\n\n\n\n\n_MinimumScalar is an alias of _minimum_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Minus\n \n \nMethod\n.\n\n\n_Minus(lhs, rhs)\n\n\n\n\n_Minus is an alias of _sub.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MinusScalar\n \n \nMethod\n.\n\n\n_MinusScalar(data, scalar)\n\n\n\n\n_MinusScalar is an alias of _minus_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Mul\n \n \nMethod\n.\n\n\n_Mul(lhs, rhs)\n\n\n\n\n_Mul is an alias of _mul.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MulScalar\n \n \nMethod\n.\n\n\n_MulScalar(data, scalar)\n\n\n\n\n_MulScalar is an alias of _mul_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._NDArray\n \n \nMethod\n.\n\n\n_NDArray(info)\n\n\n\n\nStub for implementing an operator implemented in native frontend language with ndarray.\n\n\nArguments\n\n\n\n\ninfo::, required\n:\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Native\n \n \nMethod\n.\n\n\n_Native(info, need_top_grad)\n\n\n\n\nStub for implementing an operator implemented in native frontend language.\n\n\nArguments\n\n\n\n\ninfo::, required\n:\n\n\nneed_top_grad::boolean, optional, default=True\n: Whether this layer needs out grad for backward. Should be false for loss layers.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._NoGradient\n \n \nMethod\n.\n\n\n_NoGradient()\n\n\n\n\nPlace holder for variable who cannot perform gradient\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._NotEqualScalar\n \n \nMethod\n.\n\n\n_NotEqualScalar(data, scalar)\n\n\n\n\n_NotEqualScalar is an alias of _not_equal_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Not_Equal\n \n \nMethod\n.\n\n\n_Not_Equal(lhs, rhs)\n\n\n\n\n_Not_Equal is an alias of _not_equal.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Plus\n \n \nMethod\n.\n\n\n_Plus(lhs, rhs)\n\n\n\n\n_Plus is an alias of elemwise_add.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._PlusScalar\n \n \nMethod\n.\n\n\n_PlusScalar(data, scalar)\n\n\n\n\n_PlusScalar is an alias of _plus_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Power\n \n \nMethod\n.\n\n\n_Power(lhs, rhs)\n\n\n\n\n_Power is an alias of _power.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._PowerScalar\n \n \nMethod\n.\n\n\n_PowerScalar(data, scalar)\n\n\n\n\n_PowerScalar is an alias of _power_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._RDivScalar\n \n \nMethod\n.\n\n\n_RDivScalar(data, scalar)\n\n\n\n\n_RDivScalar is an alias of _rdiv_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._RMinusScalar\n \n \nMethod\n.\n\n\n_RMinusScalar(data, scalar)\n\n\n\n\n_RMinusScalar is an alias of _rminus_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._RPowerScalar\n \n \nMethod\n.\n\n\n_RPowerScalar(data, scalar)\n\n\n\n\n_RPowerScalar is an alias of _rpower_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._add\n \n \nMethod\n.\n\n\n_add(lhs, rhs)\n\n\n\n\n_add is an alias of elemwise_add.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._arange\n \n \nMethod\n.\n\n\n_arange(start, stop, step, repeat, ctx, dtype)\n\n\n\n\nReturn evenly spaced values within a given interval. Similar to Numpy\n\n\nArguments\n\n\n\n\nstart::float, required\n: Start of interval. The interval includes this value. The default start value is 0.\n\n\nstop::, optional, default=None\n: End of interval. The interval does not include this value, except in some cases where step is not an integer and floating point round-off affects the length of out.\n\n\nstep::float, optional, default=1\n: Spacing between values.\n\n\nrepeat::int, optional, default='1'\n: The repeating time of all elements. E.g repeat=3, the element a will be repeated three times \u2013\n a, a, a.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n.Only used for imperative calls.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32'\n: Target data type.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Activation\n \n \nMethod\n.\n\n\n_backward_Activation()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_BatchNorm\n \n \nMethod\n.\n\n\n_backward_BatchNorm()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Cast\n \n \nMethod\n.\n\n\n_backward_Cast()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Concat\n \n \nMethod\n.\n\n\n_backward_Concat()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Convolution\n \n \nMethod\n.\n\n\n_backward_Convolution()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Correlation\n \n \nMethod\n.\n\n\n_backward_Correlation()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Crop\n \n \nMethod\n.\n\n\n_backward_Crop()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Custom\n \n \nMethod\n.\n\n\n_backward_Custom()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Deconvolution\n \n \nMethod\n.\n\n\n_backward_Deconvolution()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Dropout\n \n \nMethod\n.\n\n\n_backward_Dropout()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Embedding\n \n \nMethod\n.\n\n\n_backward_Embedding()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_FullyConnected\n \n \nMethod\n.\n\n\n_backward_FullyConnected()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_IdentityAttachKLSparseReg\n \n \nMethod\n.\n\n\n_backward_IdentityAttachKLSparseReg()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_InstanceNorm\n \n \nMethod\n.\n\n\n_backward_InstanceNorm()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_L2Normalization\n \n \nMethod\n.\n\n\n_backward_L2Normalization()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_LRN\n \n \nMethod\n.\n\n\n_backward_LRN()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_LeakyReLU\n \n \nMethod\n.\n\n\n_backward_LeakyReLU()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_LinearRegressionOutput\n \n \nMethod\n.\n\n\n_backward_LinearRegressionOutput()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_LogisticRegressionOutput\n \n \nMethod\n.\n\n\n_backward_LogisticRegressionOutput()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_MAERegressionOutput\n \n \nMethod\n.\n\n\n_backward_MAERegressionOutput()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_MakeLoss\n \n \nMethod\n.\n\n\n_backward_MakeLoss()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Pad\n \n \nMethod\n.\n\n\n_backward_Pad()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Pooling\n \n \nMethod\n.\n\n\n_backward_Pooling()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_RNN\n \n \nMethod\n.\n\n\n_backward_RNN()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_ROIPooling\n \n \nMethod\n.\n\n\n_backward_ROIPooling()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SVMOutput\n \n \nMethod\n.\n\n\n_backward_SVMOutput()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SequenceLast\n \n \nMethod\n.\n\n\n_backward_SequenceLast()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SequenceMask\n \n \nMethod\n.\n\n\n_backward_SequenceMask()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SequenceReverse\n \n \nMethod\n.\n\n\n_backward_SequenceReverse()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SliceChannel\n \n \nMethod\n.\n\n\n_backward_SliceChannel()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Softmax\n \n \nMethod\n.\n\n\n_backward_Softmax()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SoftmaxActivation\n \n \nMethod\n.\n\n\n_backward_SoftmaxActivation()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SoftmaxOutput\n \n \nMethod\n.\n\n\n_backward_SoftmaxOutput()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SpatialTransformer\n \n \nMethod\n.\n\n\n_backward_SpatialTransformer()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SwapAxis\n \n \nMethod\n.\n\n\n_backward_SwapAxis()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_UpSampling\n \n \nMethod\n.\n\n\n_backward_UpSampling()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__CrossDeviceCopy\n \n \nMethod\n.\n\n\n_backward__CrossDeviceCopy()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__NDArray\n \n \nMethod\n.\n\n\n_backward__NDArray()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__Native\n \n \nMethod\n.\n\n\n_backward__Native()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_abs\n \n \nMethod\n.\n\n\n_backward_abs(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_add\n \n \nMethod\n.\n\n\n_backward_add()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arccos\n \n \nMethod\n.\n\n\n_backward_arccos(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arccosh\n \n \nMethod\n.\n\n\n_backward_arccosh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arcsin\n \n \nMethod\n.\n\n\n_backward_arcsin(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arcsinh\n \n \nMethod\n.\n\n\n_backward_arcsinh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arctan\n \n \nMethod\n.\n\n\n_backward_arctan(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arctanh\n \n \nMethod\n.\n\n\n_backward_arctanh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_batch_dot\n \n \nMethod\n.\n\n\n_backward_batch_dot()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_add\n \n \nMethod\n.\n\n\n_backward_broadcast_add()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_div\n \n \nMethod\n.\n\n\n_backward_broadcast_div()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_hypot\n \n \nMethod\n.\n\n\n_backward_broadcast_hypot()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_maximum\n \n \nMethod\n.\n\n\n_backward_broadcast_maximum()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_minimum\n \n \nMethod\n.\n\n\n_backward_broadcast_minimum()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_mul\n \n \nMethod\n.\n\n\n_backward_broadcast_mul()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_power\n \n \nMethod\n.\n\n\n_backward_broadcast_power()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_sub\n \n \nMethod\n.\n\n\n_backward_broadcast_sub()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_clip\n \n \nMethod\n.\n\n\n_backward_clip()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_copy\n \n \nMethod\n.\n\n\n_backward_copy()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_cos\n \n \nMethod\n.\n\n\n_backward_cos(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_cosh\n \n \nMethod\n.\n\n\n_backward_cosh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_degrees\n \n \nMethod\n.\n\n\n_backward_degrees(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_div\n \n \nMethod\n.\n\n\n_backward_div()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_dot\n \n \nMethod\n.\n\n\n_backward_dot(transpose_a, transpose_b)\n\n\n\n\nArguments\n\n\n\n\ntranspose_a::boolean, optional, default=False\n: True if the first matrix is transposed.\n\n\ntranspose_b::boolean, optional, default=False\n: True if the second matrix is tranposed.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_expm1\n \n \nMethod\n.\n\n\n_backward_expm1(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_gamma\n \n \nMethod\n.\n\n\n_backward_gamma(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_gammaln\n \n \nMethod\n.\n\n\n_backward_gammaln(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_hypot\n \n \nMethod\n.\n\n\n_backward_hypot()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_hypot_scalar\n \n \nMethod\n.\n\n\n_backward_hypot_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nscalar::float\n: scalar value\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_log\n \n \nMethod\n.\n\n\n_backward_log(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_log1p\n \n \nMethod\n.\n\n\n_backward_log1p(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_max\n \n \nMethod\n.\n\n\n_backward_max()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_maximum\n \n \nMethod\n.\n\n\n_backward_maximum()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_maximum_scalar\n \n \nMethod\n.\n\n\n_backward_maximum_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nscalar::float\n: scalar value\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_mean\n \n \nMethod\n.\n\n\n_backward_mean()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_min\n \n \nMethod\n.\n\n\n_backward_min()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_minimum\n \n \nMethod\n.\n\n\n_backward_minimum()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_minimum_scalar\n \n \nMethod\n.\n\n\n_backward_minimum_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nscalar::float\n: scalar value\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_mul\n \n \nMethod\n.\n\n\n_backward_mul()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_nanprod\n \n \nMethod\n.\n\n\n_backward_nanprod()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_nansum\n \n \nMethod\n.\n\n\n_backward_nansum()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_power\n \n \nMethod\n.\n\n\n_backward_power()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_power_scalar\n \n \nMethod\n.\n\n\n_backward_power_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nscalar::float\n: scalar value\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_prod\n \n \nMethod\n.\n\n\n_backward_prod()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_radians\n \n \nMethod\n.\n\n\n_backward_radians(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_rdiv_scalar\n \n \nMethod\n.\n\n\n_backward_rdiv_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nscalar::float\n: scalar value\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_rpower_scalar\n \n \nMethod\n.\n\n\n_backward_rpower_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nscalar::float\n: scalar value\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_rsqrt\n \n \nMethod\n.\n\n\n_backward_rsqrt(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sign\n \n \nMethod\n.\n\n\n_backward_sign(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sin\n \n \nMethod\n.\n\n\n_backward_sin(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sinh\n \n \nMethod\n.\n\n\n_backward_sinh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_slice_axis\n \n \nMethod\n.\n\n\n_backward_slice_axis()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_smooth_l1\n \n \nMethod\n.\n\n\n_backward_smooth_l1(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_softmax_cross_entropy\n \n \nMethod\n.\n\n\n_backward_softmax_cross_entropy()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sqrt\n \n \nMethod\n.\n\n\n_backward_sqrt(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_square\n \n \nMethod\n.\n\n\n_backward_square(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sub\n \n \nMethod\n.\n\n\n_backward_sub()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sum\n \n \nMethod\n.\n\n\n_backward_sum()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_take\n \n \nMethod\n.\n\n\n_backward_take()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_tan\n \n \nMethod\n.\n\n\n_backward_tan(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_tanh\n \n \nMethod\n.\n\n\n_backward_tanh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_topk\n \n \nMethod\n.\n\n\n_backward_topk()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._broadcast\n \n \nMethod\n.\n\n\n_broadcast(src, axis, size)\n\n\n\n\nBroadcast array in the given axis to the given size\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: source ndarray\n\n\naxis::int\n: axis to broadcast\n\n\nsize::int\n: size of broadcast\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._broadcast_backward\n \n \nMethod\n.\n\n\n_broadcast_backward()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._copy\n \n \nMethod\n.\n\n\n_copy(data)\n\n\n\n\nIdentity mapping, copy src to output\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:14\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._copyto\n \n \nMethod\n.\n\n\n_copyto(src)\n\n\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._crop_assign\n \n \nMethod\n.\n\n\n_crop_assign(lhs, rhs, begin, end)\n\n\n\n\n(Assign the rhs to a cropped subset of lhs.\n\n\nRequirements\n\n\n\n\noutput should be explicitly given and be the same as lhs.\n\n\nlhs and rhs are of the same data type, and on the same device.\n\n\n\n\n)\n\n\nFrom:src/operator/tensor/matrix_op.cc:159\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Source input\n\n\nrhs::NDArray\n: value to assign\n\n\nbegin::Shape(tuple), required\n: starting coordinates\n\n\nend::Shape(tuple), required\n: ending coordinates\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._crop_assign_scalar\n \n \nMethod\n.\n\n\n_crop_assign_scalar(data, scalar, begin, end)\n\n\n\n\n(Assign the scalar to a cropped subset of the input.\n\n\nRequirements\n\n\n\n\noutput should be explicitly given and be the same as input\n\n\n\n\n)\n\n\nFrom:src/operator/tensor/matrix_op.cc:183\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nscalar::float, optional, default=0\n: The scalar value for assignment.\n\n\nbegin::Shape(tuple), required\n: starting coordinates\n\n\nend::Shape(tuple), required\n: ending coordinates\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._cvcopyMakeBorder\n \n \nMethod\n.\n\n\n_cvcopyMakeBorder(src, top, bot, left, right, type, value)\n\n\n\n\nPad image border with OpenCV. \n\n\nArguments\n\n\n\n\nsrc::NDArray\n: source image\n\n\ntop::int, required\n: Top margin.\n\n\nbot::int, required\n: Bottom margin.\n\n\nleft::int, required\n: Left margin.\n\n\nright::int, required\n: Right margin.\n\n\ntype::int, optional, default='0'\n: Filling type (default=cv2.BORDER_CONSTANT).\n\n\nvalue::double, optional, default=0\n: Fill with value.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._cvimdecode\n \n \nMethod\n.\n\n\n_cvimdecode(buf, flag, to_rgb)\n\n\n\n\nDecode image with OpenCV.  Note: return image in RGB by default, instead of OpenCV's default BGR.\n\n\nArguments\n\n\n\n\nbuf::NDArray\n: Buffer containing binary encoded image\n\n\nflag::int, optional, default='1'\n: Convert decoded image to grayscale (0) or color (1).\n\n\nto_rgb::boolean, optional, default=True\n: Whether to convert decoded image to mxnet's default RGB format (instead of opencv's default BGR).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._cvimresize\n \n \nMethod\n.\n\n\n_cvimresize(src, w, h, interp)\n\n\n\n\nResize image with OpenCV. \n\n\nArguments\n\n\n\n\nsrc::NDArray\n: source image\n\n\nw::int, required\n: Width of resized image.\n\n\nh::int, required\n: Height of resized image.\n\n\ninterp::int, optional, default='1'\n: Interpolation method (default=cv2.INTER_LINEAR).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._div_scalar\n \n \nMethod\n.\n\n\n_div_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._equal\n \n \nMethod\n.\n\n\n_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._equal_scalar\n \n \nMethod\n.\n\n\n_equal_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._grad_add\n \n \nMethod\n.\n\n\n_grad_add(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._greater\n \n \nMethod\n.\n\n\n_greater(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._greater_equal\n \n \nMethod\n.\n\n\n_greater_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._greater_equal_scalar\n \n \nMethod\n.\n\n\n_greater_equal_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._greater_scalar\n \n \nMethod\n.\n\n\n_greater_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._hypot\n \n \nMethod\n.\n\n\n_hypot(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._hypot_scalar\n \n \nMethod\n.\n\n\n_hypot_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._identity_with_attr_like_rhs\n \n \nMethod\n.\n\n\n_identity_with_attr_like_rhs()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._imdecode\n \n \nMethod\n.\n\n\n_imdecode(mean, index, x0, y0, x1, y1, c, size)\n\n\n\n\nDecode an image, clip to (x0, y0, x1, y1), subtract mean, and write to buffer\n\n\nArguments\n\n\n\n\nmean::NDArray\n: image mean\n\n\nindex::int\n: buffer position for output\n\n\nx0::int\n: x0\n\n\ny0::int\n: y0\n\n\nx1::int\n: x1\n\n\ny1::int\n: y1\n\n\nc::int\n: channel\n\n\nsize::int\n: length of str_img\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._lesser\n \n \nMethod\n.\n\n\n_lesser(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._lesser_equal\n \n \nMethod\n.\n\n\n_lesser_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._lesser_equal_scalar\n \n \nMethod\n.\n\n\n_lesser_equal_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._lesser_scalar\n \n \nMethod\n.\n\n\n_lesser_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._maximum\n \n \nMethod\n.\n\n\n_maximum(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._maximum_scalar\n \n \nMethod\n.\n\n\n_maximum_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minimum\n \n \nMethod\n.\n\n\n_minimum(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minimum_scalar\n \n \nMethod\n.\n\n\n_minimum_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minus\n \n \nMethod\n.\n\n\n_minus(lhs, rhs)\n\n\n\n\n_minus is an alias of _sub.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minus_scalar\n \n \nMethod\n.\n\n\n_minus_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._mul\n \n \nMethod\n.\n\n\n_mul(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._mul_scalar\n \n \nMethod\n.\n\n\n_mul_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._not_equal\n \n \nMethod\n.\n\n\n_not_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._not_equal_scalar\n \n \nMethod\n.\n\n\n_not_equal_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._onehot_encode\n \n \nMethod\n.\n\n\n_onehot_encode(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand to the function.\n\n\nrhs::NDArray\n: Right operand to the function.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._ones\n \n \nMethod\n.\n\n\n_ones(shape, ctx, dtype)\n\n\n\n\nfill target with ones\n\n\nArguments\n\n\n\n\nshape::Shape(tuple), optional, default=()\n: The shape of the output\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n.Only used for imperative calls.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32'\n: Target data type.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._plus\n \n \nMethod\n.\n\n\n_plus(lhs, rhs)\n\n\n\n\n_plus is an alias of elemwise_add.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._plus_scalar\n \n \nMethod\n.\n\n\n_plus_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._power\n \n \nMethod\n.\n\n\n_power(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._power_scalar\n \n \nMethod\n.\n\n\n_power_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._rdiv_scalar\n \n \nMethod\n.\n\n\n_rdiv_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._rminus_scalar\n \n \nMethod\n.\n\n\n_rminus_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._rpower_scalar\n \n \nMethod\n.\n\n\n_rpower_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_normal\n \n \nMethod\n.\n\n\n_sample_normal(loc, scale, shape, ctx, dtype)\n\n\n\n\n_sample_normal is an alias of normal.\n\n\nSample a normal distribution\n\n\nArguments\n\n\n\n\nloc::float, optional, default=0\n: Mean of the distribution.\n\n\nscale::float, optional, default=1\n: Standard deviation of the distribution.\n\n\nshape::Shape(tuple), optional, default=()\n: The shape of the output\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n.Only used for imperative calls.\n\n\ndtype::{'float32'},optional, default='float32'\n: DType of the output\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_uniform\n \n \nMethod\n.\n\n\n_sample_uniform(low, high, shape, ctx, dtype)\n\n\n\n\n_sample_uniform is an alias of uniform.\n\n\nSample a uniform distribution\n\n\nArguments\n\n\n\n\nlow::float, optional, default=0\n: The lower bound of distribution\n\n\nhigh::float, optional, default=1\n: The upper bound of distribution\n\n\nshape::Shape(tuple), optional, default=()\n: The shape of the output\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n.Only used for imperative calls.\n\n\ndtype::{'float32'},optional, default='float32'\n: DType of the output\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._zeros\n \n \nMethod\n.\n\n\n_zeros(shape, ctx, dtype)\n\n\n\n\nfill target with zeros\n\n\nArguments\n\n\n\n\nshape::Shape(tuple), optional, default=()\n: The shape of the output\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n.Only used for imperative calls.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32'\n: Target data type.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.adam_update\n \n \nMethod\n.\n\n\nadam_update()\n\n\n\n\nUpdater function for adam optimizer\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.arccos\n \n \nMethod\n.\n\n\narccos(data)\n\n\n\n\nTake arccos of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:216\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.arccosh\n \n \nMethod\n.\n\n\narccosh(data)\n\n\n\n\nTake arccosh of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:288\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.arcsin\n \n \nMethod\n.\n\n\narcsin(data)\n\n\n\n\nTake arcsin of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:207\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.arcsinh\n \n \nMethod\n.\n\n\narcsinh(data)\n\n\n\n\nTake arcsinh of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:279\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.arctan\n \n \nMethod\n.\n\n\narctan(data)\n\n\n\n\nTake arctan of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:225\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.arctanh\n \n \nMethod\n.\n\n\narctanh(data)\n\n\n\n\nTake arctanh of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:297\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.argmax\n \n \nMethod\n.\n\n\nargmax(data, axis, keepdims)\n\n\n\n\nCompute argmax\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_index.cc:11\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::int, optional, default='-1'\n: Empty or unsigned. The axis to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.argmax_channel\n \n \nMethod\n.\n\n\nargmax_channel(src)\n\n\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.argmin\n \n \nMethod\n.\n\n\nargmin(data, axis, keepdims)\n\n\n\n\nCompute argmin\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_index.cc:16\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::int, optional, default='-1'\n: Empty or unsigned. The axis to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.argsort\n \n \nMethod\n.\n\n\nargsort(src, axis, is_ascend)\n\n\n\n\nReturns the indices that would sort an array.\n\n\nFrom:src/operator/tensor/ordering_op.cc:89\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input\n\n\naxis::int or None, optional, default='-1'\n: Axis along which to sort the input tensor. If not given, the flattened array is used. Default is -1.\n\n\nis_ascend::boolean, optional, default=True\n: Whether sort in ascending or descending order.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.batch_dot\n \n \nMethod\n.\n\n\nbatch_dot(lhs, rhs, transpose_a, transpose_b)\n\n\n\n\nCalculate batched dot product of two matrices. (batch, M, K) X (batch, K, N) \u2013\n (batch, M, N).\n\n\nFrom:src/operator/tensor/matrix_op.cc:257\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left input\n\n\nrhs::NDArray\n: Right input\n\n\ntranspose_a::boolean, optional, default=False\n: True if the first matrix is transposed.\n\n\ntranspose_b::boolean, optional, default=False\n: True if the second matrix is tranposed.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.batch_take\n \n \nMethod\n.\n\n\nbatch_take(a, indices)\n\n\n\n\nTake scalar value from a batch of data vectos according to an index vector, i.e. out[i] = a[i, indices[i]]\n\n\nFrom:src/operator/tensor/indexing_op.cc:97\n\n\nArguments\n\n\n\n\na::NDArray\n: Input data array\n\n\nindices::NDArray\n: index array\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_add\n \n \nMethod\n.\n\n\nbroadcast_add(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_axis\n \n \nMethod\n.\n\n\nbroadcast_axis(data, axis, size)\n\n\n\n\nBroadcast src along axis\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:85\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: The axes to perform the broadcasting.\n\n\nsize::Shape(tuple), optional, default=()\n: Target sizes of the broadcasting axes.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_div\n \n \nMethod\n.\n\n\nbroadcast_div(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_equal\n \n \nMethod\n.\n\n\nbroadcast_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_greater\n \n \nMethod\n.\n\n\nbroadcast_greater(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_greater_equal\n \n \nMethod\n.\n\n\nbroadcast_greater_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_hypot\n \n \nMethod\n.\n\n\nbroadcast_hypot(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_lesser\n \n \nMethod\n.\n\n\nbroadcast_lesser(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_lesser_equal\n \n \nMethod\n.\n\n\nbroadcast_lesser_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_maximum\n \n \nMethod\n.\n\n\nbroadcast_maximum(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_minimum\n \n \nMethod\n.\n\n\nbroadcast_minimum(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_minus\n \n \nMethod\n.\n\n\nbroadcast_minus(lhs, rhs)\n\n\n\n\nbroadcast_minus is an alias of broadcast_sub.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_mul\n \n \nMethod\n.\n\n\nbroadcast_mul(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_not_equal\n \n \nMethod\n.\n\n\nbroadcast_not_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_plus\n \n \nMethod\n.\n\n\nbroadcast_plus(lhs, rhs)\n\n\n\n\nbroadcast_plus is an alias of broadcast_add.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_power\n \n \nMethod\n.\n\n\nbroadcast_power(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_sub\n \n \nMethod\n.\n\n\nbroadcast_sub(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_to\n \n \nMethod\n.\n\n\nbroadcast_to(data, shape)\n\n\n\n\nBroadcast src to shape\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:92\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nshape::Shape(tuple), optional, default=()\n: The shape of the desired array. We can set the dim to zero if it's same as the original. E.g \nA = broadcast_to(B, shape=(10, 0, 0))\n has the same meaning as \nA = broadcast_axis(B, axis=0, size=10)\n.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.choose_element_0index\n \n \nMethod\n.\n\n\nchoose_element_0index(lhs, rhs)\n\n\n\n\nChoose one element from each line(row for python, column for R/Julia) in lhs according to index indicated by rhs. This function assume rhs uses 0-based index.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand to the function.\n\n\nrhs::NDArray\n: Right operand to the function.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.clip\n \n \nMethod\n.\n\n\nclip(data, a_min, a_max)\n\n\n\n\nClip ndarray elements to range (a_min, a_max)\n\n\nFrom:src/operator/tensor/matrix_op.cc:289\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\na_min::float, required\n: Minimum value\n\n\na_max::float, required\n: Maximum value\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.crop\n \n \nMethod\n.\n\n\ncrop(data, begin, end)\n\n\n\n\n(Crop the input tensor and return a new one.\n\n\nRequirements\n\n\n\n\nthe input and output (if explicitly given) are of the same data type, and on the same device.\n\n\n\n\n)\n\n\nFrom:src/operator/tensor/matrix_op.cc:143\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nbegin::Shape(tuple), required\n: starting coordinates\n\n\nend::Shape(tuple), required\n: ending coordinates\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.degrees\n \n \nMethod\n.\n\n\ndegrees(data)\n\n\n\n\nTake degrees of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:234\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.elemwise_add\n \n \nMethod\n.\n\n\nelemwise_add(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: first input\n\n\nrhs::NDArray\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.expand_dims\n \n \nMethod\n.\n\n\nexpand_dims(data, axis)\n\n\n\n\nExpand the shape of array by inserting a new axis.\n\n\nFrom:src/operator/tensor/matrix_op.cc:122\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::int (non-negative), required\n: Position (amongst axes) where new axis is to be inserted.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.fill_element_0index\n \n \nMethod\n.\n\n\nfill_element_0index(lhs, mhs, rhs)\n\n\n\n\nFill one element of each line(row for python, column for R/Julia) in lhs according to index indicated by rhs and values indicated by mhs. This function assume rhs uses 0-based index.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand to the function.\n\n\nmhs::NDArray\n: Middle operand to the function.\n\n\nrhs::NDArray\n: Right operand to the function.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.fix\n \n \nMethod\n.\n\n\nfix(data)\n\n\n\n\nTake round of the src to integer nearest 0\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:102\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.flip\n \n \nMethod\n.\n\n\nflip(data, axis)\n\n\n\n\nFlip the input tensor along axis and return a new one.\n\n\nFrom:src/operator/tensor/matrix_op.cc:219\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::int, required\n: The dimension to flip\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.from_json\n \n \nMethod\n.\n\n\nfrom_json(repr :: AbstractString, ::Type{SymbolicNode})\n\n\n\n\nLoad a \nSymbolicNode\n from a JSON string representation.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.gammaln\n \n \nMethod\n.\n\n\ngammaln(data)\n\n\n\n\nTake gammaln (log of the absolute value of gamma(x)) of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:315\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_attr\n \n \nMethod\n.\n\n\nget_attr(self :: SymbolicNode, key :: Symbol)\n\n\n\n\nGet attribute attached to this \nSymbolicNode\n belonging to key.\n\n\nReturns the value belonging to key as a \nNullable\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_internals\n \n \nMethod\n.\n\n\nget_internals(self :: SymbolicNode)\n\n\n\n\nGet a new grouped \nSymbolicNode\n whose output contains all the internal outputs of this \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.grad\n \n \nMethod\n.\n\n\ngrad(self :: SymbolicNode, wrt :: Vector{SymbolicNode})\n\n\n\n\nGet the autodiff gradient of the current \nSymbolicNode\n. This function can only be used if the current symbol is a loss function.\n\n\nArguments:\n\n\n\n\nself::SymbolicNode\n: current node.\n\n\nwrt::Vector{Symbol}\n: the names of the arguments to the gradient.\n\n\n\n\nReturns a gradient symbol of the corresponding gradient.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.infer_shape\n \n \nMethod\n.\n\n\ninfer_shape(self :: SymbolicNode, args...)\ninfer_shape(self :: SymbolicNode; kwargs...)\n\n\n\n\nDo shape inference according to the input shapes. The input shapes could be provided as a list of shapes, which should specify the shapes of inputs in the same order as the arguments returned by \nlist_arguments\n. Alternatively, the shape information could be specified via keyword arguments.\n\n\nReturns a 3-tuple containing shapes of all the arguments, shapes of all the outputs and shapes of all the auxiliary variables. If shape inference failed due to incomplete or incompatible inputs, the return value will be \n(nothing, nothing, nothing)\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.infer_type\n \n \nMethod\n.\n\n\ninfer_type(self :: SymbolicNode; kwargs...)\ninfer_type(self :: SymbolicNode, args...)\n\n\n\n\nDo type inference according to the input types. The input types could be provided as a list of types, which should specify the types of inputs in the same order as the arguments returned by \nlist_arguments\n. Alternatively, the type information could be specified via keyword arguments.\n\n\nReturns a 3-tuple containing types of all the arguments, types of all the outputs and types of all the auxiliary variables. If type inference failed due to incomplete or incompatible inputs, the return value will be \n(nothing, nothing, nothing)\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.list_all_attr\n \n \nMethod\n.\n\n\nlist_all_attr(self :: SymbolicNode)\n\n\n\n\nGet all attributes from the symbol graph.\n\n\nReturns a dictionary of attributes.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.list_arguments\n \n \nMethod\n.\n\n\nlist_arguments(self :: SymbolicNode)\n\n\n\n\nList all the arguments of this node. The argument for a node contains both the inputs and parameters. For example, a \nFullyConnected\n node will have both data and weights in its arguments. A composed node (e.g. a MLP) will list all the arguments for intermediate nodes.\n\n\nReturns a list of symbols indicating the names of the arguments.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.list_attr\n \n \nMethod\n.\n\n\nlist_attr(self :: SymbolicNode)\n\n\n\n\nGet all attributes from a symbol.\n\n\nReturns a dictionary of attributes.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.list_auxiliary_states\n \n \nMethod\n.\n\n\nlist_auxiliary_states(self :: SymbolicNode)\n\n\n\n\nList all auxiliary states in the symbool.\n\n\nAuxiliary states are special states of symbols that do not corresponds to an argument, and do not have gradient. But still be useful for the specific operations. A common example of auxiliary state is the moving_mean and moving_variance in BatchNorm. Most operators do not have Auxiliary states.\n\n\nReturns a list of symbols indicating the names of the auxiliary states.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.list_outputs\n \n \nMethod\n.\n\n\nlist_outputs(self :: SymbolicNode)\n\n\n\n\nList all the outputs of this node.\n\n\nReturns a list of symbols indicating the names of the outputs.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.load\n \n \nMethod\n.\n\n\nload(filename :: AbstractString, ::Type{SymbolicNode})\n\n\n\n\nLoad a \nSymbolicNode\n from a JSON file.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.max_axis\n \n \nMethod\n.\n\n\nmax_axis(data, axis, keepdims)\n\n\n\n\nmax_axis is an alias of max.\n\n\nCompute max along axis. If axis is empty, global reduction is performed\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:66\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.min_axis\n \n \nMethod\n.\n\n\nmin_axis(data, axis, keepdims)\n\n\n\n\nmin_axis is an alias of min.\n\n\nCompute min along axis. If axis is empty, global reduction is performed\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:76\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.nanprod\n \n \nMethod\n.\n\n\nnanprod(data, axis, keepdims)\n\n\n\n\nCompute product of src along axis, ignoring NaN values. If axis is empty, global reduction is performed\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:56\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.nansum\n \n \nMethod\n.\n\n\nnansum(data, axis, keepdims)\n\n\n\n\nSum src along axis, ignoring NaN values. If axis is empty, global reduction is performed\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:46\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.negative\n \n \nMethod\n.\n\n\nnegative(data)\n\n\n\n\nNegate src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:57\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.normal\n \n \nMethod\n.\n\n\nnormal(loc, scale, shape, ctx, dtype)\n\n\n\n\nSample a normal distribution\n\n\nArguments\n\n\n\n\nloc::float, optional, default=0\n: Mean of the distribution.\n\n\nscale::float, optional, default=1\n: Standard deviation of the distribution.\n\n\nshape::Shape(tuple), optional, default=()\n: The shape of the output\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n.Only used for imperative calls.\n\n\ndtype::{'float32'},optional, default='float32'\n: DType of the output\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.radians\n \n \nMethod\n.\n\n\nradians(data)\n\n\n\n\nTake radians of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:243\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.rint\n \n \nMethod\n.\n\n\nrint(data)\n\n\n\n\nTake round of the src to nearest integer\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:97\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.rsqrt\n \n \nMethod\n.\n\n\nrsqrt(data)\n\n\n\n\nTake reciprocal square root of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:125\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.save\n \n \nMethod\n.\n\n\nsave(filename :: AbstractString, node :: SymbolicNode)\n\n\n\n\nSave a \nSymbolicNode\n to a JSON file.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.set_attr\n \n \nMethod\n.\n\n\nset_attr(self:: SymbolicNode, key :: Symbol, value :: AbstractString)\n\n\n\n\nSet the attribute key to value for this \nSymbolicNode\n.\n\n\n\n\nNote\n\n\nIt is encouraged not to call this function directly, unless you know exactly what you are doing. The recommended way of setting attributes is when creating the \nSymbolicNode\n. Changing the attributes of a \nSymbolicNode\n that is already been used somewhere else might cause unexpected behavior and inconsistency.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sgd_mom_update\n \n \nMethod\n.\n\n\nsgd_mom_update()\n\n\n\n\nUpdater function for sgd optimizer\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sgd_update\n \n \nMethod\n.\n\n\nsgd_update()\n\n\n\n\nUpdater function for sgd optimizer\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.slice_axis\n \n \nMethod\n.\n\n\nslice_axis(data, axis, begin, end)\n\n\n\n\nSlice the input along certain axis and return a sliced array. The slice will be taken from [begin, end). end can be None and axis can be negative.\n\n\nFrom:src/operator/tensor/matrix_op.cc:200\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::int, required\n: The axis to be sliced. Negative axis means to count from the last to the first axis.\n\n\nbegin::int, required\n: The beginning index to be sliced. Negative values are interpreted as counting from the backward.\n\n\nend::int or None, required\n: The end index to be sliced. The end can be None, in which case all the rest elements are used. Also, negative values are interpreted as counting from the backward.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.smooth_l1\n \n \nMethod\n.\n\n\nsmooth_l1(data, scalar)\n\n\n\n\nCalculate Smooth L1 Loss(lhs, scalar)\n\n\nFrom:src/operator/tensor/elemwise_binary_scalar_op_extended.cc:63\n\n\nArguments\n\n\n\n\ndata::NDArray\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.softmax_cross_entropy\n \n \nMethod\n.\n\n\nsoftmax_cross_entropy(data, label)\n\n\n\n\nCalculate cross_entropy(data, one_hot(label))\n\n\nFrom:src/operator/loss_binary_op.cc:12\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Input data\n\n\nlabel::NDArray\n: Input label\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.square\n \n \nMethod\n.\n\n\nsquare(data)\n\n\n\n\nTake square of the src\n\n\nFrom:src/operator/tensor/elemwise_unary_op.cc:107\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sum_axis\n \n \nMethod\n.\n\n\nsum_axis(data, axis, keepdims)\n\n\n\n\nsum_axis is an alias of sum.\n\n\nSum src along axis. If axis is empty, global reduction is performed\n\n\nFrom:src/operator/tensor/broadcast_reduce_op_value.cc:17\n\n\nArguments\n\n\n\n\ndata::NDArray\n: Source input\n\n\naxis::Shape(tuple), optional, default=()\n: Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: If true, the axis which is reduced is left in the result as dimension with size one.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.to_json\n \n \nMethod\n.\n\n\nto_json(self :: SymbolicNode)\n\n\n\n\nConvert a \nSymbolicNode\n into a JSON string.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.topk\n \n \nMethod\n.\n\n\ntopk(src, axis, k, ret_typ, is_ascend)\n\n\n\n\nReturn the top k element of an input tensor along a given axis.\n\n\nFrom:src/operator/tensor/ordering_op.cc:18\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input\n\n\naxis::int or None, optional, default='-1'\n: Axis along which to choose the top k indices. If not given, the flattened array is used. Default is -1.\n\n\nk::int, optional, default='1'\n: Number of top elements to select, should be always smaller than or equal to the element number in the given axis. A global sort is performed if set k \n 1.\n\n\nret_typ::{'both', 'indices', 'mask', 'value'},optional, default='indices'\n: The return type. \"value\" means returning the top k values, \"indices\" means returning the indices of the top k values, \"mask\" means to return a mask array containing 0 and 1. 1 means the top k values. \"both\" means to return both value and indices.\n\n\nis_ascend::boolean, optional, default=False\n: Whether to choose k largest or k smallest. Top K largest elements will be chosen if set to false.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.uniform\n \n \nMethod\n.\n\n\nuniform(low, high, shape, ctx, dtype)\n\n\n\n\nSample a uniform distribution\n\n\nArguments\n\n\n\n\nlow::float, optional, default=0\n: The lower bound of distribution\n\n\nhigh::float, optional, default=1\n: The upper bound of distribution\n\n\nshape::Shape(tuple), optional, default=()\n: The shape of the output\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n.Only used for imperative calls.\n\n\ndtype::{'float32'},optional, default='float32'\n: DType of the output\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource", 
            "title": "Symbolic API"
        }, 
        {
            "location": "/api/symbolic-node/#symbolic-api", 
            "text": "#  Base.Flatten     Method .  Flatten(data)  Flatten input into 2D by collapsing all the higher dimensions. A (d1, d2, ..., dK) tensor is flatten to (d1, d2  ...  dK) matrix.  Arguments   data::NDArray : Input data to reshape.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.SymbolicNode     Type .  SymbolicNode  SymbolicNode is the basic building block of the symbolic graph in MXNet.jl.  (self :: SymbolicNode)(args :: SymbolicNode...)\n(self :: SymbolicNode)(; kwargs...)  Make a new node by composing  self  with  args . Or the arguments can be specified using keyword arguments.  source  #  Base.LinAlg.dot     Method .  dot(lhs, rhs, transpose_a, transpose_b)  Calculate dot product of two matrices or two vectors.  From:src/operator/tensor/matrix_op.cc:231  Arguments   lhs::NDArray : Left input  rhs::NDArray : Right input  transpose_a::boolean, optional, default=False : True if the first matrix is transposed.  transpose_b::boolean, optional, default=False : True if the second matrix is tranposed.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.LinAlg.norm     Method .  norm(src)  Arguments   src::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.Math.gamma     Method .  gamma(data)  Take the gamma function (extension of the factorial function) of the src  From:src/operator/tensor/elemwise_unary_op.cc:306  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base._div     Method .  _div(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base._sub     Method .  _sub(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.abs     Method .  abs(data)  Take absolute value of the src  From:src/operator/tensor/elemwise_unary_op.cc:63  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.ceil     Method .  ceil(data)  Take ceil of the src  From:src/operator/tensor/elemwise_unary_op.cc:87  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.copy     Method .  copy(self :: SymbolicNode)  Make a copy of a SymbolicNode. The same as making a deep copy.  source  #  Base.cos     Method .  cos(data)  Take cos of the src  From:src/operator/tensor/elemwise_unary_op.cc:189  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.cosh     Method .  cosh(data)  Take cosh of the src  From:src/operator/tensor/elemwise_unary_op.cc:261  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.deepcopy     Method .  deepcopy(self :: SymbolicNode)  Make a deep copy of a SymbolicNode.  source  #  Base.exp     Method .  exp(data)  Take exp of the src  From:src/operator/tensor/elemwise_unary_op.cc:135  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.expm1     Method .  expm1(data)  Take  exp(x) - 1  in a numerically stable way  From:src/operator/tensor/elemwise_unary_op.cc:180  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.floor     Method .  floor(data)  Take floor of the src  From:src/operator/tensor/elemwise_unary_op.cc:92  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.getindex     Method .  getindex(self :: SymbolicNode, idx :: Union{Int, Base.Symbol, AbstractString})  Get a node representing the specified output of this node. The index could be a symbol or string indicating the name of the output, or a 1-based integer indicating the index, as in the list of  list_outputs .  source  #  Base.identity     Method .  identity(data)  identity is an alias of _copy.  Identity mapping, copy src to output  From:src/operator/tensor/elemwise_unary_op.cc:14  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.log     Method .  log(data)  Take log of the src  From:src/operator/tensor/elemwise_unary_op.cc:141  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.log10     Method .  log10(data)  Take base-10 log of the src  From:src/operator/tensor/elemwise_unary_op.cc:147  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.log1p     Method .  log1p(data)  Take  log(1 + x)  in a numerically stable way  From:src/operator/tensor/elemwise_unary_op.cc:171  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.log2     Method .  log2(data)  Take base-2 log of the src  From:src/operator/tensor/elemwise_unary_op.cc:153  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.max     Method .  max(data, axis, keepdims)  Compute max along axis. If axis is empty, global reduction is performed  From:src/operator/tensor/broadcast_reduce_op_value.cc:66  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.mean     Method .  mean(data, axis, keepdims)  Compute mean src along axis. If axis is empty, global reduction is performed  From:src/operator/tensor/broadcast_reduce_op_value.cc:26  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.min     Method .  min(data, axis, keepdims)  Compute min along axis. If axis is empty, global reduction is performed  From:src/operator/tensor/broadcast_reduce_op_value.cc:76  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.prod     Method .  prod(data, axis, keepdims)  Compute product of src along axis. If axis is empty, global reduction is performed  From:src/operator/tensor/broadcast_reduce_op_value.cc:36  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.round     Method .  round(data)  Take round of the src  From:src/operator/tensor/elemwise_unary_op.cc:81  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.sign     Method .  sign(data)  Take sign of the src  From:src/operator/tensor/elemwise_unary_op.cc:72  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.sin     Method .  sin(data)  Take sin of the src  From:src/operator/tensor/elemwise_unary_op.cc:162  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.sinh     Method .  sinh(data)  Take sinh of the src  From:src/operator/tensor/elemwise_unary_op.cc:252  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.sort     Method .  sort(src, axis, is_ascend)  Return a sorted copy of an array.  From:src/operator/tensor/ordering_op.cc:59  Arguments   src::NDArray : Source input  axis::int or None, optional, default='-1' : Axis along which to choose sort the input tensor. If not given, the flattened array is used. Default is -1.  is_ascend::boolean, optional, default=True : Whether sort in ascending or descending order.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.sqrt     Method .  sqrt(data)  Take square root of the src  From:src/operator/tensor/elemwise_unary_op.cc:116  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.sum     Method .  sum(data, axis, keepdims)  Sum src along axis. If axis is empty, global reduction is performed  From:src/operator/tensor/broadcast_reduce_op_value.cc:17  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.take     Method .  take(a, indices, axis, mode)  Take row vectors from an NDArray according to the indices For an input of index with shape (d1, ..., dK), the output shape is (d1, ..., dK, row_vector_length).All the input values should be integers in the range [0, column_vector_length).  From:src/operator/tensor/indexing_op.cc:59  Arguments   a::SymbolicNode : The source array.  indices::SymbolicNode : The indices of the values to extract.  axis::int, optional, default='0' : the axis of data tensor to be taken.  mode::{'clip', 'raise', 'wrap'},optional, default='raise' : specify how out-of-bound indices bahave.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.tan     Method .  tan(data)  Take tan of the src  From:src/operator/tensor/elemwise_unary_op.cc:198  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.tanh     Method .  tanh(data)  Take tanh of the src  From:src/operator/tensor/elemwise_unary_op.cc:270  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.transpose     Method .  transpose(data, axes)  Transpose the input tensor and return a new one  From:src/operator/tensor/matrix_op.cc:94  Arguments   data::NDArray : Source input  axes::Shape(tuple), optional, default=() : Target axis order. By default the axes will be inverted.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Activation     Method .  Activation(data, act_type)  Elementwise activation function.  The following activation types are supported (operations are applied elementwisely to each scalar of the input tensor):   relu : Rectified Linear Unit,  y = max(x, 0)  sigmoid :  y = 1 / (1 + exp(-x))  tanh : Hyperbolic tangent,  y = (exp(x) - exp(-x)) / (exp(x) + exp(-x))  softrelu : Soft ReLU, or SoftPlus,  y = log(1 + exp(x))   See  LeakyReLU  for other activations with parameters.  Arguments   data::SymbolicNode : Input data to activation function.  act_type::{'relu', 'sigmoid', 'softrelu', 'tanh'}, required : Activation function to be applied.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.BatchNorm     Method .  BatchNorm(data, gamma, beta, eps, momentum, fix_gamma, use_global_stats, output_mean_var)  Apply batch normalization to input.  Arguments   data::SymbolicNode : Input data to batch normalization  gamma::SymbolicNode : gamma matrix  beta::SymbolicNode : beta matrix  eps::float, optional, default=0.001 : Epsilon to prevent div 0  momentum::float, optional, default=0.9 : Momentum for moving average  fix_gamma::boolean, optional, default=True : Fix gamma while training  use_global_stats::boolean, optional, default=False : Whether use global moving statistics instead of local batch-norm. This will force change batch-norm into a scale shift operator.  output_mean_var::boolean, optional, default=False : Output All,normal mean and var  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.BlockGrad     Method .  BlockGrad(data)  Get output from a symbol and pass 0 gradient back  From:src/operator/tensor/elemwise_unary_op.cc:30  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Cast     Method .  Cast(data, dtype)  Cast array to a different data type.  Arguments   data::SymbolicNode : Input data to cast function.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'}, required : Target data type.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Concat     Method .  Concat(data, num_args, dim)  Note : Concat takes variable number of positional inputs. So instead of calling as Concat([x, y, z], num_args=3), one should call via Concat(x, y, z), and num_args will be determined automatically.  Perform a feature concat on channel dim (defaut is 1) over all  Arguments   data::SymbolicNode[] : List of tensors to concatenate  num_args::int, required : Number of inputs to be concated.  dim::int, optional, default='1' : the dimension to be concated.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Convolution     Method .  Convolution(data, weight, bias, kernel, stride, dilate, pad, num_filter, num_group, workspace, no_bias, cudnn_tune, cudnn_off, layout)  Apply convolution to input then add a bias.  Arguments   data::SymbolicNode : Input data to the ConvolutionOp.  weight::SymbolicNode : Weight matrix.  bias::SymbolicNode : Bias parameter.  kernel::Shape(tuple), required : convolution kernel size: (h, w) or (d, h, w)  stride::Shape(tuple), optional, default=() : convolution stride: (h, w) or (d, h, w)  dilate::Shape(tuple), optional, default=() : convolution dilate: (h, w) or (d, h, w)  pad::Shape(tuple), optional, default=() : pad for convolution: (h, w) or (d, h, w)  num_filter::int (non-negative), required : convolution filter(channel) number  num_group::int (non-negative), optional, default=1 : Number of group partitions. Equivalent to slicing input into num_group   partitions, apply convolution on each, then concatenate the results  workspace::long (non-negative), optional, default=1024 : Maximum tmp workspace allowed for convolution (MB).  no_bias::boolean, optional, default=False : Whether to disable bias parameter.  cudnn_tune::{None, 'fastest', 'limited_workspace', 'off'},optional, default='None' : Whether to pick convolution algo by running performance test.   Leads to higher startup time but may give faster speed. Options are:   'off': no tuning   'limited_workspace': run test and pick the fastest algorithm that doesn't exceed workspace limit.   'fastest': pick the fastest algorithm and ignore workspace limit.   If set to None (default), behavior is determined by environment   variable MXNET_CUDNN_AUTOTUNE_DEFAULT: 0 for off,   1 for limited workspace (default), 2 for fastest.  cudnn_off::boolean, optional, default=False : Turn off cudnn for this layer.  layout::{None, 'NCDHW', 'NCHW', 'NDHWC', 'NHWC'},optional, default='None' : Set layout for input, output and weight. Empty for   default layout: NCHW for 2d and NCDHW for 3d.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Correlation     Method .  Correlation(data1, data2, kernel_size, max_displacement, stride1, stride2, pad_size, is_multiply)  Apply correlation to inputs  Arguments   data1::SymbolicNode : Input data1 to the correlation.  data2::SymbolicNode : Input data2 to the correlation.  kernel_size::int (non-negative), optional, default=1 : kernel size for Correlation must be an odd number  max_displacement::int (non-negative), optional, default=1 : Max displacement of Correlation  stride1::int (non-negative), optional, default=1 : stride1 quantize data1 globally  stride2::int (non-negative), optional, default=1 : stride2 quantize data2 within the neighborhood centered around data1  pad_size::int (non-negative), optional, default=0 : pad for Correlation  is_multiply::boolean, optional, default=True : operation type is either multiplication or subduction  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Crop     Method .  Crop(data, num_args, offset, h_w, center_crop)  Note : Crop takes variable number of positional inputs. So instead of calling as Crop([x, y, z], num_args=3), one should call via Crop(x, y, z), and num_args will be determined automatically.  Crop the 2nd and 3rd dim of input data, with the corresponding size of h_w or with width and height of the second input symbol, i.e., with one input, we need h_w to specify the crop height and width, otherwise the second input symbol's size will be used  Arguments   data::SymbolicNode or SymbolicNode[] : Tensor or List of Tensors, the second input will be used as crop_like shape reference  num_args::int, required : Number of inputs for crop, if equals one, then we will use the h_wfor crop height and width, else if equals two, then we will use the heightand width of the second input symbol, we name crop_like here  offset::Shape(tuple), optional, default=(0,0) : crop offset coordinate: (y, x)  h_w::Shape(tuple), optional, default=(0,0) : crop height and weight: (h, w)  center_crop::boolean, optional, default=False : If set to true, then it will use be the center_crop,or it will crop using the shape of crop_like  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Custom     Method .  Custom(op_type)  Custom operator implemented in frontend.  Arguments   op_type::string : Type of custom operator. Must be registered first.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Deconvolution     Method .  Deconvolution(data, weight, bias, kernel, stride, pad, adj, target_shape, num_filter, num_group, workspace, no_bias)  Apply deconvolution to input then add a bias.  Arguments   data::SymbolicNode : Input data to the DeconvolutionOp.  weight::SymbolicNode : Weight matrix.  bias::SymbolicNode : Bias parameter.  kernel::Shape(tuple), required : deconvolution kernel size: (y, x)  stride::Shape(tuple), optional, default=(1,1) : deconvolution stride: (y, x)  pad::Shape(tuple), optional, default=(0,0) : pad for deconvolution: (y, x), a good number is : (kernel-1)/2, if target_shape set, pad will be ignored and will be computed automatically  adj::Shape(tuple), optional, default=(0,0) : adjustment for output shape: (y, x), if target_shape set, adj will be ignored and will be computed automatically  target_shape::Shape(tuple), optional, default=(0,0) : output shape with targe shape : (y, x)  num_filter::int (non-negative), required : deconvolution filter(channel) number  num_group::int (non-negative), optional, default=1 : number of groups partition  workspace::long (non-negative), optional, default=512 : Tmp workspace for deconvolution (MB)  no_bias::boolean, optional, default=True : Whether to disable bias parameter.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Dropout     Method .  Dropout(data, p)  Apply dropout to input. During training, each element of the input is randomly set to zero with probability p. And then the whole tensor is rescaled by 1/(1-p) to keep the expectation the same as before applying dropout. During the test time, this behaves as an identity map.  Arguments   data::SymbolicNode : Input data to dropout.  p::float, optional, default=0.5 : Fraction of the input that gets dropped out at training time  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.ElementWiseSum     Method .  ElementWiseSum(args)  Note : ElementWiseSum takes variable number of positional inputs. So instead of calling as ElementWiseSum([x, y, z], num_args=3), one should call via ElementWiseSum(x, y, z), and num_args will be determined automatically.  Perform element sum of inputs  From:src/operator/tensor/elemwise_sum.cc:56  Arguments   args::NDArray[] : List of input tensors  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Embedding     Method .  Embedding(data, weight, input_dim, output_dim)  Map integer index to vector representations (embeddings). Those embeddings are learnable parameters. For a input of shape (d1, ..., dK), the output shape is (d1, ..., dK, output_dim). All the input values should be integers in the range [0, input_dim).  From:src/operator/tensor/indexing_op.cc:18  Arguments   data::SymbolicNode : Input data to the EmbeddingOp.  weight::SymbolicNode : Embedding weight matrix.  input_dim::int, required : vocabulary size of the input indices.  output_dim::int, required : dimension of the embedding vectors.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.FullyConnected     Method .  FullyConnected(data, weight, bias, num_hidden, no_bias)  Apply matrix multiplication to input then add a bias. It maps the input of shape  (batch_size, input_dim)  to the shape of  (batch_size, num_hidden) . Learnable parameters include the weights of the linear transform and an optional bias vector.  Arguments   data::SymbolicNode : Input data to the FullyConnectedOp.  weight::SymbolicNode : Weight matrix.  bias::SymbolicNode : Bias parameter.  num_hidden::int, required : Number of hidden nodes of the output.  no_bias::boolean, optional, default=False : Whether to disable bias parameter.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Group     Method .  Group(nodes :: SymbolicNode...)  Create a  SymbolicNode  by grouping nodes together.  source  #  MXNet.mx.IdentityAttachKLSparseReg     Method .  IdentityAttachKLSparseReg(data, sparseness_target, penalty, momentum)  Apply a sparse regularization to the output a sigmoid activation function.  Arguments   data::SymbolicNode : Input data.  sparseness_target::float, optional, default=0.1 : The sparseness target  penalty::float, optional, default=0.001 : The tradeoff parameter for the sparseness penalty  momentum::float, optional, default=0.9 : The momentum for running average  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.InstanceNorm     Method .  InstanceNorm(data, gamma, beta, eps)  An operator taking in a n-dimensional input tensor (n   2), and normalizing the input by subtracting the mean and variance calculated over the spatial dimensions. This is an implemention of the operator described in \"Instance Normalization: The Missing Ingredient for Fast Stylization\", D. Ulyanov, A. Vedaldi, V. Lempitsky, 2016 (arXiv:1607.08022v2). This layer is similar to batch normalization, with two differences: first, the normalization is carried out per example ('instance'), not over a batch. Second, the same normalization is applied both at test and train time. This operation is also known as 'contrast normalization'.  Arguments   data::SymbolicNode : A n-dimensional tensor (n   2) of the form [batch, channel, spatial_dim1, spatial_dim2, ...].  gamma::SymbolicNode : A vector of length 'channel', which multiplies the normalized input.  beta::SymbolicNode : A vector of length 'channel', which is added to the product of the normalized input and the weight.  eps::float, optional, default=0.001 : Epsilon to prevent division by 0.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.L2Normalization     Method .  L2Normalization(data, eps, mode)  Set the l2 norm of each instance to a constant.  Arguments   data::SymbolicNode : Input data to the L2NormalizationOp.  eps::float, optional, default=1e-10 : Epsilon to prevent div 0  mode::{'channel', 'instance', 'spatial'},optional, default='instance' : Normalization Mode. If set to instance, this operator will compute a norm for each instance in the batch; this is the default mode. If set to channel, this operator will compute a cross channel norm at each position of each instance. If set to spatial, this operator will compute a norm for each channel.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.LRN     Method .  LRN(data, alpha, beta, knorm, nsize)  Apply convolution to input then add a bias.  Arguments   data::SymbolicNode : Input data to the ConvolutionOp.  alpha::float, optional, default=0.0001 : value of the alpha variance scaling parameter in the normalization formula  beta::float, optional, default=0.75 : value of the beta power parameter in the normalization formula  knorm::float, optional, default=2 : value of the k parameter in normalization formula  nsize::int (non-negative), required : normalization window width in elements.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.LeakyReLU     Method .  LeakyReLU(data, act_type, slope, lower_bound, upper_bound)  Apply activation function to input.  Arguments   data::SymbolicNode : Input data to activation function.  act_type::{'elu', 'leaky', 'prelu', 'rrelu'},optional, default='leaky' : Activation function to be applied.  slope::float, optional, default=0.25 : Init slope for the activation. (For leaky and elu only)  lower_bound::float, optional, default=0.125 : Lower bound of random slope. (For rrelu only)  upper_bound::float, optional, default=0.334 : Upper bound of random slope. (For rrelu only)  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.LinearRegressionOutput     Method .  LinearRegressionOutput(data, label, grad_scale)  Use linear regression for final output, this is used on final output of a net.  Arguments   data::SymbolicNode : Input data to function.  label::SymbolicNode : Input label to function.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.LogisticRegressionOutput     Method .  LogisticRegressionOutput(data, label, grad_scale)  Use Logistic regression for final output, this is used on final output of a net. Logistic regression is suitable for binary classification or probability prediction tasks.  Arguments   data::SymbolicNode : Input data to function.  label::SymbolicNode : Input label to function.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.MAERegressionOutput     Method .  MAERegressionOutput(data, label, grad_scale)  Use mean absolute error regression for final output, this is used on final output of a net.  Arguments   data::SymbolicNode : Input data to function.  label::SymbolicNode : Input label to function.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.MakeLoss     Method .  MakeLoss(data, grad_scale, valid_thresh, normalization)  Get output from a symbol and pass 1 gradient back. This is used as a terminal loss if unary and binary operator are used to composite a loss with no declaration of backward dependency  Arguments   data::SymbolicNode : Input data.  grad_scale::float, optional, default=1 : gradient scale as a supplement to unary and binary operators  valid_thresh::float, optional, default=0 : regard element valid when x   valid_thresh, this is used only in valid normalization mode.  normalization::{'batch', 'null', 'valid'},optional, default='null' : If set to null, op will not normalize on output gradient.If set to batch, op will normalize gradient by divide batch size.If set to valid, op will normalize gradient by divide # sample marked as valid  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Pad     Method .  Pad(data, mode, pad_width, constant_value)  Pads an n-dimensional input tensor. Allows for precise control of the padding type and how much padding to apply on both sides of a given dimension.  Arguments   data::SymbolicNode : An n-dimensional input tensor.  mode::{'constant', 'edge'}, required : Padding type to use. \"constant\" pads all values with a constant value, the value of which can be specified with the constant_value option. \"edge\" uses the boundary values of the array as padding.  pad_width::Shape(tuple), required : A tuple of padding widths of length 2*r, where r is the rank of the input tensor, specifying number of values padded to the edges of each axis. (before_1, after_1, ... , before_N, after_N) unique pad widths for each axis. Equivalent to pad_width in numpy.pad, but flattened.  constant_value::double, optional, default=0 : This option is only used when mode is \"constant\". This value will be used as the padding value. Defaults to 0 if not specified.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Pooling     Method .  Pooling(data, global_pool, kernel, pool_type, pooling_convention, stride, pad)  Perform spatial pooling on inputs.  Arguments   data::SymbolicNode : Input data to the pooling operator.  global_pool::boolean, optional, default=False : Ignore kernel size, do global pooling based on current input feature map. This is useful for input with different shape  kernel::Shape(tuple), required : pooling kernel size: (y, x) or (d, y, x)  pool_type::{'avg', 'max', 'sum'}, required : Pooling type to be applied.  pooling_convention::{'full', 'valid'},optional, default='valid' : Pooling convention to be applied.kValid is default setting of Mxnet and rounds down the output pooling size.kFull is compatible with Caffe and rounds up the output pooling size.  stride::Shape(tuple), optional, default=(1,1) : stride: for pooling (y, x) or (d, y, x)  pad::Shape(tuple), optional, default=(0,0) : pad for pooling: (y, x) or (d, y, x)  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.RNN     Method .  RNN(data, parameters, state, state_cell, state_size, num_layers, bidirectional, mode, p, state_outputs)  Apply a recurrent layer to input.  Arguments   data::SymbolicNode : Input data to RNN  parameters::SymbolicNode : Vector of all RNN trainable parameters concatenated  state::SymbolicNode : initial hidden state of the RNN  state_cell::SymbolicNode : initial cell state for LSTM networks (only for LSTM)  state_size::int (non-negative), required : size of the state for each layer  num_layers::int (non-negative), required : number of stacked layers  bidirectional::boolean, optional, default=False : whether to use bidirectional recurrent layers  mode::{'gru', 'lstm', 'rnn_relu', 'rnn_tanh'}, required : the type of RNN to compute  p::float, optional, default=0 : Dropout probability, fraction of the input that gets dropped out at training time  state_outputs::boolean, optional, default=False : Whether to have the states as symbol outputs.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.ROIPooling     Method .  ROIPooling(data, rois, pooled_size, spatial_scale)  Performs region-of-interest pooling on inputs. Resize bounding box coordinates by spatial_scale and crop input feature maps accordingly. The cropped feature maps are pooled by max pooling to a fixed size output indicated by pooled_size. batch_size will change to the number of region bounding boxes after ROIPooling  Arguments   data::SymbolicNode : Input data to the pooling operator, a 4D Feature maps  rois::SymbolicNode : Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]]. (x1, y1) and (x2, y2) are top left and down right corners of designated region of interest. batch_index indicates the index of corresponding image in the input data  pooled_size::Shape(tuple), required : fix pooled size: (h, w)  spatial_scale::float, required : Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Reshape     Method .  Reshape(data, target_shape, keep_highest, shape, reverse)  Reshape input according to a target shape spec. The target shape is a tuple and can be a simple list of dimensions such as (12,3) or it can incorporate special codes that correspond to contextual operations that refer to the input dimensions. The special codes are all expressed as integers less than 1. These codes effectively refer to a machine that pops input dims off the beginning of the input dims list and pushes resulting output dims onto the end of the output dims list, which starts empty. The codes are:   0  Copy     Pop one input dim and push it onto the output dims  -1  Infer    Push a dim that is inferred later from all other output dims  -2  CopyAll  Pop all remaining input dims and push them onto output dims  -3  Merge2   Pop two input dims, multiply them, and push result  -4  Split2   Pop one input dim, and read two next target shape specs,               push them both onto output dims (either can be -1 and will               be inferred from the other  The exact mathematical behavior of these codes is given in the description of the 'shape' parameter. All non-codes (positive integers) just pop a dim off the input dims (if any), throw it away, and then push the specified integer onto the output dims. Examples: Type     Input      Target            Output Copy     (2,3,4)    (4,0,2)           (4,3,2) Copy     (2,3,4)    (2,0,0)           (2,3,4) Infer    (2,3,4)    (6,1,-1)          (6,1,4) Infer    (2,3,4)    (3,-1,8)          (3,1,8) CopyAll  (9,8,7)    (-2)              (9,8,7) CopyAll  (9,8,7)    (9,-2)            (9,8,7) CopyAll  (9,8,7)    (-2,1,1)          (9,8,7,1,1) Merge2   (3,4)      (-3)              (12) Merge2   (3,4,5)    (-3,0)            (12,5) Merge2   (3,4,5)    (0,-3)            (3,20) Merge2   (3,4,5,6)  (-3,0,0)          (12,5,6) Merge2   (3,4,5,6)  (-3,-2)           (12,5,6) Split2   (12)       (-4,6,2)          (6,2) Split2   (12)       (-4,2,6)          (2,6) Split2   (12)       (-4,-1,6)         (2,6) Split2   (12,9)     (-4,2,6,0)        (2,6,9) Split2   (12,9,9,9) (-4,2,6,-2)       (2,6,9,9,9) Split2   (12,12)    (-4,2,-1,-4,-1,2) (2,6,6,2)  From:src/operator/tensor/matrix_op.cc:62  Arguments   data::NDArray : Input data to reshape.  target_shape::Shape(tuple), optional, default=(0,0) : (Deprecated! Use shape instead.) Target new shape. One and only one dim can be 0, in which case it will be inferred from the rest of dims  keep_highest::boolean, optional, default=False : (Deprecated! Use shape instead.) Whether keep the highest dim unchanged.If set to true, then the first dim in target_shape is ignored,and always fixed as input  shape::Shape(tuple), optional, default=() : Target shape, a tuple, t=(t_1,t_2,..,t_m).   Let the input dims be s=(s_1,s_2,..,s_n). The output dims u=(u_1,u_2,..,u_p) are computed from s and t. The target shape tuple elements t_i are read in order, and used to  generate successive output dims u_p: t_i:       meaning:      behavior: +ve        explicit      u_p = t_i 0          copy          u_p = s_i -1         infer         u_p = (Prod s_i) / (Prod u_j | j != p) -2         copy all      u_p = s_i, u_p+1 = s_i+1, ... -3         merge two     u_p = s_i * s_i+1 -4,a,b     split two     u_p = a, u_p+1 = b | a * b = s_i The split directive (-4) in the target shape tuple is followed by two dimensions, one of which can be -1, which means it will be inferred from the other one and the original dimension. The can only be one globally inferred dimension (-1), aside from any -1 occuring in a split directive.   reverse::boolean, optional, default=False : Whether to match the shapes from the backward. If reverse is true, 0 values in the  shape  argument will be searched from the backward. E.g the original shape is (10, 5, 4) and the shape argument is (-1, 0). If reverse is true, the new shape should be (50, 4). Otherwise it will be (40, 5).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.SVMOutput     Method .  SVMOutput(data, label, margin, regularization_coefficient, use_linear)  Support Vector Machine based transformation on input, backprop L2-SVM  Arguments   data::SymbolicNode : Input data to svm.  label::SymbolicNode : Label data.  margin::float, optional, default=1 : Scale the DType(param_.margin) for activation size  regularization_coefficient::float, optional, default=1 : Scale the coefficient responsible for balacing coefficient size and error tradeoff  use_linear::boolean, optional, default=False : If set true, uses L1-SVM objective function. Default uses L2-SVM objective  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.SequenceLast     Method .  SequenceLast(data, sequence_length, use_sequence_length)  Takes the last element of a sequence. Takes an n-dimensional tensor of the form [max sequence length, batchsize, other dims] and returns a (n-1)-dimensional tensor of the form [batchsize, other dims]. This operator takes an optional input tensor sequence_length of positive ints of dimension [batchsize] when the sequence_length option is set to true. This allows the operator to handle variable-length sequences. If sequence_length is false, then each example in the batch is assumed to have the max sequence length.  Arguments   data::SymbolicNode : n-dimensional input tensor of the form [max sequence length, batchsize, other dims]  sequence_length::SymbolicNode : vector of sequence lengths of size batchsize  use_sequence_length::boolean, optional, default=False : If set to true, this layer takes in extra input sequence_length to specify variable length sequence  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.SequenceMask     Method .  SequenceMask(data, sequence_length, use_sequence_length, value)  Sets all elements outside the sequence to a constant value. Takes an n-dimensional tensor of the form [max sequence length, batchsize, other dims] and returns a tensor of the same shape. This operator takes an optional input tensor sequence_length of positive ints of dimension [batchsize] when the sequence_length option is set to true. This allows the operator to handle variable-length sequences. If sequence_length is false, then each example in the batch is assumed to have the max sequence length, and this operator becomes the identity operator.  Arguments   data::SymbolicNode : n-dimensional input tensor of the form [max sequence length, batchsize, other dims]  sequence_length::SymbolicNode : vector of sequence lengths of size batchsize  use_sequence_length::boolean, optional, default=False : If set to true, this layer takes in extra input sequence_length to specify variable length sequence  value::float, optional, default=0 : The value to be used as a mask.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.SequenceReverse     Method .  SequenceReverse(data, sequence_length, use_sequence_length)  Reverses the elements of each sequence. Takes an n-dimensional tensor of the form [max sequence length, batchsize, other dims] and returns a tensor of the same shape. This operator takes an optional input tensor sequence_length of positive ints of dimension [batchsize] when the sequence_length option is set to true. This allows the operator to handle variable-length sequences. If sequence_length is false, then each example in the batch is assumed to have the max sequence length.  Arguments   data::SymbolicNode : n-dimensional input tensor of the form [max sequence length, batchsize, other dims]  sequence_length::SymbolicNode : vector of sequence lengths of size batchsize  use_sequence_length::boolean, optional, default=False : If set to true, this layer takes in extra input sequence_length to specify variable length sequence  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.SliceChannel     Method .  SliceChannel(num_outputs, axis, squeeze_axis)  Slice input equally along specified axis  Arguments   num_outputs::int, required : Number of outputs to be sliced.  axis::int, optional, default='1' : Dimension along which to slice.  squeeze_axis::boolean, optional, default=False : If true AND the sliced dimension becomes 1, squeeze that dimension.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Softmax     Method .  Softmax(data, grad_scale, ignore_label, multi_output, use_ignore, preserve_shape, normalization, out_grad)  DEPRECATED: Perform a softmax transformation on input. Please use SoftmaxOutput  Arguments   data::SymbolicNode : Input data to softmax.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor  ignore_label::float, optional, default=-1 : the label value will be ignored during backward (only works if use_ignore is set to be true).  multi_output::boolean, optional, default=False : If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n x_1 ...*x_n output, each has k classes  use_ignore::boolean, optional, default=False : If set to true, the ignore_label value will not contribute to the backward gradient  preserve_shape::boolean, optional, default=False : If true, for a (n_1, n_2, ..., n_d, k) dimensional input tensor, softmax will generate (n1, n2, ..., n_d, k) output, normalizing the k classes as the last dimension.  normalization::{'batch', 'null', 'valid'},optional, default='null' : If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored  out_grad::boolean, optional, default=False : Apply weighting from output gradient  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.SoftmaxActivation     Method .  SoftmaxActivation(data, mode)  Apply softmax activation to input. This is intended for internal layers. For output (loss layer) please use SoftmaxOutput. If mode=instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If mode=channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.  Arguments   data::SymbolicNode : Input data to activation function.  mode::{'channel', 'instance'},optional, default='instance' : Softmax Mode. If set to instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If set to channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.SoftmaxOutput     Method .  SoftmaxOutput(data, label, grad_scale, ignore_label, multi_output, use_ignore, preserve_shape, normalization, out_grad)  Perform a softmax transformation on input, backprop with logloss.  Arguments   data::SymbolicNode : Input data to softmax.  label::SymbolicNode : Label data, can also be probability value with same shape as data  grad_scale::float, optional, default=1 : Scale the gradient by a float factor  ignore_label::float, optional, default=-1 : the label value will be ignored during backward (only works if use_ignore is set to be true).  multi_output::boolean, optional, default=False : If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n x_1 ...*x_n output, each has k classes  use_ignore::boolean, optional, default=False : If set to true, the ignore_label value will not contribute to the backward gradient  preserve_shape::boolean, optional, default=False : If true, for a (n_1, n_2, ..., n_d, k) dimensional input tensor, softmax will generate (n1, n2, ..., n_d, k) output, normalizing the k classes as the last dimension.  normalization::{'batch', 'null', 'valid'},optional, default='null' : If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored  out_grad::boolean, optional, default=False : Apply weighting from output gradient  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.SpatialTransformer     Method .  SpatialTransformer(data, loc, target_shape, transform_type, sampler_type)  Apply spatial transformer to input feature map.  Arguments   data::SymbolicNode : Input data to the SpatialTransformerOp.  loc::SymbolicNode : localisation net, the output dim should be 6 when transform_type is affine, and the name of loc symbol should better starts with 'stn_loc', so that initialization it with iddentify tranform, or you shold initialize the weight and bias by yourself.  target_shape::Shape(tuple), optional, default=(0,0) : output shape(h, w) of spatial transformer: (y, x)  transform_type::{'affine'}, required : transformation type  sampler_type::{'bilinear'}, required : sampling type  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.SwapAxis     Method .  SwapAxis(data, dim1, dim2)  Apply swapaxis to input.  Arguments   data::SymbolicNode : Input data to the SwapAxisOp.  dim1::int (non-negative), optional, default=0 : the first axis to be swapped.  dim2::int (non-negative), optional, default=0 : the second axis to be swapped.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.UpSampling     Method .  UpSampling(data, scale, num_filter, sample_type, multi_input_mode, num_args, workspace)  Note : UpSampling takes variable number of positional inputs. So instead of calling as UpSampling([x, y, z], num_args=3), one should call via UpSampling(x, y, z), and num_args will be determined automatically.  Perform nearest neighboor/bilinear up sampling to inputs  Arguments   data::SymbolicNode[] : Array of tensors to upsample  scale::int (non-negative), required : Up sampling scale  num_filter::int (non-negative), optional, default=0 : Input filter. Only used by bilinear sample_type.  sample_type::{'bilinear', 'nearest'}, required : upsampling method  multi_input_mode::{'concat', 'sum'},optional, default='concat' : How to handle multiple input. concat means concatenate upsampled images along the channel dimension. sum means add all images together, only available for nearest neighbor upsampling.  num_args::int, required : Number of inputs to be upsampled. For nearest neighbor upsampling, this can be 1-N; the size of output will be(scale h_0,scale w_0) and all other inputs will be upsampled to thesame size. For bilinear upsampling this must be 2; 1 input and 1 weight.  workspace::long (non-negative), optional, default=512 : Tmp workspace for deconvolution (MB)  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Variable     Method .  Variable(name :: Union{Symbol, AbstractString})  Create a symbolic variable with the given name. This is typically used as a placeholder. For example, the data node, acting as the starting point of a network architecture.  Arguments   Dict{Symbol, AbstractString} attrs: The attributes associated with this  Variable .   source  #  MXNet.mx._CrossDeviceCopy     Method .  _CrossDeviceCopy()  Special op to copy data cross device  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Div     Method .  _Div(lhs, rhs)  _Div is an alias of _div.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._DivScalar     Method .  _DivScalar(data, scalar)  _DivScalar is an alias of _div_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Equal     Method .  _Equal(lhs, rhs)  _Equal is an alias of _equal.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._EqualScalar     Method .  _EqualScalar(data, scalar)  _EqualScalar is an alias of _equal_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Greater     Method .  _Greater(lhs, rhs)  _Greater is an alias of _greater.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._GreaterEqualScalar     Method .  _GreaterEqualScalar(data, scalar)  _GreaterEqualScalar is an alias of _greater_equal_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._GreaterScalar     Method .  _GreaterScalar(data, scalar)  _GreaterScalar is an alias of _greater_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Greater_Equal     Method .  _Greater_Equal(lhs, rhs)  _Greater_Equal is an alias of _greater_equal.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Hypot     Method .  _Hypot(lhs, rhs)  _Hypot is an alias of _hypot.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._HypotScalar     Method .  _HypotScalar(data, scalar)  _HypotScalar is an alias of _hypot_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Lesser     Method .  _Lesser(lhs, rhs)  _Lesser is an alias of _lesser.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._LesserEqualScalar     Method .  _LesserEqualScalar(data, scalar)  _LesserEqualScalar is an alias of _lesser_equal_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._LesserScalar     Method .  _LesserScalar(data, scalar)  _LesserScalar is an alias of _lesser_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Lesser_Equal     Method .  _Lesser_Equal(lhs, rhs)  _Lesser_Equal is an alias of _lesser_equal.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Maximum     Method .  _Maximum(lhs, rhs)  _Maximum is an alias of _maximum.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._MaximumScalar     Method .  _MaximumScalar(data, scalar)  _MaximumScalar is an alias of _maximum_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Minimum     Method .  _Minimum(lhs, rhs)  _Minimum is an alias of _minimum.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._MinimumScalar     Method .  _MinimumScalar(data, scalar)  _MinimumScalar is an alias of _minimum_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Minus     Method .  _Minus(lhs, rhs)  _Minus is an alias of _sub.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._MinusScalar     Method .  _MinusScalar(data, scalar)  _MinusScalar is an alias of _minus_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Mul     Method .  _Mul(lhs, rhs)  _Mul is an alias of _mul.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._MulScalar     Method .  _MulScalar(data, scalar)  _MulScalar is an alias of _mul_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._NDArray     Method .  _NDArray(info)  Stub for implementing an operator implemented in native frontend language with ndarray.  Arguments   info::, required :  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Native     Method .  _Native(info, need_top_grad)  Stub for implementing an operator implemented in native frontend language.  Arguments   info::, required :  need_top_grad::boolean, optional, default=True : Whether this layer needs out grad for backward. Should be false for loss layers.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._NoGradient     Method .  _NoGradient()  Place holder for variable who cannot perform gradient  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._NotEqualScalar     Method .  _NotEqualScalar(data, scalar)  _NotEqualScalar is an alias of _not_equal_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Not_Equal     Method .  _Not_Equal(lhs, rhs)  _Not_Equal is an alias of _not_equal.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Plus     Method .  _Plus(lhs, rhs)  _Plus is an alias of elemwise_add.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._PlusScalar     Method .  _PlusScalar(data, scalar)  _PlusScalar is an alias of _plus_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Power     Method .  _Power(lhs, rhs)  _Power is an alias of _power.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._PowerScalar     Method .  _PowerScalar(data, scalar)  _PowerScalar is an alias of _power_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._RDivScalar     Method .  _RDivScalar(data, scalar)  _RDivScalar is an alias of _rdiv_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._RMinusScalar     Method .  _RMinusScalar(data, scalar)  _RMinusScalar is an alias of _rminus_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._RPowerScalar     Method .  _RPowerScalar(data, scalar)  _RPowerScalar is an alias of _rpower_scalar.  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._add     Method .  _add(lhs, rhs)  _add is an alias of elemwise_add.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._arange     Method .  _arange(start, stop, step, repeat, ctx, dtype)  Return evenly spaced values within a given interval. Similar to Numpy  Arguments   start::float, required : Start of interval. The interval includes this value. The default start value is 0.  stop::, optional, default=None : End of interval. The interval does not include this value, except in some cases where step is not an integer and floating point round-off affects the length of out.  step::float, optional, default=1 : Spacing between values.  repeat::int, optional, default='1' : The repeating time of all elements. E.g repeat=3, the element a will be repeated three times \u2013  a, a, a.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned .Only used for imperative calls.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32' : Target data type.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Activation     Method .  _backward_Activation()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_BatchNorm     Method .  _backward_BatchNorm()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Cast     Method .  _backward_Cast()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Concat     Method .  _backward_Concat()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Convolution     Method .  _backward_Convolution()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Correlation     Method .  _backward_Correlation()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Crop     Method .  _backward_Crop()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Custom     Method .  _backward_Custom()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Deconvolution     Method .  _backward_Deconvolution()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Dropout     Method .  _backward_Dropout()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Embedding     Method .  _backward_Embedding()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_FullyConnected     Method .  _backward_FullyConnected()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_IdentityAttachKLSparseReg     Method .  _backward_IdentityAttachKLSparseReg()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_InstanceNorm     Method .  _backward_InstanceNorm()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_L2Normalization     Method .  _backward_L2Normalization()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_LRN     Method .  _backward_LRN()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_LeakyReLU     Method .  _backward_LeakyReLU()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_LinearRegressionOutput     Method .  _backward_LinearRegressionOutput()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_LogisticRegressionOutput     Method .  _backward_LogisticRegressionOutput()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_MAERegressionOutput     Method .  _backward_MAERegressionOutput()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_MakeLoss     Method .  _backward_MakeLoss()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Pad     Method .  _backward_Pad()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Pooling     Method .  _backward_Pooling()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_RNN     Method .  _backward_RNN()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_ROIPooling     Method .  _backward_ROIPooling()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_SVMOutput     Method .  _backward_SVMOutput()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_SequenceLast     Method .  _backward_SequenceLast()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_SequenceMask     Method .  _backward_SequenceMask()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_SequenceReverse     Method .  _backward_SequenceReverse()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_SliceChannel     Method .  _backward_SliceChannel()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Softmax     Method .  _backward_Softmax()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_SoftmaxActivation     Method .  _backward_SoftmaxActivation()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_SoftmaxOutput     Method .  _backward_SoftmaxOutput()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_SpatialTransformer     Method .  _backward_SpatialTransformer()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_SwapAxis     Method .  _backward_SwapAxis()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_UpSampling     Method .  _backward_UpSampling()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward__CrossDeviceCopy     Method .  _backward__CrossDeviceCopy()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward__NDArray     Method .  _backward__NDArray()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward__Native     Method .  _backward__Native()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_abs     Method .  _backward_abs(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_add     Method .  _backward_add()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_arccos     Method .  _backward_arccos(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_arccosh     Method .  _backward_arccosh(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_arcsin     Method .  _backward_arcsin(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_arcsinh     Method .  _backward_arcsinh(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_arctan     Method .  _backward_arctan(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_arctanh     Method .  _backward_arctanh(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_batch_dot     Method .  _backward_batch_dot()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_broadcast_add     Method .  _backward_broadcast_add()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_broadcast_div     Method .  _backward_broadcast_div()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_broadcast_hypot     Method .  _backward_broadcast_hypot()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_broadcast_maximum     Method .  _backward_broadcast_maximum()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_broadcast_minimum     Method .  _backward_broadcast_minimum()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_broadcast_mul     Method .  _backward_broadcast_mul()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_broadcast_power     Method .  _backward_broadcast_power()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_broadcast_sub     Method .  _backward_broadcast_sub()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_clip     Method .  _backward_clip()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_copy     Method .  _backward_copy()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_cos     Method .  _backward_cos(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_cosh     Method .  _backward_cosh(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_degrees     Method .  _backward_degrees(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_div     Method .  _backward_div()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_dot     Method .  _backward_dot(transpose_a, transpose_b)  Arguments   transpose_a::boolean, optional, default=False : True if the first matrix is transposed.  transpose_b::boolean, optional, default=False : True if the second matrix is tranposed.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_expm1     Method .  _backward_expm1(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_gamma     Method .  _backward_gamma(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_gammaln     Method .  _backward_gammaln(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_hypot     Method .  _backward_hypot()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_hypot_scalar     Method .  _backward_hypot_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  scalar::float : scalar value  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_log     Method .  _backward_log(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_log1p     Method .  _backward_log1p(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_max     Method .  _backward_max()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_maximum     Method .  _backward_maximum()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_maximum_scalar     Method .  _backward_maximum_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  scalar::float : scalar value  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_mean     Method .  _backward_mean()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_min     Method .  _backward_min()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_minimum     Method .  _backward_minimum()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_minimum_scalar     Method .  _backward_minimum_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  scalar::float : scalar value  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_mul     Method .  _backward_mul()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_nanprod     Method .  _backward_nanprod()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_nansum     Method .  _backward_nansum()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_power     Method .  _backward_power()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_power_scalar     Method .  _backward_power_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  scalar::float : scalar value  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_prod     Method .  _backward_prod()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_radians     Method .  _backward_radians(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_rdiv_scalar     Method .  _backward_rdiv_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  scalar::float : scalar value  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_rpower_scalar     Method .  _backward_rpower_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  scalar::float : scalar value  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_rsqrt     Method .  _backward_rsqrt(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_sign     Method .  _backward_sign(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_sin     Method .  _backward_sin(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_sinh     Method .  _backward_sinh(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_slice_axis     Method .  _backward_slice_axis()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_smooth_l1     Method .  _backward_smooth_l1(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_softmax_cross_entropy     Method .  _backward_softmax_cross_entropy()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_sqrt     Method .  _backward_sqrt(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_square     Method .  _backward_square(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_sub     Method .  _backward_sub()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_sum     Method .  _backward_sum()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_take     Method .  _backward_take()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_tan     Method .  _backward_tan(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_tanh     Method .  _backward_tanh(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_topk     Method .  _backward_topk()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._broadcast     Method .  _broadcast(src, axis, size)  Broadcast array in the given axis to the given size  Arguments   src::NDArray : source ndarray  axis::int : axis to broadcast  size::int : size of broadcast  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._broadcast_backward     Method .  _broadcast_backward()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._copy     Method .  _copy(data)  Identity mapping, copy src to output  From:src/operator/tensor/elemwise_unary_op.cc:14  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._copyto     Method .  _copyto(src)  Arguments   src::NDArray : Source input to the function.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._crop_assign     Method .  _crop_assign(lhs, rhs, begin, end)  (Assign the rhs to a cropped subset of lhs.  Requirements   output should be explicitly given and be the same as lhs.  lhs and rhs are of the same data type, and on the same device.   )  From:src/operator/tensor/matrix_op.cc:159  Arguments   lhs::NDArray : Source input  rhs::NDArray : value to assign  begin::Shape(tuple), required : starting coordinates  end::Shape(tuple), required : ending coordinates  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._crop_assign_scalar     Method .  _crop_assign_scalar(data, scalar, begin, end)  (Assign the scalar to a cropped subset of the input.  Requirements   output should be explicitly given and be the same as input   )  From:src/operator/tensor/matrix_op.cc:183  Arguments   data::NDArray : Source input  scalar::float, optional, default=0 : The scalar value for assignment.  begin::Shape(tuple), required : starting coordinates  end::Shape(tuple), required : ending coordinates  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._cvcopyMakeBorder     Method .  _cvcopyMakeBorder(src, top, bot, left, right, type, value)  Pad image border with OpenCV.   Arguments   src::NDArray : source image  top::int, required : Top margin.  bot::int, required : Bottom margin.  left::int, required : Left margin.  right::int, required : Right margin.  type::int, optional, default='0' : Filling type (default=cv2.BORDER_CONSTANT).  value::double, optional, default=0 : Fill with value.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._cvimdecode     Method .  _cvimdecode(buf, flag, to_rgb)  Decode image with OpenCV.  Note: return image in RGB by default, instead of OpenCV's default BGR.  Arguments   buf::NDArray : Buffer containing binary encoded image  flag::int, optional, default='1' : Convert decoded image to grayscale (0) or color (1).  to_rgb::boolean, optional, default=True : Whether to convert decoded image to mxnet's default RGB format (instead of opencv's default BGR).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._cvimresize     Method .  _cvimresize(src, w, h, interp)  Resize image with OpenCV.   Arguments   src::NDArray : source image  w::int, required : Width of resized image.  h::int, required : Height of resized image.  interp::int, optional, default='1' : Interpolation method (default=cv2.INTER_LINEAR).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._div_scalar     Method .  _div_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._equal     Method .  _equal(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._equal_scalar     Method .  _equal_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._grad_add     Method .  _grad_add(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._greater     Method .  _greater(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._greater_equal     Method .  _greater_equal(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._greater_equal_scalar     Method .  _greater_equal_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._greater_scalar     Method .  _greater_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._hypot     Method .  _hypot(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._hypot_scalar     Method .  _hypot_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._identity_with_attr_like_rhs     Method .  _identity_with_attr_like_rhs()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._imdecode     Method .  _imdecode(mean, index, x0, y0, x1, y1, c, size)  Decode an image, clip to (x0, y0, x1, y1), subtract mean, and write to buffer  Arguments   mean::NDArray : image mean  index::int : buffer position for output  x0::int : x0  y0::int : y0  x1::int : x1  y1::int : y1  c::int : channel  size::int : length of str_img  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._lesser     Method .  _lesser(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._lesser_equal     Method .  _lesser_equal(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._lesser_equal_scalar     Method .  _lesser_equal_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._lesser_scalar     Method .  _lesser_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._maximum     Method .  _maximum(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._maximum_scalar     Method .  _maximum_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._minimum     Method .  _minimum(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._minimum_scalar     Method .  _minimum_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._minus     Method .  _minus(lhs, rhs)  _minus is an alias of _sub.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._minus_scalar     Method .  _minus_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._mul     Method .  _mul(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._mul_scalar     Method .  _mul_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._not_equal     Method .  _not_equal(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._not_equal_scalar     Method .  _not_equal_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._onehot_encode     Method .  _onehot_encode(lhs, rhs)  Arguments   lhs::NDArray : Left operand to the function.  rhs::NDArray : Right operand to the function.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._ones     Method .  _ones(shape, ctx, dtype)  fill target with ones  Arguments   shape::Shape(tuple), optional, default=() : The shape of the output  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned .Only used for imperative calls.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32' : Target data type.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._plus     Method .  _plus(lhs, rhs)  _plus is an alias of elemwise_add.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._plus_scalar     Method .  _plus_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._power     Method .  _power(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._power_scalar     Method .  _power_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._rdiv_scalar     Method .  _rdiv_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._rminus_scalar     Method .  _rminus_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._rpower_scalar     Method .  _rpower_scalar(data, scalar)  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sample_normal     Method .  _sample_normal(loc, scale, shape, ctx, dtype)  _sample_normal is an alias of normal.  Sample a normal distribution  Arguments   loc::float, optional, default=0 : Mean of the distribution.  scale::float, optional, default=1 : Standard deviation of the distribution.  shape::Shape(tuple), optional, default=() : The shape of the output  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned .Only used for imperative calls.  dtype::{'float32'},optional, default='float32' : DType of the output  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sample_uniform     Method .  _sample_uniform(low, high, shape, ctx, dtype)  _sample_uniform is an alias of uniform.  Sample a uniform distribution  Arguments   low::float, optional, default=0 : The lower bound of distribution  high::float, optional, default=1 : The upper bound of distribution  shape::Shape(tuple), optional, default=() : The shape of the output  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned .Only used for imperative calls.  dtype::{'float32'},optional, default='float32' : DType of the output  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._zeros     Method .  _zeros(shape, ctx, dtype)  fill target with zeros  Arguments   shape::Shape(tuple), optional, default=() : The shape of the output  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned .Only used for imperative calls.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32' : Target data type.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.adam_update     Method .  adam_update()  Updater function for adam optimizer  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.arccos     Method .  arccos(data)  Take arccos of the src  From:src/operator/tensor/elemwise_unary_op.cc:216  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.arccosh     Method .  arccosh(data)  Take arccosh of the src  From:src/operator/tensor/elemwise_unary_op.cc:288  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.arcsin     Method .  arcsin(data)  Take arcsin of the src  From:src/operator/tensor/elemwise_unary_op.cc:207  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.arcsinh     Method .  arcsinh(data)  Take arcsinh of the src  From:src/operator/tensor/elemwise_unary_op.cc:279  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.arctan     Method .  arctan(data)  Take arctan of the src  From:src/operator/tensor/elemwise_unary_op.cc:225  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.arctanh     Method .  arctanh(data)  Take arctanh of the src  From:src/operator/tensor/elemwise_unary_op.cc:297  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.argmax     Method .  argmax(data, axis, keepdims)  Compute argmax  From:src/operator/tensor/broadcast_reduce_op_index.cc:11  Arguments   data::NDArray : Source input  axis::int, optional, default='-1' : Empty or unsigned. The axis to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.argmax_channel     Method .  argmax_channel(src)  Arguments   src::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.argmin     Method .  argmin(data, axis, keepdims)  Compute argmin  From:src/operator/tensor/broadcast_reduce_op_index.cc:16  Arguments   data::NDArray : Source input  axis::int, optional, default='-1' : Empty or unsigned. The axis to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.argsort     Method .  argsort(src, axis, is_ascend)  Returns the indices that would sort an array.  From:src/operator/tensor/ordering_op.cc:89  Arguments   src::NDArray : Source input  axis::int or None, optional, default='-1' : Axis along which to sort the input tensor. If not given, the flattened array is used. Default is -1.  is_ascend::boolean, optional, default=True : Whether sort in ascending or descending order.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.batch_dot     Method .  batch_dot(lhs, rhs, transpose_a, transpose_b)  Calculate batched dot product of two matrices. (batch, M, K) X (batch, K, N) \u2013  (batch, M, N).  From:src/operator/tensor/matrix_op.cc:257  Arguments   lhs::NDArray : Left input  rhs::NDArray : Right input  transpose_a::boolean, optional, default=False : True if the first matrix is transposed.  transpose_b::boolean, optional, default=False : True if the second matrix is tranposed.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.batch_take     Method .  batch_take(a, indices)  Take scalar value from a batch of data vectos according to an index vector, i.e. out[i] = a[i, indices[i]]  From:src/operator/tensor/indexing_op.cc:97  Arguments   a::NDArray : Input data array  indices::NDArray : index array  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_add     Method .  broadcast_add(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_axis     Method .  broadcast_axis(data, axis, size)  Broadcast src along axis  From:src/operator/tensor/broadcast_reduce_op_value.cc:85  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : The axes to perform the broadcasting.  size::Shape(tuple), optional, default=() : Target sizes of the broadcasting axes.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_div     Method .  broadcast_div(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_equal     Method .  broadcast_equal(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_greater     Method .  broadcast_greater(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_greater_equal     Method .  broadcast_greater_equal(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_hypot     Method .  broadcast_hypot(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_lesser     Method .  broadcast_lesser(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_lesser_equal     Method .  broadcast_lesser_equal(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_maximum     Method .  broadcast_maximum(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_minimum     Method .  broadcast_minimum(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_minus     Method .  broadcast_minus(lhs, rhs)  broadcast_minus is an alias of broadcast_sub.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_mul     Method .  broadcast_mul(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_not_equal     Method .  broadcast_not_equal(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_plus     Method .  broadcast_plus(lhs, rhs)  broadcast_plus is an alias of broadcast_add.  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_power     Method .  broadcast_power(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_sub     Method .  broadcast_sub(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_to     Method .  broadcast_to(data, shape)  Broadcast src to shape  From:src/operator/tensor/broadcast_reduce_op_value.cc:92  Arguments   data::NDArray : Source input  shape::Shape(tuple), optional, default=() : The shape of the desired array. We can set the dim to zero if it's same as the original. E.g  A = broadcast_to(B, shape=(10, 0, 0))  has the same meaning as  A = broadcast_axis(B, axis=0, size=10) .  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.choose_element_0index     Method .  choose_element_0index(lhs, rhs)  Choose one element from each line(row for python, column for R/Julia) in lhs according to index indicated by rhs. This function assume rhs uses 0-based index.  Arguments   lhs::NDArray : Left operand to the function.  rhs::NDArray : Right operand to the function.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.clip     Method .  clip(data, a_min, a_max)  Clip ndarray elements to range (a_min, a_max)  From:src/operator/tensor/matrix_op.cc:289  Arguments   data::NDArray : Source input  a_min::float, required : Minimum value  a_max::float, required : Maximum value  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.crop     Method .  crop(data, begin, end)  (Crop the input tensor and return a new one.  Requirements   the input and output (if explicitly given) are of the same data type, and on the same device.   )  From:src/operator/tensor/matrix_op.cc:143  Arguments   data::NDArray : Source input  begin::Shape(tuple), required : starting coordinates  end::Shape(tuple), required : ending coordinates  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.degrees     Method .  degrees(data)  Take degrees of the src  From:src/operator/tensor/elemwise_unary_op.cc:234  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.elemwise_add     Method .  elemwise_add(lhs, rhs)  Arguments   lhs::NDArray : first input  rhs::NDArray : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.expand_dims     Method .  expand_dims(data, axis)  Expand the shape of array by inserting a new axis.  From:src/operator/tensor/matrix_op.cc:122  Arguments   data::NDArray : Source input  axis::int (non-negative), required : Position (amongst axes) where new axis is to be inserted.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.fill_element_0index     Method .  fill_element_0index(lhs, mhs, rhs)  Fill one element of each line(row for python, column for R/Julia) in lhs according to index indicated by rhs and values indicated by mhs. This function assume rhs uses 0-based index.  Arguments   lhs::NDArray : Left operand to the function.  mhs::NDArray : Middle operand to the function.  rhs::NDArray : Right operand to the function.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.fix     Method .  fix(data)  Take round of the src to integer nearest 0  From:src/operator/tensor/elemwise_unary_op.cc:102  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.flip     Method .  flip(data, axis)  Flip the input tensor along axis and return a new one.  From:src/operator/tensor/matrix_op.cc:219  Arguments   data::NDArray : Source input  axis::int, required : The dimension to flip  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.from_json     Method .  from_json(repr :: AbstractString, ::Type{SymbolicNode})  Load a  SymbolicNode  from a JSON string representation.  source  #  MXNet.mx.gammaln     Method .  gammaln(data)  Take gammaln (log of the absolute value of gamma(x)) of the src  From:src/operator/tensor/elemwise_unary_op.cc:315  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.get_attr     Method .  get_attr(self :: SymbolicNode, key :: Symbol)  Get attribute attached to this  SymbolicNode  belonging to key.  Returns the value belonging to key as a  Nullable .  source  #  MXNet.mx.get_internals     Method .  get_internals(self :: SymbolicNode)  Get a new grouped  SymbolicNode  whose output contains all the internal outputs of this  SymbolicNode .  source  #  MXNet.mx.grad     Method .  grad(self :: SymbolicNode, wrt :: Vector{SymbolicNode})  Get the autodiff gradient of the current  SymbolicNode . This function can only be used if the current symbol is a loss function.  Arguments:   self::SymbolicNode : current node.  wrt::Vector{Symbol} : the names of the arguments to the gradient.   Returns a gradient symbol of the corresponding gradient.  source  #  MXNet.mx.infer_shape     Method .  infer_shape(self :: SymbolicNode, args...)\ninfer_shape(self :: SymbolicNode; kwargs...)  Do shape inference according to the input shapes. The input shapes could be provided as a list of shapes, which should specify the shapes of inputs in the same order as the arguments returned by  list_arguments . Alternatively, the shape information could be specified via keyword arguments.  Returns a 3-tuple containing shapes of all the arguments, shapes of all the outputs and shapes of all the auxiliary variables. If shape inference failed due to incomplete or incompatible inputs, the return value will be  (nothing, nothing, nothing) .  source  #  MXNet.mx.infer_type     Method .  infer_type(self :: SymbolicNode; kwargs...)\ninfer_type(self :: SymbolicNode, args...)  Do type inference according to the input types. The input types could be provided as a list of types, which should specify the types of inputs in the same order as the arguments returned by  list_arguments . Alternatively, the type information could be specified via keyword arguments.  Returns a 3-tuple containing types of all the arguments, types of all the outputs and types of all the auxiliary variables. If type inference failed due to incomplete or incompatible inputs, the return value will be  (nothing, nothing, nothing) .  source  #  MXNet.mx.list_all_attr     Method .  list_all_attr(self :: SymbolicNode)  Get all attributes from the symbol graph.  Returns a dictionary of attributes.  source  #  MXNet.mx.list_arguments     Method .  list_arguments(self :: SymbolicNode)  List all the arguments of this node. The argument for a node contains both the inputs and parameters. For example, a  FullyConnected  node will have both data and weights in its arguments. A composed node (e.g. a MLP) will list all the arguments for intermediate nodes.  Returns a list of symbols indicating the names of the arguments.  source  #  MXNet.mx.list_attr     Method .  list_attr(self :: SymbolicNode)  Get all attributes from a symbol.  Returns a dictionary of attributes.  source  #  MXNet.mx.list_auxiliary_states     Method .  list_auxiliary_states(self :: SymbolicNode)  List all auxiliary states in the symbool.  Auxiliary states are special states of symbols that do not corresponds to an argument, and do not have gradient. But still be useful for the specific operations. A common example of auxiliary state is the moving_mean and moving_variance in BatchNorm. Most operators do not have Auxiliary states.  Returns a list of symbols indicating the names of the auxiliary states.  source  #  MXNet.mx.list_outputs     Method .  list_outputs(self :: SymbolicNode)  List all the outputs of this node.  Returns a list of symbols indicating the names of the outputs.  source  #  MXNet.mx.load     Method .  load(filename :: AbstractString, ::Type{SymbolicNode})  Load a  SymbolicNode  from a JSON file.  source  #  MXNet.mx.max_axis     Method .  max_axis(data, axis, keepdims)  max_axis is an alias of max.  Compute max along axis. If axis is empty, global reduction is performed  From:src/operator/tensor/broadcast_reduce_op_value.cc:66  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.min_axis     Method .  min_axis(data, axis, keepdims)  min_axis is an alias of min.  Compute min along axis. If axis is empty, global reduction is performed  From:src/operator/tensor/broadcast_reduce_op_value.cc:76  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.nanprod     Method .  nanprod(data, axis, keepdims)  Compute product of src along axis, ignoring NaN values. If axis is empty, global reduction is performed  From:src/operator/tensor/broadcast_reduce_op_value.cc:56  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.nansum     Method .  nansum(data, axis, keepdims)  Sum src along axis, ignoring NaN values. If axis is empty, global reduction is performed  From:src/operator/tensor/broadcast_reduce_op_value.cc:46  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.negative     Method .  negative(data)  Negate src  From:src/operator/tensor/elemwise_unary_op.cc:57  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.normal     Method .  normal(loc, scale, shape, ctx, dtype)  Sample a normal distribution  Arguments   loc::float, optional, default=0 : Mean of the distribution.  scale::float, optional, default=1 : Standard deviation of the distribution.  shape::Shape(tuple), optional, default=() : The shape of the output  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned .Only used for imperative calls.  dtype::{'float32'},optional, default='float32' : DType of the output  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.radians     Method .  radians(data)  Take radians of the src  From:src/operator/tensor/elemwise_unary_op.cc:243  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.rint     Method .  rint(data)  Take round of the src to nearest integer  From:src/operator/tensor/elemwise_unary_op.cc:97  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.rsqrt     Method .  rsqrt(data)  Take reciprocal square root of the src  From:src/operator/tensor/elemwise_unary_op.cc:125  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.save     Method .  save(filename :: AbstractString, node :: SymbolicNode)  Save a  SymbolicNode  to a JSON file.  source  #  MXNet.mx.set_attr     Method .  set_attr(self:: SymbolicNode, key :: Symbol, value :: AbstractString)  Set the attribute key to value for this  SymbolicNode .   Note  It is encouraged not to call this function directly, unless you know exactly what you are doing. The recommended way of setting attributes is when creating the  SymbolicNode . Changing the attributes of a  SymbolicNode  that is already been used somewhere else might cause unexpected behavior and inconsistency.   source  #  MXNet.mx.sgd_mom_update     Method .  sgd_mom_update()  Updater function for sgd optimizer  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.sgd_update     Method .  sgd_update()  Updater function for sgd optimizer  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.slice_axis     Method .  slice_axis(data, axis, begin, end)  Slice the input along certain axis and return a sliced array. The slice will be taken from [begin, end). end can be None and axis can be negative.  From:src/operator/tensor/matrix_op.cc:200  Arguments   data::NDArray : Source input  axis::int, required : The axis to be sliced. Negative axis means to count from the last to the first axis.  begin::int, required : The beginning index to be sliced. Negative values are interpreted as counting from the backward.  end::int or None, required : The end index to be sliced. The end can be None, in which case all the rest elements are used. Also, negative values are interpreted as counting from the backward.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.smooth_l1     Method .  smooth_l1(data, scalar)  Calculate Smooth L1 Loss(lhs, scalar)  From:src/operator/tensor/elemwise_binary_scalar_op_extended.cc:63  Arguments   data::NDArray : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.softmax_cross_entropy     Method .  softmax_cross_entropy(data, label)  Calculate cross_entropy(data, one_hot(label))  From:src/operator/loss_binary_op.cc:12  Arguments   data::NDArray : Input data  label::NDArray : Input label  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.square     Method .  square(data)  Take square of the src  From:src/operator/tensor/elemwise_unary_op.cc:107  Arguments   data::NDArray : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.sum_axis     Method .  sum_axis(data, axis, keepdims)  sum_axis is an alias of sum.  Sum src along axis. If axis is empty, global reduction is performed  From:src/operator/tensor/broadcast_reduce_op_value.cc:17  Arguments   data::NDArray : Source input  axis::Shape(tuple), optional, default=() : Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : If true, the axis which is reduced is left in the result as dimension with size one.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.to_json     Method .  to_json(self :: SymbolicNode)  Convert a  SymbolicNode  into a JSON string.  source  #  MXNet.mx.topk     Method .  topk(src, axis, k, ret_typ, is_ascend)  Return the top k element of an input tensor along a given axis.  From:src/operator/tensor/ordering_op.cc:18  Arguments   src::NDArray : Source input  axis::int or None, optional, default='-1' : Axis along which to choose the top k indices. If not given, the flattened array is used. Default is -1.  k::int, optional, default='1' : Number of top elements to select, should be always smaller than or equal to the element number in the given axis. A global sort is performed if set k   1.  ret_typ::{'both', 'indices', 'mask', 'value'},optional, default='indices' : The return type. \"value\" means returning the top k values, \"indices\" means returning the indices of the top k values, \"mask\" means to return a mask array containing 0 and 1. 1 means the top k values. \"both\" means to return both value and indices.  is_ascend::boolean, optional, default=False : Whether to choose k largest or k smallest. Top K largest elements will be chosen if set to false.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.uniform     Method .  uniform(low, high, shape, ctx, dtype)  Sample a uniform distribution  Arguments   low::float, optional, default=0 : The lower bound of distribution  high::float, optional, default=1 : The upper bound of distribution  shape::Shape(tuple), optional, default=() : The shape of the output  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned .Only used for imperative calls.  dtype::{'float32'},optional, default='float32' : DType of the output  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source", 
            "title": "Symbolic API"
        }, 
        {
            "location": "/api/nn-factory/", 
            "text": "Neural Network Factory\n\n\nNeural network factory provide convenient helper functions to define common neural networks.\n\n\n#\n\n\nMXNet.mx.MLP\n \n \nMethod\n.\n\n\nMLP(input, spec; hidden_activation = :relu, prefix)\n\n\n\n\nConstruct a multi-layer perceptron. A MLP is a multi-layer neural network with fully connected layers.\n\n\nArguments:\n\n\n\n\ninput::SymbolicNode\n: the input to the mlp.\n\n\nspec\n: the mlp specification, a list of hidden dimensions. For example,         \n[128, (512, :sigmoid), 10]\n. The number in the list indicate the         number of hidden units in each layer. A tuple could be used to specify         the activation of each layer. Otherwise, the default activation will         be used (except for the last layer).\n\n\nhidden_activation::Symbol\n: keyword argument, default \n:relu\n, indicating         the default activation for hidden layers. The specification here could be overwritten         by layer-wise specification in the \nspec\n argument. Also activation is not         applied to the last, i.e. the prediction layer. See \nActivation\n for a         list of supported activation types.\n\n\nprefix\n: keyword argument, default \ngensym()\n, used as the prefix to         name the constructed layers.\n\n\n\n\nReturns the constructed MLP.\n\n\nsource", 
            "title": "Neural Networks Factory"
        }, 
        {
            "location": "/api/nn-factory/#neural-network-factory", 
            "text": "Neural network factory provide convenient helper functions to define common neural networks.  #  MXNet.mx.MLP     Method .  MLP(input, spec; hidden_activation = :relu, prefix)  Construct a multi-layer perceptron. A MLP is a multi-layer neural network with fully connected layers.  Arguments:   input::SymbolicNode : the input to the mlp.  spec : the mlp specification, a list of hidden dimensions. For example,          [128, (512, :sigmoid), 10] . The number in the list indicate the         number of hidden units in each layer. A tuple could be used to specify         the activation of each layer. Otherwise, the default activation will         be used (except for the last layer).  hidden_activation::Symbol : keyword argument, default  :relu , indicating         the default activation for hidden layers. The specification here could be overwritten         by layer-wise specification in the  spec  argument. Also activation is not         applied to the last, i.e. the prediction layer. See  Activation  for a         list of supported activation types.  prefix : keyword argument, default  gensym() , used as the prefix to         name the constructed layers.   Returns the constructed MLP.  source", 
            "title": "Neural Network Factory"
        }, 
        {
            "location": "/api/executor/", 
            "text": "Executor\n\n\n#\n\n\nMXNet.mx.Executor\n \n \nType\n.\n\n\nExecutor\n\n\n\n\nAn executor is a realization of a symbolic architecture defined by a \nSymbolicNode\n. The actual forward and backward computation specified by the network architecture can be carried out with an executor.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.GRAD_REQ\n \n \nType\n.\n\n\nbind(sym, ctx, args; args_grad=Dict(), aux_states=Dict(), grad_req=GRAD_WRITE)\n\n\n\n\nCreate an \nExecutor\n by binding a \nSymbolicNode\n to concrete \nNDArray\n.\n\n\nArguments\n\n\n\n\nsym::SymbolicNode\n: the network architecture describing the computation graph.\n\n\nctx::Context\n: the context on which the computation should run.\n\n\nargs\n: either a list of \nNDArray\n or a dictionary of name-array pairs. Concrete         arrays for all the inputs in the network architecture. The inputs typically include         network parameters (weights, bias, filters, etc.), data and labels. See \nlist_arguments\n         and \ninfer_shape\n.\n\n\nargs_grad\n:\n\n\naux_states\n:\n\n\ngrad_req\n:\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.debug_str\n \n \nMethod\n.\n\n\nGet a debug string about internal execution plan.\n\n\nCan be used to get an estimated about the memory cost.\n\n\n  net = ... # Symbol\n  dProvider = ... # DataProvider\n  exec = mx.simple_bind(net, mx.cpu(), data=size(dProvider.data_batch[1]))\n  dbg_str = mx.debug_str(exec)\n  println(split(ref, ['\\n'])[end-2])\n\n\n\n\nsource", 
            "title": "Executor"
        }, 
        {
            "location": "/api/executor/#executor", 
            "text": "#  MXNet.mx.Executor     Type .  Executor  An executor is a realization of a symbolic architecture defined by a  SymbolicNode . The actual forward and backward computation specified by the network architecture can be carried out with an executor.  source  #  MXNet.mx.GRAD_REQ     Type .  bind(sym, ctx, args; args_grad=Dict(), aux_states=Dict(), grad_req=GRAD_WRITE)  Create an  Executor  by binding a  SymbolicNode  to concrete  NDArray .  Arguments   sym::SymbolicNode : the network architecture describing the computation graph.  ctx::Context : the context on which the computation should run.  args : either a list of  NDArray  or a dictionary of name-array pairs. Concrete         arrays for all the inputs in the network architecture. The inputs typically include         network parameters (weights, bias, filters, etc.), data and labels. See  list_arguments          and  infer_shape .  args_grad :  aux_states :  grad_req :   source  #  MXNet.mx.debug_str     Method .  Get a debug string about internal execution plan.  Can be used to get an estimated about the memory cost.    net = ... # Symbol\n  dProvider = ... # DataProvider\n  exec = mx.simple_bind(net, mx.cpu(), data=size(dProvider.data_batch[1]))\n  dbg_str = mx.debug_str(exec)\n  println(split(ref, ['\\n'])[end-2])  source", 
            "title": "Executor"
        }, 
        {
            "location": "/api/visualize/", 
            "text": "Network Visualization\n\n\n#\n\n\nMXNet.mx.to_graphviz\n \n \nMethod\n.\n\n\nto_graphviz(network)\n\n\n\n\n\n\nnetwork::SymbolicNode\n: the network to visualize.\n\n\ntitle::AbstractString:\n keyword argument, default \"Network Visualization\",         the title of the GraphViz graph.\n\n\ninput_shapes\n: keyword argument, default \nnothing\n. If provided,         will run shape inference and plot with the shape information. Should         be either a dictionary of name-shape mapping or an array of shapes.\n\n\n\n\nReturns the graph description in GraphViz \ndot\n language.\n\n\nsource", 
            "title": "Network Visualization"
        }, 
        {
            "location": "/api/visualize/#network-visualization", 
            "text": "#  MXNet.mx.to_graphviz     Method .  to_graphviz(network)   network::SymbolicNode : the network to visualize.  title::AbstractString:  keyword argument, default \"Network Visualization\",         the title of the GraphViz graph.  input_shapes : keyword argument, default  nothing . If provided,         will run shape inference and plot with the shape information. Should         be either a dictionary of name-shape mapping or an array of shapes.   Returns the graph description in GraphViz  dot  language.  source", 
            "title": "Network Visualization"
        }
    ]
}